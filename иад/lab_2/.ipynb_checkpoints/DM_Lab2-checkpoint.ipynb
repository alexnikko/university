{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №2\n",
    "# Препроцессинг данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Препроцессинг данных включает в себя широкий круг методов для очистки, выбора и преобразования данных с целью улучшения качества последующего интеллектуального анализа данных. \n",
    "\n",
    "Программные средства для препроцессинга данных имеются как в библиотеке Pandas, так и основной библиотеке машинного обучения scikit-learn (sklearn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Качество наборов данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Плохое качество данных оказывает негативное воздействие на процесс анализа данных. Наиболее часто встречающиеся проблемы включают шум, выбросы, отсутствующие значения и дублирующиеся данные. \n",
    "\n",
    "Начнем с примера набора данных из репозитария UCI с информацией о пациентах с раком груди. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/'+\n",
    "                   'breast-cancer-wisconsin/breast-cancer-wisconsin.data', header=None)\n",
    "data.columns = ['Sample code', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape',\n",
    "                'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei', 'Bland Chromatin',\n",
    "                'Normal Nucleoli', 'Mitoses','Class']\n",
    "\n",
    "data = data.drop(['Sample code'],axis=1)        # удаляем ненужный столбец\n",
    "print('Число записей = %d' % (data.shape[0]))\n",
    "print('Число признаков = %d' % (data.shape[1]))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Отсутствующие (пропущенные) значения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Достаточно часто в записи отсутствуют одно или несколько значений признаков. Иногда не хватает информации, а иногда некоторые значения не подходят для данных. Существуют различные подходы для работы с отсутствующими значениями. \n",
    "\n",
    "В наборах данных репозитария UCI отсутствующие значения часто кодируются как символьная строка '?'. Первая задача состоит в конвертации отсутствующих значений в значение NaNs (NaN - Not a Number). Далее можно подсчитать количество отсутствующих значений в каждом столбце набора данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # для использования np.NaN\n",
    "\n",
    "data = data.replace('?',np.NaN)\n",
    "\n",
    "print('Число записей = %d' % (data.shape[0]))\n",
    "print('Число признаков = %d' % (data.shape[1]))\n",
    "\n",
    "print('Число отсутствующих значений:')\n",
    "for col in data.columns:\n",
    "    print('\\t%s: %d' % (col,data[col].isna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Среди всех столбцов только столбец 'Bare Nuclei' содержит отсутствующие значения. Отсутствующие значения в столбце 'Bare Nuclei' могут быть заменены на медиану столбца при помощи метода `fillna()` (значения до и после замены показаны на подмножестве записей)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data['Bare Nuclei']\n",
    "\n",
    "print('До замены отсутствующих значений:')\n",
    "print(data2[20:25])\n",
    "data2 = data2.fillna(data2.median())\n",
    "\n",
    "print('\\nПосле замены отсутствующих значений:')\n",
    "print(data2[20:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вместо замены отсутствующих значений можно удалить записи (строки), содержащие отсутствующие значения. Для этого можно использовать метод `dropna()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Число записей в исходных данных = %d' % (data.shape[0]))\n",
    "\n",
    "data2 = data.dropna()\n",
    "print('Число записей после удаления отсутствующих значений = %d' % (data2.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбросы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбросами (outliers) называются записи (строки) с характеристиками, которые существенно отличаются от характеристик остальных записей набора данных. \n",
    "\n",
    "Ниже мы изобразим диаграммы размаха (boxplot) столбцов, чтобы найти столбцы таблицы, которые содержат выбросы. Так как столбец 'Bare Nuclei' идентифицирован Pandas как строковый (из-за отсутствующих значений, представленных строками '?'), нам придется конвертировать столбец в числовые значения для того, чтобы использовать диаграмму размаха. В противном случае столбец не будет отображаться на рисунке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "data2 = data.drop(['Class'],axis=1)\n",
    "data2['Bare Nuclei'] = pd.to_numeric(data2['Bare Nuclei'])\n",
    "data2.boxplot(figsize=(20,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Диаграммы размаха показывают, что только пять столбцов (Marginal Adhesion, Single Epithetial Cell Size, Bland Cromatin, Normal Nucleoli, Mitoses) содержат ненормально большие значения. \n",
    "\n",
    "Чтобы убрать выбросы, можно посчитать стандартизованную оценку (Z-score) для каждого признака и убрать записи, содержащие атрибуты с ненормально высоким или низким Z-score (например, $Z>3$ или $Z<-3$). Для нормального распределения вероятность отклонения случайной величины от своего математического ожидания более чем на три стандартных отклонения практически равна нулю (правило трех сигм).\n",
    "\n",
    "Следующий код показывает результаты стандартизации стоблцов с данными. Отсутствующие значения (NaN) не затрагиваются процессом стандартизации. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = (data2-data2.mean())/data2.std()\n",
    "Z[20:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующий код показывает результаты удаления строк, для которых $Z>3$ или $Z<-3$. Число 9 соответствует количеству столбцов в Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Число записей до удаления выбросов = %d' % (Z.shape[0]))\n",
    "\n",
    "Z2 = Z.loc[((Z >= -3).sum(axis=1)==9) & ((Z <= 3).sum(axis=1)==9),:]\n",
    "print('Число записей после удаления выбросов = %d' % (Z2.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дублирующиеся данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые наборы данных, особенно полученные слиянием данных из нескольких источников, могут содержать дублирующиеся записи. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups = data.duplicated()\n",
    "print('Число дублирующихся записей = %d' % (dups.sum()))\n",
    "data.loc[[11,28]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `duplicated()` возвращает булевский массив, который показывает является ли запись дубликатом какой-либо предыдущей записи в таблице. Результат означает, что в наборе данных пациентов с раком груди имеется 236 дублирующихся записей. Например, строка с индексом 11 имеет те же значения признаков, что и строка с индексом 28. \n",
    "\n",
    "Хотя дублирующиеся записи могут соответствовать данным различных пациентов, допустим, что дублирующиеся записи  соответствуют одному и тому же пациенту и удалим их:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Число записей до удаления дубликатов = %d' % (data.shape[0]))\n",
    "data2 = data.drop_duplicates()\n",
    "print('Число записей после удаления дубликатов = %d' % (data2.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Трансформация (преобразование) данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стандартизация признака"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стандартизацией случайной величины $X$ называют ее линейное преобразование, приводящее к случайной величине с математическим ожиданием 0 и стандартным отклонением 1:\n",
    "\n",
    "$\\tilde{X}=\\frac{X-\\mathbb{E}\\left[X\\right]}{\\sqrt{\\mathbb{V}\\left[X\\right]}},$\n",
    "\n",
    "где $\\mathbb{E}$ – операция вычисления математического ожидания, $\\mathbb{V}$ – операция вычисления дисперсии. \n",
    "\n",
    "Стандартизация набора данных является существенным условием для применения многих алгоритмов машинного обучения, а именно, алгоритмы дают приемлемый результат, только если отдельные признаки распределены примерно как стандартные нормальные величины (с нулевым матожиданием и единичной дисперсией).\n",
    "\n",
    "Для стандартизации признаков набора данных может быть использована функция `scale()` из модуля `preprocessing`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "X = np.array([[ 1., -1.,  2.],\n",
    "              [ 2.,  0.,  0.],\n",
    "              [ 0.,  1., -1.]])\n",
    "X_scaled = preprocessing.scale(X)\n",
    "print(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стандартизованный набор данных имеет признаки с нулевыми средними и единичной дисперсией:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_scaled.mean(axis=0))\n",
    "print(X_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также модуль preprocessing содержит класс `StandardScaler`, который позволяет сохранить математическое ожидание и стандартное отклонение обучающей выборки и затем применять то же преобразование к другим выборкам. \n",
    "\n",
    "Альтернативным вариантом стандартизации является масштабирование признака между заданным минимальным и максимальным значениями (нормализация). Этот эффект может быть достигнут при помощи функций `MinMaxScaler()` или `MaxAbsScaler()`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[ 1., -1.,  2.],\n",
    "              [ 2.,  0.,  0.],\n",
    "              [ 0.,  1., -1.]])\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_minmax = min_max_scaler.fit_transform(X)\n",
    "X_minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция `MaxAbsScaler()` используется аналогично, но масштабирует данные в диапазон $\\left[-1,\\,1\\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Агрегирование данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Агрегирование данных – это процесс преобразования данных с высокой степенью детализации к более обобщенному представлению, когда значения двух и более объектов комбинируются в один объект. \n",
    "\n",
    "Целями агрегирования являются:\n",
    "\n",
    "1. уменьшение размера обрабатываемых данных\n",
    "\n",
    "2. изменение (укрупнение) масштабов анализа \n",
    "\n",
    "3. улучшение стабильности данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В примере ниже мы используем дневные временные ряды с данными об атмосферных осадках от метеорологической станции. Временные ряды с дневными данными об осадках будут сравниваться с месячными значениями. \n",
    "\n",
    "Программный код загружает дневные данные об осадках и рисует график значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = pd.read_csv('DTW_prec.csv', header='infer')\n",
    "daily.index = pd.to_datetime(daily['DATE'])\n",
    "daily = daily['PRCP']\n",
    "ax = daily.plot(kind='line',figsize=(15,3))\n",
    "ax.set_title('Дневные осадки (дисперсия = %.4f)' % (daily.var()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дневные данные об осадках выглядят весьма хаотическими и изменяются существенно от одного момента времени к другому. Дневные данные могут быть сгрупированы и агрегированы по месяцам, чтобы получить общие месячные значения осадков. Полученные в результате агрегирования данные выглядят более гладкими по сравнению с дневными данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly = daily.groupby(pd.Grouper(freq='M')).sum()\n",
    "ax = monthly.plot(kind='line',figsize=(15,3))\n",
    "ax.set_title('Месячные осадки (дисперсия = %.4f)' % (monthly.var()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В примере ниже дневные данные сгруппированы и агрегированы по годам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual = daily.groupby(pd.Grouper(freq='Y')).sum()\n",
    "ax = annual.plot(kind='line',figsize=(15,3))\n",
    "ax.set_title('Годовые осадки (дисперсия = %.4f)' % (annual.var()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Семплирование данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Семплирование (от англ. sample — выборка), или методы управления выборкой данных, – это подход, направленный на:\n",
    "\n",
    "1. сокращение объема данных для анализа данных и масштабирования алгоритмов для приложений с большими данными\n",
    "\n",
    "2. количественную оценку неопределенностей из-за различного распределения данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существуют различные методы выборки данных, такие как выборка без замены, когда каждый выбранный экземпляр удаляется из набора данных, и выборка с заменой, где каждый выбранный экземпляр не удаляется, что позволяет выбирать его более одного раза.\n",
    "\n",
    "В примере ниже мы применим выборку с заменой и без замены с набору данных пациентов с раком груди.\n",
    "\n",
    "Выведем первые пять записей набора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее данные для выборки размера 3 (без замены) выбираются случайным образом из исходных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data.sample(n=3)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В следующем примере мы случайным образом выбираем 1% данных (без замены) и выводим выбранные записи. Параметр `random_state` задает начальное значение для генератора случайных чисел. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data.sample(frac=0.01, random_state=1)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, выполним выборку с заменой размером, равным 1% всех данных. Можно увидеть повторяющиеся записи в выборке, если увеличить ее размеры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data.sample(frac=0.01, replace=True, random_state=1)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дискретизация данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дискретизация – это этап препроцессинга, который часто используется при преобразовании непрерывного признака в категориальный. \n",
    "\n",
    "Пример ниже иллюстрирует два простых, но часто применяемых метода дискретизации (равной ширины,  равных частот) для признака 'Clump Thickness' набора данных пациентов с раком груди.\n",
    "\n",
    "Вначале нарисуем гистограмму, которая показывает распределение значений признака. Метод `value_counts()` также может быть использован, чтобы подсчитать частоты каждого значения признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Clump Thickness'].hist(bins=10)\n",
    "data['Clump Thickness'].value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При использовани метода равной ширины можно использовать метод `cut()`, чтобы дискретизировать признак в 4 бина, имеющих равную ширину. Метод `value_counts()` может быть использован для определения числа записей в каждом из бинов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = pd.cut(data['Clump Thickness'],4)\n",
    "bins.value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При использовани метода равных частот можно использовать метод `qcut()` для разделения значений признака на 4 бина, имеющих примерно равное число записей. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = pd.qcut(data['Clump Thickness'],4)\n",
    "bins.value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дискретизация также возможна при помощи средств библиотеки scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кодирование категориальных признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В большинстве наборов данных присутствуют категориальные признаки, которые содержат значения в текстовом формате. Примерами являются цвета (“Red”, “Green”, “Yellow”, “Blue”), размеры (“Small”, “Medium”, “Large”, “Extra Large”), географические обозначения (страны, города и т.п.). Независимо от назначения категориальных признаков возникает вопрос, как использовать категориальные признаки при анализе данных. Многие алгоритмы машинного обучения поддерживают категориальные значения без необходимости каких-либо манипуляций с данными, однако есть и такие алгоритмы, которые требуют преобразования текстовых значений в числовые для дальнейшей обработки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Набор данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим набор данных Automobile из репозитария UCI, содержащий как категориальные, так и непрерывные признаки. \n",
    "\n",
    "Импортируем данные, выполняя попутно обработку пропущенных значений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# определяем метки столбцов\n",
    "headers = [\"symboling\", \"normalized_losses\", \"make\", \"fuel_type\", \"aspiration\",\n",
    "    \"num_doors\", \"body_style\", \"drive_wheels\", \"engine_location\",\n",
    "    \"wheel_base\", \"length\", \"width\", \"height\", \"curb_weight\",\n",
    "    \"engine_type\", \"num_cylinders\", \"engine_size\", \"fuel_system\",\n",
    "    \"bore\", \"stroke\", \"compression_ratio\", \"horsepower\", \"peak_rpm\",\n",
    "    \"city_mpg\", \"highway_mpg\", \"price\"]\n",
    "# считываем CSV файл и конвертируем значения \"?\" в NaN\n",
    "df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data\",\n",
    "    header=None, names=headers, na_values=\"?\" )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы понять, с какими типами данным мы имеем дело, рассмотрим свойство"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как нас интересуют только категориальные признаки, оставим в наборе столбцы с типом `“object”`. Pandas содержит удобный метод `select_dtypes()`, который можно использовать, чтобы оставить в наборе только столбцы с типом `“object”` (категориальные признаки):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df = df.select_dtypes(include=['object']).copy()\n",
    "obj_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построенный набор содержит несколько строк с пропущенными значениями, которые нужно заполнить:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df[obj_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В наборе наиболее часто встречается значение \"four\" (4 двери):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df[\"num_doors\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для простоты заполним пропущенные значения этим значением:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df = obj_df.fillna({\"num_doors\": \"four\"})\n",
    "obj_df[obj_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь набор не содержит пропущенных значений и мы можем приступить к кодированию категориальных значений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Замена значений признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В двух столбцах набора данных текстовые значения представляют собой числа, а именно, число цилиндров в двигателе и число дверей в автомобиле. \n",
    "\n",
    "Признак \"num_cylinders\" принимает 7 значений, которые легко преобразуются в целые числа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df[\"num_cylinders\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `replace()` из Pandas имеет множество опций, в частности, опцию словаря, содержащего названия столбцов и словари для отображения старых значений в новые значения.\n",
    "\n",
    "Словарь для преобразования признаков \"num_doors\" и \"num_cylinders\" в числовые значения задается следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_nums = {\"num_doors\": {\"four\": 4, \"two\": 2},\n",
    "    \"num_cylinders\": {\"four\": 4, \"six\": 6, \"five\": 5, \"eight\": 8,\n",
    "    \"two\": 2, \"twelve\": 12, \"three\":3 }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для преобразования признаков в числовые значения выполним код: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df.replace(cleanup_nums, inplace=True)\n",
    "obj_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas автоматически преобразует тип признаков в числовой (int64):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описанный подход работает в тех случаях, когда имеется понятный способ интерпретации текстовых значений как числовых."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Кодирование меток"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кодирование меток (label encoding) – это способ конвертации значений в столбцах в числа.\n",
    "\n",
    "Например, столбец body_style содержит 5 различных значений. Можно закодировать их так: \n",
    "\n",
    "    convertible -> 0  \n",
    "    hardtop -> 1  \n",
    "    hatchback -> 2  \n",
    "    sedan -> 3  \n",
    "    wagon -> 4  \n",
    "\n",
    "Можно использовать Pandas, чтобы преобразовать столбец в категорию (категория – это тип данных в Pandas, принимающий несколько значений), а потом использовать значения категории для кодирования меток:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df[\"body_style\"] = obj_df[\"body_style\"].astype('category')\n",
    "obj_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее можно присвоить закодированные значения признака новому столбцу \"body_style_cat\" используя свойство `cat.codes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df[\"body_style_cat\"] = obj_df[\"body_style\"].cat.codes\n",
    "obj_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Особенностью этого подхода является то, что появляется возможность использовать преимущества категорий Pandas (компактность данных, возможность упорядочения, поддержка визуализации), при этом категории могут быть легко конвертированы в числовые значения для дальнейшего анализа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Прямое кодирование (One Hot Encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кодирование меток имеет преимущество в виде простоты реализации и недостаток, состоящий в том, что числовое значение может быть некорректно интерпретировано алгоритмами машинного обучения. Например, значение 0, очевидно, меньше значения 2, но соответствует ли эта зависимость реальной ситуации для текстовых значений? \n",
    "\n",
    "Альтернативный подход (прямое кодирование) состоит в том, чтобы конвертировать каждую категорию в новый столбец, принимающий значения 1 или 0 (True/False). Преимуществом этого подхода является то, что между категориальными значениями не устанавливаются несуществующие связи, а недостатком – что в наборе данных появляются дополнительные столбцы. \n",
    "\n",
    "Pandas поддерживает этот подход в функции `get_dummies()`, которая создает новые столбцы вида “столбец_значение”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим пример для столбца drive_wheels со значениями 4wd , fwd, rwd. Используя `get_dummies()` мы конвертируем этот столбец в три столбца со значениями 1 или 0, соответствующими правильному значению исходного признака (столбца):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(obj_df, columns=[\"drive_wheels\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Столбец \"drive_wheels\" пропал, при этом новый набор данных содержит три новых столбца:\n",
    "\n",
    "• drive_wheels_4wd  \n",
    "• drive_wheels_rwd  \n",
    "• drive_wheels_fwd  \n",
    "\n",
    "В функцию `get_dummies()` можно передать несколько столбцов с категориальными признаками, а также передать префиксы для именования новых столбцов с целью упростить последующий анализ данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(obj_df, columns=[\"body_style\", \"drive_wheels\"], prefix=[\"body\", \"drive\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прямое кодирование является очень полезным инструментом, однако может приводить к резкому увеличению числа столбцов в наборе, если категориальные признаки имеют большое число различных значений. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Двоичное кодирование, управляемое пользователем "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В зависимости от особенностей набора данных можно использовать различные комбинации кодирования меток и прямого кодирования, которые в наибольшей степени соответствуют целям дальнейшего анализа. \n",
    "\n",
    "Для иллюстрации двоичного кодирования, управляемого пользователем (custom binary encoding), рассмотрим следующий пример. В наборе данных имеется столбец engine_type (тип двигателя), который содержит несколько различных значений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df[\"engine_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Допустим, что требуется выделить в отдельную группу все двигатели с верхней камерой (Overhead Cam или OHC). Другими словами, различные версии OHC эквивалентны для анализа. В это случае можно использовать свойство `str` и функцию `np.where`, чтобы создать новый столбец как индикатор того, что двигатель автомобиля имеет тип OHC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df[\"OHC_Code\"] = np.where(obj_df[\"engine_type\"].str.contains(\"ohc\"), 1, 0)\n",
    "obj_df[\"OHC_Code\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате получаем набор данных, включающий столбец OHC_Code (показываем в наборе только три столбца):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df[[\"make\", \"engine_type\", \"OHC_Code\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный подход является по-настоящему полезным, если имеется возможность консолидировать бинарные значения (да/нет) в новом столбце. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Возможности кодирования в библиотеке Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека scikit-learn также содержит функцонал для кодирования текстовых признаков.\n",
    "\n",
    "Например, чтобы кодировать метки для производителей автомобиля, используем объект `LabelEncoder` и метод `fit_transform()` для столбца с данными:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb_make = LabelEncoder()\n",
    "obj_df[\"make_code\"] = lb_make.fit_transform(obj_df[\"make\"])\n",
    "obj_df[[\"make\", \"make_code\"]].head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn также поддерживает бинарное кодирование при помощи объекта `LabelBinarizer`. Можно использовать процедуру, аналогичную приведенной выше, чтобы преобразовать данные, но требуются некоторые дополнительные шаги."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb_style = LabelBinarizer()\n",
    "lb_results = lb_style.fit_transform(obj_df[\"body_style\"])\n",
    "pd.DataFrame(lb_results, columns=lb_style.classes_).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На следующем шаге нужно включить эти данные в исходный набор данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отбор признаков "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Набор признаков, используемых для обучения модели, оказывает значительное влияние на качество результатов. Присутствие в наборе данных малоинформативных признаков приводит к снижению точности многих моделей, особенно моделей регрессии.\n",
    "\n",
    "Отбор признаков (feature selection) – это процесс выбора признаков, обеспечивающий более высокое качество модели машинного обучения.\n",
    "\n",
    "Отбор признаков перед построением модели обеспечивает следующие преимущества:\n",
    "\n",
    "• Уменьшение переобучения. Чем меньше избыточных данных, тем меньше возможностей для модели принимать решения на основе «шума».\n",
    "\n",
    "• Повышение точности. Чем меньше противоречивых данных, тем выше точность.\n",
    "\n",
    "• Сокращение времени обучения. Чем меньше данных, тем быстрее обучается модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем работать с набором данных, содержащим информацию о качество вина. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удаление признаков с низкой дисперсией"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простейшим подходом к отбору признаков является исключение признаков с низкой дисперсией. Если дисперсия признака равна нулю, то признак для всех записей имеет одно и то же значение и может не приниматься во внимание при анализе данных. Если дисперсия признака близка к нулю, то признак принимает значения, близкие к некоторому (среднему) значению и, скорее всего, является несущественным. \n",
    "\n",
    "В качестве примера рассмотрим гипотетический набор данных с булевыми признаками и допустим, что мы хотим удалить все признаки, в которых нули или единицы составляют более чем 80% значений. Булевы признаки могут быть интепретированы как случайные величины с распределением Бернулли, имеющие дисперсию\n",
    "\n",
    "$V\\left[X\\right]=p\\,\\left(1-p\\right),$\n",
    "\n",
    "поэтому при отборе признаков можем использовать пороговое значение $0.8\\,(1-0.8)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "X = [[0, 0, 1], \n",
    "     [0, 1, 0], \n",
    "     [1, 0, 0], \n",
    "     [0, 1, 1], \n",
    "     [0, 1, 0], \n",
    "     [0, 1, 1]]\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "sel.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и ожидалось, метод `VarianceThreshold` удалил первый столбец, для которого вероятность нулевого значения $p=\\frac{5}{6}>0.8$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Одномерный отбор признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Признаки, имеющие наиболее выраженную взаимосвязь с целевой переменной, могут быть отобраны с помощью статистических критериев. Библиотека scikit-learn содержит класс `SelectKBest`, реализующий одномерный отбор признаков (univariate feature selection). Этот класс можно применять совместно с различными статистическими критериями для отбора заданного количества признаков.\n",
    "\n",
    "В примере ниже используется критерий хи-квадрат (chi-squared test) для неотрицательных признаков, чтобы отобрать 4 лучших признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# отбор признаков при помощи одномерных статистических тестов \n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "\n",
    "# загрузка данных - качество вина\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "df = pd.read_csv(url,sep=\";\")\n",
    "print(\"\\nИсходный набор данных:\\n\",df.head())\n",
    "array = df.values\n",
    "X = array[:,0:11] # входные переменные (11 признаков)\n",
    "Y = array[:,11]   # выходная переменная - качество (оценка между 0 и 10)\n",
    "\n",
    "# отбор признаков\n",
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "fit = test.fit(X, Y)\n",
    "\n",
    "# оценки признаков\n",
    "print(\"\\nОценки признаков:\\n\",fit.scores_)\n",
    "\n",
    "cols = test.get_support(indices=True)\n",
    "df_new = df.iloc[:,cols]\n",
    "print(\"\\nОтобранные признаки:\\n\",df_new.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим оценки для каждого признака и 4 отобранных признака (с наивысшими оценками): volatile acidity, free sulfur dioxide, total sulfur dioxide и alcohol.\n",
    "\n",
    "Если выходная (зависимая) переменная представляет собой класс, то можно использовать статистические критерии `chi2` или `f_classif`. Если выходная (зависимая) переменная представляет собой признак, принимающий непрерывные значения, то следует использовать статистический критерий `f_regression`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Отбор на основе важности признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ансамблевые алгоритмы на основе деревьев решений, такие как случайный лес (random forest), позволяют оценить важность признаков.\n",
    "\n",
    "В представленном ниже примере мы обучаем классификатор `ExtraTreesClassifier`, чтобы с его помощью определить важность признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance with Extra Trees Classifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# загрузка данных - качество вина\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "df = pd.read_csv(url,sep=\";\")\n",
    "\n",
    "array = df.values\n",
    "X = array[:,0:11] # входные переменные (11 признаков)\n",
    "Y = array[:,11]   # выходная переменная - качество (оценка между 0 и 10)\n",
    "\n",
    "# feature extraction\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, Y)\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы получили оценки для каждого признака. Чем больше значение оценки, тем важнее признак. Таким образом, согласно данному методу отбора, двумя наиболее важными признаками являются: total sulfur dioxide и sulphates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод главных компонент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод главных компонент (principal component analysis, PCA) позволяет уменьшить размерность данных с помощью преобразования на основе линейной алгебры. Пользователь может задать требуемое количество измерений (главных компонент) в результирующих данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем набор данных \"Ирисы\" и сократим его размерность до двух:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "url = \\\n",
    "\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "\n",
    "# считываем данные в объект data frame\n",
    "my_data = pd.read_csv( url, header=None, prefix=\"V\", usecols=(0,1,2,3) )\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "pcad = pca.fit_transform(my_data) # numpy array\n",
    "\n",
    "print( \"*** Первые 5 строк данных:\" )\n",
    "for x in range(0,5):\n",
    "  print( pcad[x] )  \n",
    "\n",
    "print( \"*** Дисперсии компонент:\\n\", pca.explained_variance_ratio_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим уровень объясняемой дисперсии для различных значений параметра `n_components`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(1,5):\n",
    "  pca = PCA( n_components = r )\n",
    "  pca.fit( my_data )\n",
    "  print( \"r =\",r,\"\\tДисперсия =\",\n",
    "        sum(pca.explained_variance_ratio_)*100,\"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В примере ниже выделим 3 главных компоненты с помощью PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from pandas import read_csv\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# загрузка данных - качество красного вина\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "df = pd.read_csv(url,sep=\";\")\n",
    "\n",
    "array = df.values\n",
    "X = array[:,0:11] # входные переменные (11 признаков)\n",
    "\n",
    "# главные компоненты\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(X)\n",
    "features = fit.transform(X)\n",
    "\n",
    "# результаты\n",
    "print(\"Объясняемая дисперсия:\", sum(fit.explained_variance_ratio_)*100)\n",
    "print(features[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат преобразования (3 главных компоненты) совсем не похож на исходные данные и содержит отрицательные значения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем Matplotlib для визуализации графиков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.figure( figsize=(8, 6), dpi=80 ) # размер 8x6 дюйма, 80 точек на дюйм\n",
    "plt.subplot( 1, 1, 1 )               # новый график на сетке 1x1\n",
    "\n",
    "X = np.linspace( -np.pi, np.pi, 256, endpoint=True ) \n",
    "C, S = np.cos(X), np.sin(X) \n",
    "\n",
    "# рисуем косинус синей сплошной линией шириной 2 пикселя \n",
    "plt.plot( X, C, color=\"blue\", linewidth=2.0, linestyle=\"-\" ) \n",
    "# рисуем синус красными точками шириной 5 пикселей \n",
    "plt.plot( X, S, color=\"red\", linewidth=5.0, linestyle=\":\" ) \n",
    "\n",
    "plt.xlim( -4.0, 4.0 )                              # пределы оси x  \n",
    "plt.xticks( np.linspace(-4, 4, 9, endpoint=True) ) # метки на оси x \n",
    "plt.ylim( -1.0, 1.0 )                              # пределы оси y\n",
    "plt.yticks( np.linspace(-1, 1, 5, endpoint=True) );# метки на оси y\n",
    "\n",
    "# plt.savefig('plot0.png', dpi=72) # сохранение в файл 72 точки на дюйм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем Matplotlib с настройкой меток осей и легендой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2, 100)\n",
    "plt.plot(x, x, label='линейная функция') \n",
    "plt.plot(x, x**2, label='квадратичная функция') \n",
    "plt.plot(x, x**3, label='кубическая функция')\n",
    "plt.xlabel('Ось x') \n",
    "plt.ylabel('Ось y')\n",
    "plt.title('Графики трех функций')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем Matplotlib для визуализации набора данных со сниженной размерностью (разными цветами):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from contextlib import closing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "with closing(urlopen(url)) as u, open(\"iris.csv\", \"w\") as f:\n",
    "    f.write(u.read().decode())\n",
    "\n",
    "data = np.genfromtxt( \"iris.csv\", delimiter=\",\", usecols=(0,1,2,3) ) \n",
    "target = np.genfromtxt( \"iris.csv\", delimiter=\",\", usecols=(4), dtype=str )\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pcad = pca.fit_transform( data )\n",
    "\n",
    "plt.figure( figsize=(8, 6), dpi=200 )\n",
    "plt.plot(pcad[target==\"Iris-setosa\",0],pcad[target==\"Iris-setosa\",1],\"bo\") \n",
    "plt.plot(pcad[target==\"Iris-versicolor\",0],pcad[target==\"Iris-versicolor\",1],\"r.\") \n",
    "plt.plot(pcad[target==\"Iris-virginica\",0],pcad[target==\"Iris-virginica\",1],\"go\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание на лабораторную работу №2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание (10 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для закрепленного за Вами варианта лабораторной работы:\n",
    "\n",
    "1. Используя функционал библиотеки Pandas, cчитайте заданный набор данных из репозитария UCI. \n",
    "\n",
    "2. Проведите исследование набора данных, выявляя числовые признаки. Если какие-то из числовых признаков были неправильно классифицированы, то преобразуйте их в числовые. Если в наборе для числовых признаков присутствуют пропущенные значения ('?'), то заполните их медианными значениями. \n",
    "\n",
    "3. Определите признак, содержащий метку класса. Определите числовой признак, имеющий максимальную дисперсию.\n",
    "\n",
    "4. При помощи класса SelectKBest библиотеки scikit-learn найдите два признака, имеющих наиболее выраженную взаимосвязь с признаком, имеющим максимальную дисперсию.\n",
    "\n",
    "5. Визуализируйте набор данных в виде точек плоскости с координатами, соответствующими найденным признакам, отображая точки различных классов разными цветами. \n",
    "\n",
    "6. Оставляя в наборе данных только числовые признаки, найдите и выведите на экран размерность метода главных компонент (параметр `n_components`), для которой доля объясняемой дисперсии будет не менее 99%.\n",
    "\n",
    "7. Пользуясь методом главных компонент, снизьте размерность набора данных до двух признаков и изобразите полученный набор данных в виде точек на плоскости, отображая точки различных классов разными цветами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
