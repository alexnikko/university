Introduction
In pharmaceutical drug product manufacturing, one of the most important activities after manufacturing a drug is the extensive testing to verify the product quality prior to release. For example, for an immediate release tablet with a BCS Class III drug, the following five properties are measured: content uniformity, dissolution, weight, hardness, and strength. Each test must be performed according to a protocol that has either been proposed by the manufacturer and approved by the regulatory agency, or described in the US pharmacopeia (USP). Typically, the number of samples to be tested is part of the protocol. In addition, there are strict documentation requirements because the regulator may require access to all/any of the data during audits or investigations. A typical lot release may take anywhere from a few days to few weeks depending on the battery of required tests and problems experienced during testing. If the quality criteria are not satisfied, the entire batch of product may be discarded or (more rarely) re-worked resulting in a significant loss to the manufacturer. In addition, the failed lots, investigations, and the root causes must be reported to the regulatory agency. If there are too many failed lots, a variety of remedies may be sought by the regulators ranging from penalties to withdrawing product to facility closure.
The three critical factors affecting lot release are different analytical tests performed, number of samples for each test, and the statistical methods used in analyzing the results. While which specific methods are used to perform the tests is determined by various factors related to the drug such as its chemistry, release mechanism, stability, and so on, the number of samples and analysis method come within the purview of statistical science. The regulatory agencies do continually explore ways to improve the lot release criteria and reduce the burden on manufacturers so that the time and effort required for releasing a lot are reasonable. The main objective of the project was to develop alternative statistical methods to determine the number of samples that must be tested for lot release. The proposed work envisaged considerable analytical experimentation and statistical analysis of data. An important aspect of the project implementation was the adaptation of novel data management techniques. The KProMS system [ , ] developed at Purdue University was used for this purpose. KProMS is a knowledge management and data curation system that supports collaborative research.
Data curation in general is the active and ongoing management of data through its lifecycle and usefulness to scholarship, science, and education. Data curation enables data discovery and retrieval, maintains data quality, adds value, and provides for reuse over time through activities including authentication, archiving, management, preservation, and representation [ ]. One driving force for curating data is the inherent fact that digital data can be easily shared, thereby facilitating new discoveries and uses. Data have always held value beyond their original purpose, and today, digital data can travel and reach worldwide audiences at unprecedented speeds [ ]. Other significant drivers for curating data are the emerging funding requirements for data sharing. Over the last several years, national funding agencies and political administrations worldwide have developed a growing awareness of and the need for public access to the results of government-funded research and the long-term preservation of these unique digital research data sets [ , ]. A key turning point in the USA was the memorandum [ ] by the White House Office of Science and Technology Policy (OSTP) directing federal agencies to develop plans to ensure all resulting publications and research data are publicly accessible. The memo’s requirements for sharing digital research data in ways that make the data “publicly accessible to search, retrieve, and analyze” suggested that federally funded researchers will soon be faced with many new requirements as follows: Ensure that the data are richly described with machine-actionable metadata Ensure that data are complete, self-explanatory, and accurate (quality) Protect confidentiality and privacy of the sponsoring company when making data available to regulatory agencies Account for the long-term access and preservation needs that go beyond the life of a grant. Identify and/or create trusted digital repositories to steward data over time
The efforts for sharing nationally funded research data run parallel to an open data movement for government-authored data. This movement is characterized by the G8 adoption of the “Open Data Charter” and demonstrated by the principles set forth in the US Open Data Action Plan released in 2014 [ ]. Additionally, private funders of research, such as the Ford Foundation, the Alfred P. Sloan Foundation, and the Bill & Melinda Gates Foundation, now require their funded projects release underlying data with some degree of openness.
Although the Open Data Action Plan set forth the principles for releasing data, there are no guidelines for good practices, or standards for publishing data. As a result, most of the full context and metadata for the published information are not readily available. Additionally, the quality of published data is not fully tested. These factors discourage wider use of published data, even though more data are widely becoming available. Based on a survey conducted by Wiley of recent authors of papers in the health, life, physical, and social sciences, and humanities interesting observations can be made about how data are shared by researchers [ ]. Around 38% of all respondents stated that they have shared data as supplementary material in journals, compared with only 19% who say they use a discipline specific repository and just 6% who report using a general repository. Many researchers report sharing data in informal, often, impermanent ways. In addition, 37% say that they are using a personal, institutional, or project website to share data—again, unlikely to meet any data sharing mandates, and certainly not the best way of ensuring any kind of long-term preservation of the data.
The need for sharing data, and consequently the need for well curated data, is expected to grow in the future. The KProMS framework helps the users in producing shareable and reusable data by (1) capturing the context of data creation needed for reuse, (2) streamlining submission of data to repositories, and (3) organizing data for access within and across web sites. Since KProMS is hosted on a web server, representing one web site, the system is fully accessible to registered users, constrained only by how the web server is set up, whether on intranet or internet. Additionally, services can be set up to provide open access to specific information as deemed appropriate by the host. Also, the number groups generating the content (data) can be restricted to as few as one. Thus, each group in an organization can have a web site of curated data.
In this paper, the use of KProMS for knowledge management and data curation is described through its application to the project mentioned above. First, the specific needs for managing and curating data for this project are described, followed by the details of the various aspects of data generation, sharing, and mining. The interface with the analytical methods is described in the end with the help of some simple examples. The discussion related to the alternate statistical methodology for lot release is beyond the scope of this paper.
Lot Release Criteria
Protocols for Analytical Techniques
In this paper, the following six different immediate release (IR) drug products were studied: atenolol, metformin, naproxen, chorpheniramine, ibuprofen, and glyburide. Tablets from a variety of manufacturers, with multiple lots from each manufacturer were acquired from distributors. Thus, the tablets presumably had all met the required quality criteria. The standard analytical tests typically required for lot release were performed on these procured drug products. For the six drug products, the following tests performed: content uniformity (CU); dissolution (DIS); and three physical properties (PP) weight, thickness, and hardness. All analytical tests were performed guided by the appropriate chapters in the U.S. Pharmacoepia modified to expand on some of the data collected.
Statistical Analysis of Data
The objective for generating and systematically recording the product quality data is to investigate alternative statistical methods and criteria for established intra and inter lot variability and lot release. The statistical methods can be structured as scientific workflows which use as input the data stored using experimental workflows and generate as output the required variability, confidence interval, and related statistical performance parameters. The well-structured form of the data retained in the experimental workflows facilitates the execution of these statistical workflows and the well-defined structure of these scientific workflows facilitates documentation and archiving of the statistical analysis results. While the details of the statistical analysis are beyond the scope of this paper, the statistical results are a key data type necessary to completely characterize a product's variability. The associated workflow constructs will be discussed in more detail in the Experimental Data Generation section.
Reporting Requirements
All the data were collected using methods (existing, modified, or new) validated in the Long Island University at Brooklyn (LIU) laboratories and are shared with all project participants, including the FDA collaborators. As the purpose of the research is to explore the variability of marketed products, data beyond that required for lot release must be gathered to fully characterize the properties and behavior/performance of the products and to explore statistical modeling opportunities. To that end, for example, full dissolution profiles are determined and individual tablets are used to estimate content uniformity. The sheer volume of the data generated, therefore, requires advanced data curation in order to track and share the data and perform the statistical analyses required.
Knowledge Provenance Management
The knowledge provenance management system [ , ] (KProMS) was used for managing and curating all of the information generated by the project. A brief description of KProMS and details of workflow templates for this project are given in this section, while the data curation aspects are discussed in the subsequent section.
KProMS System
The architecture of KProMS is shown in Fig. 1 . It has been developed for use on HUBzero [ ], a middleware based on Joomla. Both Joomla and HUBzero are available under the LGPL 3.0 license. HUBzero provides the MySQL server for the relational database and the Apache web server. The server side scripting was done using PHP and the graphics are rendered using SVG. HUBzero also provides the functionality for managing user accounts, backups, item tagging, and the ability to post information such as educational material, forums, and so on. Fig. 1 Architecture of KProMS
For this project, KProMS was implemented on pharmahub , a website ( www.pharmahub.org ) that is a specific installation of HUBzero managed by Purdue University’s ITaP organization. The open sourced components of KproMS have large user base, are well maintained and used extensively by some of the largest companies and universities worldwide. In addition, the MySQL database provides data security on par with any commercial database package.
The two most visible components of KProMS are the Workflow builder and KM Graphical User Interface (GUI). The workflow builder allows the user to create workflow that accurately model the knowledge generation processes, while the KM GUI provides access to all the data management functions of KProMS [ , ].
Data Flow and Management
This project consisted of three groups which were located in three different locations. The group responsible for performing the analytical tests on various drug products worked at the Long Island University’s Lachman Institute of Pharmaceutical Analysis in Brooklyn, NY. The group responsible for the statistical analysis worked at Purdue University in West Lafayette, IN. The regulatory agency project monitors located at different sites were provided access to all the results from analytical tests as well as statistical analysis. KProMS, by virtue of being a web based application, was perfectly suited for providing access from any location. More importantly, the information that was accessed was always up to date. Figure 2 shows the relationships between data creation, data flow and data management, and data analysis performed in the project. Fig. 2 Relationships between data creation, data flow, and management
For each drug product lot, a set of analytical tests was performed. For each test, a workflow was developed which represented all the steps associated with performing the experiment, uploading the data generated by analytical instruments to pharmahub , and performing data transformation and verification on the hub. The experimental conditions are recorded by specifying appropriate values for various parameters specific to the test. The raw data was backed up on a server at LIU. However, only the verified data on the hub was used in data analysis. The LIU group used the experimental workflows to build the data repository on pharmahub .
HUBzero is an inherently multiuser system and is designed for concurrent access to information. In addition, the most recent version of information is accessed by the user or application. Any number of simultaneous users can be active at any time, accessing any information in the repository, albeit constrained by the capabilities of the underlying hardware. The only possible conflict could occur during data creation if multiple users work on the same workflow instance at the same time. The likelihood of such occurrences is minimal.
Data analysis was performed by the PU group using a set of scientific workflows that represented the typical steps in analysis: data extraction, invoking a suitable data analysis program, storing the results in the repository.
The project monitoring group could access all the information in the repository by either directly browsing any workflow instance or through special purpose summary tables.
Workflow Templates
A workflow template was created for to accurately capture the details of each test listed above. The values recorded during each test are either defined as task and subtask parameters entered by the user, or as textual data files uploaded using the appropriate data nodes. The data files were typically generated by the instruments and consisted of a mix of structured and unstructured information. As discussed later, one of the key tasks of data curation was interpreting the unstructured information and converting it into structured information to facilitate analysis and reuse.
The workflow templates for measuring content uniformity are discussed in this section, whereas the dissolution and physical properties workflows are given in the Appendix .
Measuring content uniformity consisted of two steps: sample preparation and high-performance liquid chromatography (HPLC). During sample preparation, a tablet is dissolved in a predetermined amount of specific solvent. The concentration of the active in the sample is determined using HPLC. The two workflows are shown in Figs. 3 and 4 . Fig. 3 Sample preparation workflow Fig. 4 HPLC (Isocratic) workflow
The drug product associated with the tablet used in preparing a sample is specified with the Matl icon, whereas the solvent used is specified with the Solvent icon. The weight of the tablet, the composition and volume of the solvent added are also recorded. During the Dissolve task, the tablet is first crushed, then the solvent is added and the mixture is stirred until the drug is completely extracted. The solution is filtered and stored in labeled aliquots.
For content uniformity determination, 10 tablets are randomly selected and analyzed. For analyzing tablets from one lot, 10 samples were prepared in advance and were analyzed in one HPLC run. In addition to the aliquots containing samples, aliquots (samples) of standard solutions of the active ingredient are also processed to ensure the performance reliability of the instrument.
The various symbols representing the workflow building blocks are given in Fig. 19 .
The workflow shown in Fig. 4 , named HPLCIsocratic , is used when the mobile phase composition is constant. This was adequate for the range of drug products reported on in this paper. The volumetric flowrate and composition of the mobile phase are specified with the MobilePh icon. The following parameters of the Setup subtask of HPLCLIU task define the column characteristics: chemistry, particle size, length, internal diameter, and operating temperature. The SysSuitability subtask, i.e., system suitability, ensures that the system is performing as required before the sample aliquots are analyzed. The material coming out of the column passes throught one of two detectors represented by tasks Detector1 and Detector2 . The outputs from the detectors are stored in separate data files, which are stored on a dedicated server at LIU, and the paths to the files are stored in associated data nodes DA2LIUWk and SDWLIUWk . The Export subtask of the ExpDataLIUWk task executes the Empower 3 software, using the data files from the detectors as input. The program creates a text file which is stored on the server at LIU, and is also uploaded to pharmahub using the data node ExpData . The Sync subtask of the Sync task executes a special purpose PHP program which performs several data curation steps (discussed later), creating a file which is stored on pharmahub and is accessible through the SampDataPHUB data node.
Workflow Development
The development of workflow templates discussed above and in the appendix represents a significant initial investment of resources to ensure that all important steps are accurately captured in the top level graphical networks, and fields are defined for all the important values to be recorded in a workflow.
The workflow editor tool of pharmahub was used for creating all the graphical networks and defining the sets of parameter keywords with the workflow building blocks. The final versions of workflows represent several hours of brainstorming sessions, several webex conferences, and several versions created along the way. It can be safely said that all members of the experimental and data analysis groups agreed on the level of detail in the workflows, and that nothing important has been overlooked or is missing. For example, in the HPLCIsocratic workflow, specific data nodes were added to accommodate the requirement of LIU that the data are automatically backed up on its server. The data nodes with names ending in LIUWk are used for storing the paths to files on the LIU server thereby providing complete reference to raw data. The files uploaded to pharmahub (data node ExpData ) and created by server side programs (data node SampDataPHUB ) represent curated data.
Special Tables
The following project specific tables were added to KProMS to address the particular needs of this project: DrugSubstance and DrugProduct. In addition, the MaterialLot table was modified to accommodate additional attributes associated with drug product lots in pharmaceutical industry. All tables have an attribute named ID, which is a unique integer assigned automatically to each row in the table. Also, if an attribute name has ID at end, its value points to a row (foreign key) in the table whose name is embedded in the attribute name.
The DrugSubstance table stores information about the actives associated with various drug products, and has the following attributes: ID, Name, Active and BCS. The Name attribute is typically the commercial name of the active compound, for example, Atenolol. The Active attribute identifies the chemical which is the therapeutic substance. The BCS attribute is Biopharmaceutics Classification System class to which the drug substance belongs. The possible values are I, II, III, and IV.
The DrugProduct table stores information about drug products, and has the following attributes: ID, Name, DrugSubstanceID, DosageForm, RelMechanism, Dose, DosageFormWt. The Name attribute has a unique value and provides a more familiar identifier than an integer. For example, a combination of Drug Substance name and Vendor, such as Atenolol YYY where YYY is the vendor name. DrugSubstanceID attribute is the ID of the row selected from the DrugSubstance table (a Foreign Key in a database). DosageForm attribute defines the form in which the drug product is delivered, for example tablet, capsule etc. The RelMechanism attribute defines the drug release mechanism, for example, immediate release, delayed release, etc. The Dose attribute defines the mass of active in the associated drug product, for example, 50 mg, 500 mg etc. The DosageFormWt defines the net weight, active plus excipients, of the associated drug product form.
The MaterialLot table stores information about all the material lots procured for the study, and has the following attributes: ID, LotIdentifier, DrugProductID, PackSize, NumberOfPacks, ExpirationDate, LotNumber. LotIdentifier is an identifier, created by the user based on a naming convention (explained later), to uniquely identify each lot. DrugProductID attribute is the ID of the row selected from the DrugProduct table. PackSize attribute is the number of tablets/capsules in each pack procured. NumberOfPacks attribute is the number of packs procured from the associated lot. ExpirationDate attribute is the expiration date for the material from the associated lot. LotNumber attribute is the unique identifier assigned to the associated lot. The manufacturers are required to print the expiration date and lot number on the packaging.
A lot identifier created by the user has the following structure, XXX_YYY_LZZ, where XXX is a drug substance identifier, YYY is a manufacturer identifier and ZZ is a maximum 2-digit lot index. Thus, the lot identifier ATN_YYY_L1 means Atenolol drug substance, YYY manufacturer, lot number 1. A lot identifier is always included in specifying the Purpose parameter associated with each workflow instance. Since a lot number is a key identifier of the material lot, the associated lot identifier becomes the key identifier for all analytical tests recorded in the repository.
Experimental Data Generation
The analytical tests performed by the LIU group generated the raw data used in this study. For each drug product lot procured, the following tests/procedures were performed: 10 sample preparations for measuring content uniformity, one HPLCIsocratic run for the 10 samples, two dissolution runs each using 6 tablets, 10 different tablets were used for measuring each physical property.
To record each sample preparation, one instance of the SamplePrep template was created. The value of the Purpose parameter consisted of the lot identifier and tablet index. For example, ATN_YYY_L1T2 means sample prepared from tablet 2 of the lot identifier ATN_YYY_L1.
The HPLC instrument manufactured by Waters Corp. (Alliance system with PDA or dual wavelength UV detectors, and Empower 3 software) was used for measuring the content uniformity. To record HPLC analysis, 1 instance of the HPLCIsocratic template was created. The value of the Purpose parameter was the lot identifier. The output file from the execution of Empower 3 program was uploaded to pharmahub in the ExpData data node.
The dissolution instrument manufactured by Distek, Inc. (Symphony 7100 bathless dissolution system with the Opt-Dis 405 - fiber optic in situ UV probes) was used for studying dissolution of tablets. To record dissolution, one instance of the DissolutionExpt template was created for each run. The value of the Purpose parameter was the lot identifier appended with either R1 or R2 , for run 1 or run 2, respectively. The report generated by the OPT-DISS software provided by the Distek company was uploaded to pharmahub in the ReportPHUB data node.
All physical properties measured for tablets from one lot were recorded in one instance of the PhysicalProperties template. The ten values of weight, thickness and hardness properties were entered in a table with three columns, and the file was uploaded to pharmahub in the PPDataPHUB data node.
The key analytical instruments and equipment used in various experiments are given in Table 1 . Table 1 Analytical instruments and equipment used for various experiments Experiment Instrument/equipment Sample prep Volumetric flask HPLC Two HPLCs, one UPLC Dissolution OPT-DISS Weight Analytical balance Thickness Vernier caliper Hardness Hardness testing
The analytical experiments as well as the recording of information on pharmahub were done by the LIU group using the KProMS software on desktop PC and Chrome browser. Since the structure of workflow templates does not change, macros can be created to automate the data entry through the use of software from Auto Keybot. As a first step, the sequence in which the specific keys are pressed in traversing a workflow is captured. The values of various parameters are specified in a text file in the order in which they are entered during the workflow traversal. The Auto Keybot software uses the workflow traversal and text file to match fields with corresponding values to populate an instance.
For the study reported in this paper, a total of 1300 workflow instances have been created in the repository. The system is currently in use for the ongoing work and the number represents the current population of the database.
Data Curation
A repository created and managed through the use of KProMS for this project satisfies the requirements of an Open Archival Information System (OAIS) archive [ ]. The OAIS model has six functional entities: Ingest, archival storage, data management, administration, preservation and access. How these functions are accomplished within KProMS is discussed in this section. Also, special functionalities developed for ensuring data quality and generating master records for a condensed status of repository contents are discussed.
Ingest
Ingest is the set of processes responsible for accepting information submitted by producers and preparing it for inclusion in the archival repository. Specific functions performed by Ingest include receipt of information, validation that the information received is uncorrupted and complete, transformation of the submitted information into a suitable form if necessary, and extraction and/or creation of descriptive metadata.
The previous section described how the experimental data are uploaded to the hub repository using workflow templates. Various steps are taken to validate the data before it is uploaded to pharmahub .
Data Verification
The data in the input file for Auto Keybot is verified by a supervisor prior to processing. The data file uploaded with a physical properties workflow is also checked by a supervisor to ensure there are no errors. The steps to verify data depended on the associated experiment. For physical properties data, the data was uploaded as a spreadsheet, consisting of 10 rows and 3 columns. Such data were manually verified by scanning the uploaded file in the browser. For content uniformity and dissolution experiments, the data were post processed after uploading. The post processing software performed some basic checks such as numericity, range etc. For dissolution experiments, dissolution curves were generated during the data transformation step which provided visual verification of the data.
Data Transformation
The raw data from HPLC is uploaded using the ExpData data node of HPLCIsocratic workflow (Fig. 4 ). The Sync subtask of the Sync task executes a special purpose PHP program, named LinkData, to verify and transform the raw data. The before and after examples of data in the ExpData and SampDataPHUB data nodes are shown in Figs. 5 and 6 , respectively (the manufacturer names have been inentionally edited). Since some of the information in the raw data file is entered manually, the program verifies that for each sample in the SampleName column there is a matching instance of the SamplePrep workflow. Also, for each sample row it extracts the dilution factor from the HPLCIsocratic workflow and adds it to a new column named DilFac. If the program fails, it reports the errors in the raw data file which must be corrected and resubmitted to the repository. Thus, the file in the SampDataPHUB is fully verified and its metadata is known. Fig. 5 Output data file from program Empower3 Fig. 6 Output of the LinkData library program used in the Sync subtask
The raw data file from the Distek apparatus, generated by the OPT-DISS program and uploaded to the repository using the data node ReportPHUB , is unstructured, and as such is not easily machine readable. It consists of lines with one pair of keyword and value on each line, interspersed with tables with time series data from the 6 UV channels in the apparatus.
An interpreter for the Distek report file, named DigData , was pieced together by using functions that were developed to perform the following basic tasks: Assuming that the specified data file contains lines of delimited data, identify the delimiter Skip lines until a specified pattern is detected Extract the keyword and value pair from a line and write it as a comma-separated pair Starting with the current or next line, parse data as delimited tabular data until a line with specified pattern is detected
The ProcessRep subtask of the DissolutionExpt workflow (see Fig. 16 ), parses the report file and creates two outputs, a condensed report of the end condition in the apparatus and a graph of the dissolution profiles generated from the 6 channels as shown in Figs. 7 and 8 , respectively. These outputs are stored in the GrAndSummary data nodes of the DissolutionExpt workflow. Fig. 7 Report of the end condition from a dissolution run Fig. 8 Dissolution profiles from a Distek run
The UV probes which measure absorbance in the Distek apparatus are very delicate and have a tendency to break. Also, they are sensitive to formation of bubbles from dissolved gases. The end condition report and dissolution profiles were used to confirm that the data from a run was of good quality. If a probe breaks during a run, the corresponding profile shows a flat line. The bubble formations manifest as minor excursions from the trend in the profile, and are often not significant. Only qualitative analyses of profiles were performed for accepting or rejecting runs. The interpreter assumes that the input data contains certain recurring patterns. Therefore it is robust as long as the structure of raw data does not change.
The LinkData and DigData programs ensure that the actionable data in the HPLCIsocratic and DissolutionExpt workflow instances are of good quality. Also, since the data are stored as .csv text files, the metadata can be extracted using the KProMS utilities.
Archival Storage
Archival Storage is the portion of the archival system that manages the long-term storage and maintenance of digital materials entrusted to the OAIS. For this project, pharmahub was used as the repository. Once a workflow instance is archived in the repository, the information will remain complete and accessible, as long as the hub is running.
Data Management
The Data Management function maintains databases of descriptive metadata identifying and describing the archived information in support of the OAIS’s finding aids. It also manages the administrative data supporting the OAIS’s internal system operations, such as system performance data or access statistics. The primary functions of Data Management include maintaining the databases for which it is responsible, performing queries on these databases and generating reports in response to requests from other functional entities within the OAIS, and conducting updates to the databases as new information arrives, or existing information is modified or deleted. The pharmahub is hosted by Purdue University’s ITaP organization on a dedicated web site. The administrator of pharmahub is responsible for managing these data and performing the administrative tasks such as regular backups, recovery of data if the server crashes, and updates to the content management system.
In managing these databases, the Data Management function supports search and retrieval of the OAIS’s archived content, and administration of the OAIS’s internal operations.
Preservation Planning
Preservation Planning is responsible for mapping out the OAIS’s preservation strategy, as well as recommending appropriate revisions to this strategy in response to evolving conditions in the OAIS environment. Purdue University’s ITaP organization keeps the HUBzero framework used in pharmahub up to date.
Access
The Access function manages the processes and services by which users locate, request, and receive delivery of items residing in the OAIS’s archival store.
Since all analytical tests performed were lot centric, and all statistical analyses performed were product centric, special functionality was added to KProMS to create master tables which provided references to all experimental workflows associated with a lot, and all analysis workflows associated with a drug product.
Lot Based Master Table
The most up to date version of the lot based master table can be created at any time by invoking a special program through the KProMS GUI. An example of the lot master table is given in Fig. 9 (Values in LotNo, DrugProd and Manuf columns have been intentionally edited). Fig. 9 Example of the lot master table
Each row in the lot master table gives key attributes about each material lot for which completed analytical test workflow instances have been submitted in the repository.
When the toggle associated with a lot, LotNo FW in this case, is clicked the table to the right of the main table is populated. The information about a lot consists of a Show button that opens a summary of results from the associated analytical tests, and clickable links to the individual workflow instances (underlined integers in blue) associated with the tests performed. An example of Lot Summary is shown in Fig. 10 . Fig. 10 An example of lot summary
If any of the instances are clicked, the workflow is displayed in the top right quadrant for browsing.
The normal browsing capability allows the user to select a desirable instance in a hierarchical, context sensitive search.
Product Based Master Table
The most up to date version of the product based master table can be created at any time by invoking a special program through the KProMS GUI. An example of the product master table is given in Fig. 11 (Values in column Comments have been intentionally edited). Fig. 11 Example of the product master table
In this project, several lots of a given drug product produced by different manufactures were procured and analyzed by the LIU group. For the statistical analysis, the analytical data was grouped according to multiple criteria and processed using the R package. Each statistical analysis was recorded in the repository as an instance of the ClientSideR workflow, which is discussed later. The Product Master Record table lists all the statistical analyses performed using the analytical data.
For a given row, the first column identifies the associated drug product, and the integers with hyperlinks in the CU , DSLN , and PP columns are the IDs of the ClientSideR workflow instances that used the data from content uniformity, dissolution and physical properties tests respectively. The Comments column specifies the secondary criterion used for grouping data. For example, consider the first row in Fig. 10 . The value of ATN in the DrugProd column means it is for the drug product Atenolol. The value of XYZ in the Comments column means all Atenolol lots made by the XYZ manufacturer were used in the analysis. Instance with ID 1862 uses the data from all HPLCIsocratic instances (value in the CU column), instance ID 1868 uses the data from all DissolutionExpt instances (value in the DSLN column), and instance 1863 uses the data from all PhysicalProperties instances (value in the PP column). Clicking on the ID opens the ClientSideR instance in the top right quadrant of the KProMS GUI.
Administration
The Administration function is responsible for managing the day-to-day operations of the OAIS, as well as coordinating the activities of the other five high-level OAIS functional entities. Other responsibilities include interacting with users. The Administration function is also responsible for overseeing the operation of the archiving and access systems, monitoring system performance, and coordinating updates to the system as appropriate. Purdue University’s ITaP organization performs the administrative functions for pharmahub .
Data Analysis
The main purpose for generating all of the product lot quality data was to use it in statistical analysis. In this section, the use of workflow-based approach for recording each analysis stored in the repository is described.
The first step in any data analysis is of course to extract the required data and structure it so that the analysis program can read it. In that regard, KProMS provided the complete provenance of the information stored in the repository, which facilitated data extraction. Each piece of information stored in the repository using KProMS is accessible with a unique tuple. For example, a task parameter is identified by the triplet {workflow name, task name, parameter keyword}, a subtask parameter by the 4-tuple {workflow name, task name, subtask name, parameter keyword}, a column in a data file by the 4-tuple {Workflow name, data node name, parameter keyword, column name}, and so on [ ].
The BAnaOnpcDSLN workflow, shown in Fig. 12 , was used for recording all data analyses performed with the dissolution data. Fig. 12 Workflow used for analyzing dissolution data
The Select data node is used for specifying the data to be extracted. In this case, the entire data file stored in the ReportPHUB data node is extracted for all the selected instances of the DissolutionExpt workflow. An example of a fully specified form for selecting data is shown in Fig. 13 . Since the primary reference point for any data in the repository is the associated workflow, first the template of the associated workflow is selected, DissolutionExpt in this case. Next, the instances from which data are extracted are selected. The form for selecting instances is shown in Fig. 14 (values in the Purpose and Perfrmd by columns have been intentionally edited). Since the value of the Purpose parameter of each workflow instance is according to a convention described earlier, it is used as a filter to narrow down the number of available choices. For example, if ATN_XYZ* is entered in the ‘Purpose has’ filter, all dissolution experiment instances based on Atenolol manufactured by XYZ appear in the selection list. To select all, the box in the header of the first column can be checked. To select dissolution experiment instances based on Atenolol from all manufacturers, ATN* is entered in the filter. Fig. 13 Form for selecting data for extraction Fig. 14 Form for selecting workflow instances for data extraction
Similarly, the specification of a tuple which identifies the data to be extracted is facilitated by exploiting the relationships defined by the underlying workflow. The Workflow field in Fig. 13 has the value of the selected template. The choice list for the WfObjLvl1 field consists of the children of the associated Workflow icon, namely, tasks, material sources and data nodes. Once the value of this field is selected, the choice list of WfObjLvl2 consists of the children of the WfObjLvl1 selection. Finally, the choice list of the Parameter field consists of the parameters associated with the selection identified by the three fields Workflow, WfObjLvl1 and WfObjLvl2. For the analysis of dissolution data, the entire file in the ReportPHUB data node is selected because its metadata is not known.
The MergeFile subtask uses the special interpreter, named DigData, discussed earlier. For this subtask the output directive is ‘Split and zip’, that is, a predefined list of parameters and tables from the report file are extracted from the specified instances, and a compressed zip file is created and stored in the IntFile data node.
The DoR subtask has only one parameter to record the name of the analysis package used. The analysis is performed on a user’s PC. First the zip file in the IntFile node is downloaded on the PC, the R package is executed, and the results are uploaded to the Results data node.
The BAnaOnpcCUPP workflow shown in Fig. 18 , was used for recording all data analyses performed using the content uniformity or physical properties data. The structure of this workflow is the same as the BAnaOnpcDSLN workflow. In the Select data node the entire file in the SampDataPHUB of the HPLCIsocratic workflow, or the PPDataPHUB data node of the PhysicalProperties workflow are selected. The ‘Extract Data’ library model is used for the MergeFiles subtask (see Fig. 18 ) which catenates all the files and stores it in the IntFile data node. The analysis is performed by downloading the catenated data file on a PC, using the appropriate R package and uploading the results.
An example of the use of R to estimate between (inter) and within (intra) lot variability is described here. A special purpose R package was used in this analysis. In addition to textual information such as mean and standard deviation, the output included the change in between/within lot variabilities based on the number of lots analyzed and tolerance intervals. Figure 15 shows tolerance intervals of content uniformity (CU) test based on the data from six lots. Solid red indicated 90/99 (90% confidence/99% coverage) parametric tolerance interval while dashed red indicates 90/99 non-parametric tolerance interval. Green circles indicated actual observations. Box and whisker diagrams are shown for each lot. API amount measurements for each tablet are in % of label claim (LC). Fig. 15 Tolerance intervals for content uniformity using six lots
The output from R is stored as a pdf file in the repository and is accessed through the Results data node.
Conclusions
The information and knowledge generated from the analytical data and statistical analysis in this project were curated using a workflow-based knowledge provenance management system, KProMS, which uses the HUBzero framework. The workflows for the associated SOPs of tests and computational steps in the statistical analysis provided the complete context for the underlying data. Since it is a web based system, geographically distributed groups, each with different roles in the project, were able to create and share information available in the repository. KProMS satisfies the requirements of an Open Archival Information System (OAIS) archive and can be published as is, if so desired.
This paper describes a specific application of a very general purpose workflow-based framework. At its core are the abilities to model explicitly and graphically the logical relationships between any piece of information and its context. The graphical model in turn facilitates sharing and harvesting of the associated knowledge. This core capability can be beneficial across a wide range of applications and domains. For example, the ability to share knowledge is crucial in all aspects of product lifecycle management, such as process development, scale-up, process improvement, trouble shooting and so on.