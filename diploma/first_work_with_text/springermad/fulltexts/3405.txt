Introduction
Since the dawn of the new millennium, the field of toxicology is reorienting itself, investigating toxicological problems using new approaches. After the initial observations like lethal dose determination, differences in body weight, and macroscopic and histopathological “end-point” alterations in the organism, new principles aim for a more mechanistic interpretation of inflicted hazards (Stierum et al. ; Ramirez et al. , ; Vinken ). The mechanisms involved in adverse reactions to xenobiotics are complex, often involving multiple steps and/or organ systems. Understanding these processes rather than describing their final outcome is an important trend in the assessment of potential toxicological hazards.
The liver is a multifunctional organ, and its high metabolic rate and biotransformation capacity make it very vulnerable for both metabolically induced, as well as drug-induced liver injury (DILI) (Rang et al. ; Willebrords et al. ). Non-alcoholic fatty liver disease (NAFLD), which is often seen as a manifestation of the metabolic syndrome, is of growing concern, and has a prevalence of up to 25% in Western countries (Younossi et al. ). Although DILI contribution to the NAFLD population is limited, many drugs have an important impact on steatosis progression (Dash et al. ). DILI poses a very important threat and is of primary concern in safety testing of new chemical entities, since it is the most frequently reported specific target of toxicity (Holt and Ju ). The occurrence of DILI in drug development results in high attrition rates and even post-market drug withdrawal (Chen et al. ). Therefore, good in vitro methodologies could help in identifying potential hepatotoxic compounds early during the drug development process, reducing health hazards, and improving cost-efficiencies for the industry (Ramirez et al. ; Usui et al. ; Van Summeren et al. ).
The rise of omics techniques during the last decades, such as genomics (1990s), transcriptomics and proteomics (2000s), and metabolomics (2010s) allows an unprecedented systematic screening of different genes, proteins and metabolites. These approaches generate large amounts of data that represent the complex regulations involved in the endogenous metabolism (Ramirez et al. ; Balcke et al. ). The integration of biochemical profiles provides a perspective on the distortions in the cell, and allows to generate hypotheses regarding the modes of action (MOA) involved in the toxicological outcome (Vinken ).
Metabolomics is defined as the study of the biochemical profile at the small molecule level in an organism (Nicholson et al. ). Because the metabolome is the most downstream level in the biomolecular organisation of a system, metabolomics fingerprints are very dynamic and alterations may be induced even by tiny initial external triggers (Ramirez et al. ; Balcke et al. ). The metabolome fingerprint provides a rich data set that not only could help to characterise the toxicological outcome, but also the potential mechanisms involved (Ramirez et al. ).
Although metabolomics was initially mainly applied to in vivo studies to search for valuable mechanistic biomarkers, its applications are expanding towards in vitro methodologies (Ruiz-Aracama et al. ). Especially for compound safety evaluation, a shift can be observed from in vivo to in vitro toxicology. Indeed, the use of animals in research is a topic of debate in many countries (European Medicines Agency ). Ethical concerns, interspecies differences, high costs, and time investments of such in vivo experiments are important reasons to shift from animal methods to in vitro and in silico alternatives (European Medicines Agency ; ECHA ; Williams et al. ; Rovida ; Godoy et al. ). Other advantages of in vitro models are their reduced complexity, allowing a specific focus on single mechanisms. A general low variability within each model also reduces noise and biases, while improving throughput (Ramirez et al. ; Godoy et al. ).
This review focuses on in vitro investigations of hepatotoxicity using experimental metabolomics set-ups. The earliest articles date from 2011; the metabolomic research in in vitro hepatotoxicity is gaining momentum starting from 2014, with 17 of the 28 papers published after 2015. With the growing interest in metabolomics as a novel approach for the in vitro investigation of hepatotoxicity, this is an ideal moment to review the current methods, comparing them with good metabolomics practices, and to reveal which metabolites are consistently altered among the different articles and could, therefore, serve as potential markers of hepatotoxicity.
Study designs
Hepatic in vitro systems
Careful design of the experiment is vital for a successful metabolomics application. Due to the inherent characteristics of each cell type and culture platform, there are similarities and differences in comparison to the in vivo situation. The selection of the cell type is vital to the outcome of the experiment and its implications should carefully be considered when designing the experiment. Two comparative articles show the different metabolic capacity of various cells. Kim et al. ( ) investigated the difference between human adult and fetal primary hepatocytes, observing differences in the amino acid metabolism, the glycolysis and the citric acid cycle, and in the urea cycle. Their interpretations state that the fetal hepatocytes rely on glycolytic activity because of mitochondrial inactivation, in contrast to mature cells which show gluconeogenetic activity, while they rely on lipogenic beta-oxidation (Kim et al. ). Sugiyama et al. ( ) compared steatogenic principle of HCV-infected Huh-7 cells, resulting in increased inflammatory intermediates, sugar and amino acid uptake, and inhibited cholesterol synthesis.
The HepG2 cell line is the most popular cell model, being applied in ± 35% of the studies (Ramirez et al. ; Ruiz-Aracama et al. ; Snouber et al. ; Zhang et al. ; Li et al. ; García-Cañaveras et al. ; Chatterjee et al. ; Yu et al. ; Meissen et al. ; Van den Hof et al. ), followed by primary hepatocytes human (10%) (Kim et al. ; Vorrink et al. ), rodent (10%) (Carretero et al. ; Wang et al. ; Toyoda et al. ), and fish (10%) (Olsvik et al. , ; Søfteland et al. ). The list is completed with L02 (10%) (Liu et al. ; Shi et al. ; Li et al. ), HepaRG (10%) (Brown et al. ; Van den Eede et al. ; Cuykx et al. , ), Huh-7 (10%) (Sugiyama et al. ; Seeßle et al. ), murine hepatoma Hepa1c1c7 (Kalkhof et al. ), and V79-fibroblasts (Balcke et al. ) (Table 1 ). Primary human hepatocytes are considered to be the golden standard for in vitro hepatotoxicity studies, but they have a high interdonor variability, low availability, and limited lifespan and metabolic stability in vitro (Willebrords et al. ; Godoy et al. ). Rodent hepatic cell cultures offer a cheap alternative to scarce primary human hepatocytes, but they have the same scientific drawbacks as their human counterparts. Furthermore, the use of rodent hepatocytes does not account for interspecies differences (Godoy et al. ). Søfteland and Olsvik exposed fish hepatocytes to pesticides, bisphenol A, polyaromated hydrocarbons, and genistein in the context of agricultural applications (Olsvik et al. , ; Søfteland et al. ). Although interspecies differences between fish and human hepatic metabolism are even more pronounced than between rat and human primary hepatocytes, information resulting from in vitro experiments with animal primary hepatocytes can still be useful when common metabolic pathways are investigated.
Table 1 Culture conditions applied during in vitro experiments Article Year Cell type Culture type Tested compounds Viability assay Exposure Concentration Concentrations Replicates References Balcke 2011 V79 fibroblast 6-well plate Oxidative uncouplers: KCN, sodium-azide, dinitrofenol, trifluoromethoxyphenylhydrazone MTT 1–24 h IC20 and IC50 10 µM (FCPP) to 500 µM (DNP) 1 mM (KCN/NaN 3 ) 5 Balcke et al. ( ) Ruiz-Aracama 2011 HepG2 cell line T75 flask TCDD MTT 48 h IC20 10 nM 5 × 5 Ruiz-Aracama et al. ( ) Snouber 2012 HepG2 cell line Biochip: 2 cm 2 Flutamide N/A 48 h Plasma C 10 µM 3 Snouber et al. ( ) Brown 2013 HepaRG cell line 12-well plate Palmitate and oleate N/A 48 h N/R 0.5 mM 3 Brown et al. ( ) Carretero 2014 Primary rat hepatocytes 6-well plate Cumene hydroperoxide N/R 24 h N/R 500 µM 6 Carretero et al. ( ) Kalkhof 2014 Murine Hepatoma Hepa1c1c7 Petri dish 15 cm BenzoApyrene NRU 7 pt–24 h Subacute—IC20 50 nM – 5 µM 3 Kalkhof et al. ( ) Kim 2014 Human adult vs fetus hepatocytes Equivalent of 2 million cells Comparison of control conditions N/A N/R N/A1 N/A 8 Kim et al. ( ) Liu 2014 L02 cell line 10 cm petri dish Cisplatin CCK8 24 h Hormetic C and inhibitory C 1 nM–1 µM–1 mM 8 Liu et al. ( ) Søfteland 2014 Primary salmon hepatocytes 6-well plate PAH/pesticides impedance 48 h IC30 1 µM–100 µM 5 Søfteland et al. ( ) Sugiyama 2014 HPI vs huh-7 cell line Equivalent of 1 million cells Comparison of cell types N/A N/R N/A N/A N/R Sugiyama et al. ( ) Li 2015 HepG2 cell line T75 flask Matrine N/R 3 pt 48 h N/A 0.2–0.4–0.7 µM 5-4-3 Li et al. ( ) Meissen 2015 HepG2 cell line 12-well plate Glucose vs fructose N/A 5 pt 48 h N/A 5 mM 6 Meissen et al. ( ) SeeßLe 2015 Huh-7 cell line 12-well plate Palmitate N/A 8 h N/A 300 µM 6 Seeßle et al. ( ) Van den Eede 2015 HepaRG cell line 12-well plate Triphenylphosphate Acetaminophen MTT 72 h IC10 50 µM – 2 mM 6 Van den Eede et al. ( ) Van den Hof 2015 HepG2 cell line T75 flask Cyclosporine A MTT 24–72 h IC20 3–20 µM 5 Van den Hof et al. ( ) Wang 2015 Primary rat hepatocytes 12-well plate Hexaconazole MTT 24 h N/R 2 µM 3 Wang et al. ( ) Zhang 2015 HepG2 cell line 6-well plate HBCD and DMBA MTT 24 h 1/10 of IC50 4–10 µM 6 Zhang et al. ( ) Garcìa-cañaveras 2016 HepG2 cell line 6-well plate Different drugs: toxic and non-toxic MTT 24 h 1/10 IC10–IC10 µM-to-Mm range 3 García-Cañaveras et al. ( ) Shi 2016 L02 cell line 6-well plate Pekinenal MTT 48 h Non-toxic C vs IC20 and IC30 5–10–20 µM 6 Shi et al. ( ) Chatterjee 2017 HepG2 cell line 6-well plate Nanotubes ez-cytox 24 h IC20 2–10 mg/L N/R Chatterjee et al. ( ) Olsvik 2017 Primary fish hepatocytes 6-well plate Pirimiphos-methyl MTT 48 h Not cytotoxic 0.1–1 mM 6 Olsvik et al. ( ) Olsvik 2017 Primary salmon hepatocytes 6-well plate BPA and genistein MTT 48 h Not cytotoxic 100 µM 5 Olsvik et al. ( ) Ramirez 2017 HepG2 cell line Petri dish 35 mm Different hepatotoxicants WST-1 48 h I1/3 IC20–IC20 10 µm–1 mM 8 Ramirez et al. ( ) Toyoda 2017 Primary mouse hepatocytes 6-well plate Acetaminophen WST-1 12 h IC90 10 mM 4 Toyoda et al. ( ) vorrink 2017 Primary human hepatocytes 12-well vs 3D culture Comparison of control conditions N/A N/A N/A N/A 9 Vorrink et al. ( ) Yu 2017 HepG2 cell line Petri dish 10 cm Cationic liposomes CCK8 24 h IC50 120 µg/mL 6 Yu et al. ( ) Li 2017 L02 cell line Petri dish 10 cm Trichloropropyl phosphate CCK8 24 h Hormesis C and toxic C 1–100 µM 8 Li et al. ( ) Cuykx 2018 HepaRG cell line Chamber slides 4 cm 2 Valproic acid NRU 24–72 h IC10 and 1/10 IC10 66.5–665 µM 6 Cuykx et al. ( )
Immortalised cell lines, such as the HepG2, L02, Huh-7, and HepaRG cell lines, are alternative human first-screening tools because of their low variability (Godoy et al. ). However, HepG2 and Huh-7 have a poor biotransformation capacity and are not stable in long-term 2D cultures (Godoy et al. ; Aninat et al. ). HepaRG cells have biotransformation capacity and are genetically more stable than the other hepatic cell lines (Antherieu et al. ; Lubberstedt et al. ). Their ability to differentiate both in hepatocyte-like and biliary cells allows to detect cholestatic effects (Ramirez et al. ; Guillouzo et al. ; Rodrigues et al. ). The downside of this cell line is the fact that only undifferentiated cells proliferate and hepatic differentiation is necessary, thereby introducing an additional variation that in turn reduces the reproducibility of metabolomics measurements (Ramirez et al. ).
In vitro metabolomics experiments need a relative large amount of cell material when compared to most trivial cellular assays. Cell cultures are usually cultured in 6-well plates (35%); but larger surfaces, such as T75 flasks and petri dishes (30%), are also popular. An alternative is the 12-well plate, which is used to economise on culture material (21%). 2D monolayer cultures are the standard procedures and facilitate cell scraping as harvesting method. Adjustments to improve cell harvesting exist, such as the use of carrier glasses (Martano et al. ), chamber slides (Cuykx et al. ), and membrane carriers (Balcke et al. ; Bordag et al. ), but they are not widely applied (Ramirez et al. ; Balcke et al. ; Cuykx et al. ). Advanced culture platforms, such as biochips and 3D cultures, can be incorporated to improve the in vitro–in vivo simulation, but their applications in metabolomics are still limited (Godoy et al. ; Snouber et al. ; Vorrink et al. ). Vorrink et al. ( ) investigated the stability of primary human hepatocytes in 3D spheroid cultures, and observed stable levels of ornithine, cholic acids, acyl carnitines, and citric acid as well as improved reduced and oxidised glutathione (GSH/GSSG) ratios.
Cell supernatants are analysed by Snouber et al. ( ), Wang et al. ( ), Ramirez et al. ( ), Vorrink et al. ( ), and Toyoda et al. ( ). The main advantage of sampling cell supernatant is the non-invasive procedure, which gives the opportunity to investigate the evolution of metabolome of the same cell unit at different time-points. However, the supernatant does not necessarily represent all intracellular alterations, and extrapolations to intracellular mechanisms should be considered carefully. Ramirez et al. ( ) observed that the investigated toxicological MOAs invoked more specific changes in the intracellular metabolome than in the extracellular medium.
Experimental design
A comprehensive overview of all experimental designs is given in Table 1 . The standard in vitro metabolomics setting involves the study of a single compound which is administered in different concentrations and time points. García-Cañaveras et al. ( ), García-Cañaveras et al. ( ), and Ramirez et al. ( ) reported multiple experiments including different hepatotoxicants with the ultimate goal to build classifier models able to distinguish hepatotoxic compounds according to different modes of action. To increase the biochemical impact of steatotic agents on the lipid metabolism, García-Cañaveras et al. ( ) applied lipid loading before exposure, resulting in more distinct metabolic profiles. Søfteland et al. ( ) applied a full-factorial exposure design of polycyclic aromatic hydrocarbons (PAH) and pesticides to determine synergistic hepatotoxicity in fish. Brown et al. ( ), Seeßle et al. ( ), and Meissen et al. ( ) investigated dietary impacts on the hepatic metabolome by exposure to palmitate/oleate and glucose/fructose, respectively.
Exposure times
The time-frame in in vitro metabolomics experiments is short, with 24 h (48%) and 48 h (44%) accounting for the majority of time-points. Shorter time-frames have also been considered (24%), often in combination with multiple time-point investigations to observe time-related differences, with the final end-point at 24 or 48 h (Balcke et al. ; Li et al. ; Meissen et al. ; Van den Hof et al. ; Kalkhof et al. ). A longer time-frame (72 h) is considered in a minority of the applications (12%) to evaluate sub-chronic effects or to increase the impact of the MOA on the metabolome, resulting in more pronounced differences (Van den Hof et al. ; Van den Eede et al. ; Cuykx et al. ).
Testing concentrations
Determination of the concentrations to be used is performed by viability assays, which is reported in 85% of the applications concerning xenobiotic toxicity. The most popular assay (88%) applies a bioconversion of 3-(4,5-dimethylthiazol-2-yl)-2,5-diphenyltetrazolium bromide (MTT) or water soluble tetrazolium salts (WST-1 and CCK-8) through the mitochondrial activity and NADPH flux as a projection of cell viability (Sladowski et al. ; Ishiyama et al. ). However, these bioconversion assays are biased towards lower concentrations, since these conversions are linked to mitochondrial metabolism pathways, which can be specifically inhibited by the tested compound, thereby providing a biased indication of cell viability. The neutral red uptake assay, which is based on lysosomal storage of the cationic dye, and cell adhesion/impedance assays are alternative method often applied when the energy metabolism is suspected to be part of the toxicant’s MOA (Zhang et al. , ).
The determined maximal concentrations to be applied are at sub-cytotoxic levels, usually determined as the inhibitory concentration (IC) 20, i.e., the concentration where the exposed concentration leads to 20% less viable cells. Although low IC values are considered to be sub-cytotoxic, the toxicological insult can be considered to be significant. As a result, early onset alterations can be missed in such investigations. Toxic concentrations, such as the IC30 by Søfteland et al. ( ) or Chatterjee et al. ( ) and the IC50 by Balcke et al. ( ) or Yu et al. ( ), have such a strong impact that only alterations related to general toxicity can be observed, impeding mechanistic fingerprinting. Combining a high- and low-dose exposure is recommended to investigate the trends of multiple markers of toxicity through the toxicological process; a second exposure at lower dosages (e.g., one-tenth of the reported IC concentration) is often applied to investigate metabolic alterations before any cell death occurs.
Number of replicates
The number of replicates is often limited, with 48% of the experiments reporting more than six replicates for a defined exposure window. Increasing the number of replicates allows for a more accurate reflection of the true distribution of the metabolites within a tested class, improving the statistical power of the experiment. In addition, the use of cell lines instead of primary cells improves statistical power as they originate from the same donor and, hence, are less prone to biological variability (Godoy et al. ; Madji Hounoum et al. ). Even with the reduced biological variation, enough replicates should still be considered to account for the analytical/technical variation. According to Sumner et al. ( ) and Martano et al. ( ), at least three replicates and preferably > five replicates are required for reliable in vitro metabolomics applications.
Randomisation
Four publications explicitly mention a randomisation of their samples during processing and/or LC–MS analysis (Ruiz-Aracama et al. ; García-Cañaveras et al. ; Van den Eede et al. ; Cuykx et al. ). Although the effect of population bias (e.g., gender, age, diet, etc.) is not prevalent in in vitro research using cell lines or pooled primary cells, biases can still be introduced through processing order (e.g., seeding, degradation during sample processing, and the loss of sensitivity because of time-dependent source contamination). The randomisation of all the samples during exposure and sample preparation, and LC–MS injection is an important step in untargeted metabolomics to remove potential correlations between exposure groups and non-controlled experimental factors and to prevent technical/instrumental biases (Dunn et al. ; Broadhurst and Kell ). Randomisation can be performed blindly over the entire data set or can be constrained to ensure an equal distribution of the exposure categories over the entire handling process (Jonsson et al. ).
Combination with other assays
Metabolomics can be applied as a stand-alone approach to obtain information regarding metabolic pathways involved in the MOA of the toxicological insult. Yet, the combination of metabolomics with other assays generates an additional information, which improves the biochemical interpretation. Functional assays (23%) and stainings (12%) can directly link histological and biochemical outcomes with the obtained information from the metabolomics experiment (Sugiyama et al. ; Snouber et al. ; Chatterjee et al. ; Toyoda et al. ; Shi et al. ; Brown et al. ; Seeßle et al. ).
The combination of metabolomics with other omics to provide a systems biology perspective is a powerful approach, and the technique is often applied to in vitro hepatotoxicity. Metabolomics is mostly combined with transcriptomics (30%), followed by genomics (15%) and proteomics (4%) (Kim et al. ; Sugiyama et al. ; Zhang et al. ; Li et al. ; Chatterjee et al. ; Van den Hof et al. ; Vorrink et al. ; Olsvik et al. ; Søfteland et al. ; Seeßle et al. ; Kalkhof et al. ). A recent report from Rodrigues et al. ( ) combined transcriptomics, proteomics, and metabolomics in vitro data to identify drug-induced cellular responses involved in cholestasis and validate a previously developed AOP for drug-induced cholestasis. Alterations in both the transcriptome and the metabolome are a good indication of the pathways involved in the toxicological mechanism. Combining the outcomes of both omics levels with pathway analysis approaches, the metabolic accumulations and depletions can be interpreted together with the cellular responses to these differences in the intracellular homeostasis. This strategy assists in revealing the different metabolic pathways involved in the MOA, improving the final biological interpretation.
Analytical methods
Sample preparation procedures and data-acquisition methods are crucial parts of the analytical workflow. Many methods have been developed for different metabolomics applications to extract and separate the metabolome as efficiently as possible (Madji Hounoum et al. ; Lorenz et al. ; Cuykx et al. ; García-Cañaveras et al. ). The extraction of intracellular metabolites has been widely investigated, comparing culture carriers (Martano et al. ; Cuykx et al. ; Bordag et al. ) and extraction solvents (Lorenz et al. ; García-Cañaveras et al. ; Matyash et al. ; Bi et al. ). The chemical space of metabolites is vast and there is consensus that one optimal method for all metabolites does not exist (Cuykx et al. ; Leon et al. ). Although generic analytical methods are often applied, the development of a protocol according to state-of-the-art recommendations yields higher results concerning metabolic coverage, accuracy, and precision than the application of a standard non-tailored protocol (Leon et al. ; Hayton et al. ).
The discussion of the method section of all articles is divided in sections describing the pre-analytical and instrumental phase followed by the data processing and statistical analysis and identification strategies. A comprehensive overview is shown in Table 2 .
Table 2 Pre-analytical design and instrumental platforms applied for in vitro hepatic metabolomics Quenching Extraction Platform Data-preparation Normalisation QC Data analysis References Cold ethanol/DCM DCM–ETOH–water IP–UPLC–QQQ–MS Log transformation Pareto scale Protein determination N/R PLS-DA Balcke et al. ( ) Cold isotone water/ultrasonication Chloroform–water 400 MHz NMR; GC–MS and RP-UPLC–TOF-MS Log transformation Phospholipid signal NMR N/R ANOVA PCA Ruiz-Aracama et al. ( ) Not applicable Protein precipitation 800 MHz NMR Manual fitting Cell number N/R MWU PLS-DA Snouber et al. ( ) Methanol-room temperature Protein precipitation GC and RP-LC–IT–MSMS Log transformation Protein determination N/R Welch t test/ANOVA + FDR PCA Brown et al. ( ) Liquid nitrogen Protein precipitation RP-LC TOF Quantified N/R Stock solutions MWU Carretero et al. ( ) Methanol/water Protein precipitation RP-LC–QTRAP Quantified Internal standards N/R Wilcoxon + FDR Kalkhof et al. ( ) Hypertonic solution Chloroform–methanol–water CE-TOF-MS Internal standard ratio Cell number N/R Welch t test PCA Kim et al. ( ) Cold PBS Chloroform–methanol–water 500 MHz NMR N/R Mean centered, total area N/R ANOVA PCA and PLS-DA Liu et al. ( ) Lyophilisation Chloroform–methanol–water FTICR and 600 MHz NMR Filter MV imputation Log transformation Mean centered, PQN QC pool ANOVA + FDR PCA and PLS-DA Søfteland et al. ( ) Methanol Protein precipitation Two platform CE/LC–TOF-MS Fold change N/R N/R PCA and HCA Sugiyama et al. ( ) Trypsinisation Chloroform–methanol–water 400 MHz NMR PCA-outlier removal Pareto scale N/R N/R ANOVA + FDR OPLS Li et al. ( ) Cold PBS/methanol Protein precipitation HILIC–LC and GC TOF Quantified Average sum of intensities N/R Differential (univariate) statistics PLS Meissen et al. ( ) Freeze–thaw cycles Methylation for GC GC–Q-MS Not applied N/R N/R ANOVA + FDR Seeßle et al. ( ) Liquid nitrogen Chloroform–methanol–water RP-UPLC–QTOF Filter Log transformation Mean of QC QC pools and blanks t test + FDR Van den Eede et al. ( ) Cold isotone water Chloroform–water NMR + RP-LC–QQQ-MS Log transformation PQN N/R t test Van den Hof et al. ( ) Cooled Protein precipitation RP-LC–QQQ-MS Not applied N/R N/R t test Wang et al. ( ) Methanol/water DCM–methanol–water DI-FTICR Filters MV imputation Log transformation PQN N/R ANOVA + FDR PCA Zhang et al. ( ) Liquid nitrogen Chloroform–methanol–water 3 LC–QTOF-MS platforms Log transformation Unit variance scaled Mean centered Protein determination QC pools and blanks ANOVA + FDR PCA–PLS-DA García-Cañaveras et al. ( ) Freeze–thaw cycles Protein precipitation RP-UPLC–QTOF–MS Markerlynx Base peak intensity Blanks t test PLS-DA Shi et al. ( ) Not reported Protein precipitation 600 MHz NMR Not applied Total intensity area N/R ANOVA PLS Chatterjee et al. ( ) Liquid nitrogen METABOLON, not mentioned GC–MS, RP-LC–LIT/FTICR Log transformation Protein determination N/R ANOVA + FDR Olsvik et al. ( ) Liquid nitrogen METABOLON, not mentioned GC–MS, RP-LC–LIT/FTICR MV imputation Normality check Outlier removal Log transformation Protein determination N/R ANOVA + FDR Olsvik et al. ( ) Liquid nitrogen DCM–ethanol–water GC–MS, RP-LC–QTRAP Log transformation Mean centering Unit variance scaled Pool-baselined, median normalised QC pool and blanks t test correlation PCA Ramirez et al. ( ) Mannitol washing and methanol Chloroform–methanol–water CE-TOF-MS Not applied Mouse albumin N/R t test Toyoda et al. ( ) Organic solvent and liquid nitrogen Protein precipitation RP-LC–QQQ and orbitrap-MS Quantified N/R Blanks Correlation analysis Vorrink et al. ( ) Methanol Chloroform–methanol RP-LC–QTOF-MS Filters Autoscaling QC pool t test PLS-DA Yu et al. ( ) Not reported Chloroform–methanol–water 500 MHz NMR Log transformation Mean centering N/R ANOVA Li et al. ( ) Liquid nitrogen Chloroform–methanol–water 4 LC–QTOF-MS platforms Filter Log transformation Outlier removal PQN, median centering QC pool and blanks PCA and PLS-DA Cuykx et al. ( ) CE capillary electrophoresis, DI-FTICR direct infusion Fourier-transformed ion cyclotron, IP ion pairing, IT ion trap, MS mass spectrometry, NMR nuclear magnetic resonance, QTRAP quadrupole ion trap, QQQ triple quadrupole, RP-LC reversed-phase liquid chromatography, TOF time of flight, UPLC ultra-performance liquid chromatography
Quenching and extraction
All protocols perform a cold-quenching method. Liquid nitrogen is applied in 30% of the cases in the literature, followed by methanol addition (19%). Eukaryotic cells only have a phospholipid cell membrane that can easily be disrupted during cold solvent extraction. Hence, harsh conditions such as acidic and high-temperature extractions are not necessary. Snap freezing and cold extractions are, indeed, often applied methods, since they prevent heat-related degradation, while they denature proteins, inhibiting enzymatic catalysis (Bi et al. ; Leon et al. ; Dietmair et al. ).
Metabolite extraction can be generally divided in the “protein precipitation” method (41%) and the “Bligh-and-Dyer”-based methods (52%); each of them having their own advantages and disadvantages.
The “protein precipitation” method generally adds a polar organic solvent (e.g., methanol) to precipitate proteins, followed by evaporation of the supernatant and reconstitution of the metabolites in an appropriate solvent for the instrumental analysis. These methods are straightforward, introduce less analytical variation, and improve throughput. However, the lack of selectivity can introduce downstream problems, such as solubility issues (lipids vs polar) and instrumental issues, such as ion suppression (Vuckovic ). Protein precipitation methods should, therefore, only be considered for untargeted one-platform experiments.
The variations on the Bligh-and-Dyer method use water, methanol or ethanol, and a non-polar solvent such as chloroform, dichloromethane, or methyl- tert -butyl ether (Matyash et al. ; Bligh and Dyer ; Wu et al. ). Liquid–liquid extraction provides a rough separation between polar and non-polar metabolites (Bligh and Dyer ; Wu et al. ), but it does not introduce much variation and removes potential interfering, high-abundant metabolites, such as phospholipids. This extraction is, nonetheless, non-specific, which makes it very interesting for semi-targeted platforms.
Application methods using Bligh-and-Dyer methods are often combined with multi-platform or semi-targeted/targeted approaches. Balcke et al. ( ), Kim et al. ( ), Toyoda et al. ( ), and Liu et al. ( ) removed interfering phospholipids, increasing sensitivity, and resolution for the polar metabolite analysis using capillary electrophoresis–MS and NMR applications. Ruiz-Aracama et al. ( ), Søfteland et al. ( ), Van den Eede et al. ( ), Van den Hof et al. ( ), García-Cañaveras et al. ( ), Ramirez et al. ( ), and Cuykx et al. ( ) obtained two fractions which can be analysed using dedicated methods for both fractions, reducing interferences within one analysis and increasing coverage and signal reproducibility over the entire platform.
Analytical platforms
Chromatography hyphenated to mass spectrometry is the most applied method, being mentioned in 70% of the literature. Versatility, high sensitivity, and broad metabolic coverage are the main reasons why these analytical instruments are often applied in metabolomics. Reversed-phase (RP) methods using C18 stationary phases in combination with mobile phases consisting of water (A) and methanol or acetonitrile (B), with additional formic acid, are often described because of the non-specific retention mechanism, making it a preferred choice for non-expert research groups. Although this method does separate many metabolites, the single platform is not-optimal, lacking retention for polar metabolites and lacking resolution for many non-polar metabolites (García-Cañaveras et al. , ; Cuykx et al. ; Dunn et al. ). Hydrophilic liquid interaction chromatography (HILIC), capillary electrophoresis, and ion-pairing reversed-phase chromatography are solutions often described in metabolomic applications to increase the separation of polar metabolites (Fei et al. ; Zhang et al. ; Jandera ; Knee et al. ; McCloskey et al. ; Acunha et al. ), but they are not widely implemented in in vitro hepatic research (Balcke et al. ; Kim et al. ; Sugiyama et al. ; García-Cañaveras et al. ; Toyoda et al. ; Cuykx et al. ). GC–MS applications can be considered for lipidomics, especially when the desired analyses focus on fatty-acid compositions (Ramirez et al. ; Meissen et al. ; Olsvik et al. , ; Brown et al. ; Seeßle et al. ).
In untargeted applications, the eluting mobile phase is coupled to high-resolution/accurate mass spectrometers (HR/AM-MS) to improve separation of metabolites in a second ( m / z ) dimension. Time-of-flight (TOF) instruments have the best trade-off between scan-time and resolution (10,000–40,000), making them ideal instruments to be applied for UPLC applications (Ruiz-Aracama et al. ; Kim et al. ; Sugiyama et al. ; García-Cañaveras et al. ; Yu et al. ; Meissen et al. ; Carretero et al. ; Toyoda et al. ; Shi et al. ; Van den Eede et al. ). Ion Trap/orbitrap instruments have a lower scan speed, but provide higher resolution (> 100,000) which is an advantage for high-molecular-weight (glycomics/lipidomics) metabolomics fields (Dunn et al. ). Targeted approaches do not rely on accurate mass and m / z resolution; triple quadrupole configurations, more robust platforms for quantitation, are, therefore, often applied for targeted metabolomics (Balcke et al. ; Van den Hof et al. ; Vorrink et al. ; Wang et al. ; Seeßle et al. ; Sandra and Sandra ).
Although nuclear magnetic resonance (NMR) was initially more employed, the current LC–MS systems provide exceptional improvements regarding sensitivity, resolution, and repeatability (Nicholson et al. ; Leon et al. ; Dunn et al. ; Amathieu ). Nonetheless, NMR still plays an important, complementary role in metabolomics research, since it is fast, reproducible, and sensitive for polar compounds. This complementarity is acknowledged and applied by Ruiz-Aracama et al. ( ), Søfteland et al. ( ), and Van den Hof et al. ( ). The quest for good resolution is also present in NMR applications, with all groups using > 400 MHz instrumentation.
Data processing
Metabolomics platforms generate large and complex data. As in other omics’ disciplines, there is a typical discrepancy between a relatively low number of samples (and replicates) and a large number of variables or features (Broadhurst and Kell ). In the downstream data processing, there is an extensive range of algorithms that can be used, each with an even larger number of parameters that need to be set. The software implementations range from vendor software, which uses often proprietary methods, to academic (often open-source) platforms, which provide more methodological transparency. Both have their advantages and limitations: vendor software provides ready-to-use solutions that typically allow performing a complete analysis in the same software environment. However, besides the lack of transparency, this comes at the cost of limited flexibility. Open-source tools provide a better transparency and can, with sufficient programming expertise, be further adapted to the needs of the application. However, the learning curve is steeper and the choice of initial parameters requires optimisation.
In both cases, every data processing step, from peak picking, to missing value imputations, filter steps, scaling, and normalisation, has an important impact on the outcome of the experiment (Yin and Xu ; Alonso ). Properly reporting all the steps and parameters in the data processing workflow is essential and full transparency determines the reproducibility of the experiment. Standards regarding reporting have been established by the community during several years and their implementation is now encouraged by the metabolomics community and publishers (Sumner et al. ; Dunn et al. , ; Broadhurst and Kell ; Hayton et al. ; Gromski et al. ; Godzien et al. ).
Different peak picking algorithms exist to convert the raw data into variables representing the measured metabolites which are further grouped to create a data matrix for statistical analysis. This conversion is mostly performed through the vendor software (61%), which is a straightforward choice when minimal interaction is preferred (Ramirez et al. ; Balcke et al. ; Kim et al. ; Snouber et al. ; Li et al. ; Chatterjee et al. ; Vorrink et al. ; Toyoda et al. ; Olsvik et al. ; Van den Eede et al. ; Cuykx et al. ). Biological data are often skewed and, unless absolute quantification is performed, quantitative data are usually log-transformed, normalised, and mean-centered. To correct for unequal sample amounts or systematic quantitative biases, normalisation procedures are applied (Hayton et al. ; Ejigu et al. ; Wu and Li ). In the literature, popular techniques were based on protein quantity (30%) or on data-driven transformation (44%). Normalisation based on protein content is considered a pre-analytical normalisation strategy to account for differences in the amount of cell material extracted, anticipating for eventual cell death related to the exposed groups. Silva et al. (Silva et al. ) determined the optimal normalisation to be based on DNA material, since protein determination is less reliable for the normalisation of each sample within the data set. However, normalising by cellular material has the risk of introducing additional bias. Examples of normalisation bias include the co-extraction of dead cells in 3D cultures and the distortion of the DNA/RNA/protein expression by the xenobiotic’s MOA, e.g., through the induction of Cytochrome P450 families. Alternative data-transforming approaches omit the need for additional experiments as they are entirely data-driven. Signal correction is then either dependent on the direct signal acquired from the instrument, such as the total ion count or phospholipid signal, or it is based on the data to be used in the multivariate analysis (Ejigu et al. ; Wu and Li ; Dieterle et al. ). According to Ejigu et al. ( ), data-driven normalisation standards are preferred over internal standard normalisation in untargeted approaches, since the behaviour of the internal standard is not identical or similar to all metabolites.
Data filters (based on, e.g., variance and missing values) are mentioned by several studies (Zhang et al. ; Yu et al. ; Olsvik et al. ; Søfteland et al. ; Van den Eede et al. ; Cuykx et al. ). The removal of unreliable features because of high variance or many missing values is a form of quality assurance (QA) which reduces the noise and sparsity of the final data set, preventing biases in the further statistical analysis (Alonso ; Godzien et al. ). The filtering of the data set has benefits for subsequent univariate and multivariate analyses, as it reduces errors and increases the contribution of the exposure to the overall variance in the data set, respectively (Dunn et al. ; Broadhurst and Kell ; Godzien et al. ).
A commonly applied additional filtering step consists of the subtraction of blank samples (Ramirez et al. ; García-Cañaveras et al. ; Vorrink et al. ; Shi et al. ; Van den Eede et al. ; Cuykx et al. ). Blank subtraction is an easy approach to remove features that are related only to the platform and sample preparation. Such features do not represent the metabolome of the hepatic cultures and could, otherwise, introduce unnecessary variables and complexity, obscuring the biological results.
Since absolute validation is impossible in untargeted analyses, QC is essential in untargeted metabolomics applications and should always be implemented in any metabolomics experiment (Dunn et al. ; Godzien et al. ). QC is often performed by pooling aliquots of all samples, generating a QC pool representing the average of the data set (Dunn et al. ). This QC approach can be implemented in multiple levels of the analytical workflow, ranging from system equilibration to analytical variability assessment and identification (Dunn et al. ; Cuykx et al. ). This form of quality control has been reported by 20% of the reviewed literature (Ramirez et al. ; García-Cañaveras et al. ; Yu et al. ; Søfteland et al. ; Van den Eede et al. ; Cuykx et al. ).
The detailed description of all the processing steps and QC implementations was limited prior to 2015. With the growing awareness of the need for good metabolomics practices, the scientific excellence of metabolomics in in vitro hepatotoxicity assays is expected to continue to increase.
Statistical interpretations
Most statistical interpretations perform a classification based on multivariate analysis and apply univariate tests to select the significant biomarkers.
Univariate tests, such as t tests and ANOVA, are straightforward tests to compare the intensities of a single variable between different groups. The outcome and interpretation of this statistical approach is very clear, but the implementation of univariate tests for hundreds of variables inflates the risk of false positive features (Broadhurst and Kell ; Kim and Wiel ). Corrections for these false discoveries can be applied (such as Bonferroni and Benjamini–Hochberg correction), and according to Kim and Wiel ( ), the best compromise is achieved with the Benjamini–Hochberg correction. The Benjamini–Hochberg correction is less conservative than the standard Bonferroni correction, which is considered too strict, since many variables are not independent of each other (Kim and Wiel ). Univariate tests are applied in the majority of the articles (86%), but only 40% of them explicitly report the implementation of a multiple testing correction.
Principal component analysis (PCA) is the most used unsupervised multivariate technique. PCA projects the maximum variance of a multi-dimensional space in principal components and summarizes the data set in a limited number of components. Being unsupervised, this technique does not explicitly account for class-based separations and is, therefore, mostly used as an exploratory technique. In the reviewed literature, PCA is often applied to interpret if the dependence of all variables projects a biologically relevant pattern, e.g., to distinguish different levels of toxicity.
A commonly used supervised variant of PCA is the (orthogonal) partial least-squares (PLS) analysis, which describes the most variance to distinguish the experimental classes, unravelling the metabolic patterns most important for the classification. The metabolites that have a large impact on the projection are selected using either S plots or variable importance of projection (VIP) values. Supervised multivariate approaches are prone to overfitting to irrelevant or noisy features. Cross-validation procedures use a fraction of the data set as an independent test set to estimate the performance of the model and are essential to evaluate the discriminative capacity of the revealed markers (Broadhurst and Kell ; Gromski et al. ).
Identification
Annotation of the significant signals in untargeted metabolomics data sets is the current bottleneck in metabolomics research. NMR identification relies on small differences in proton coupling, related to the chemical structure of the metabolite. Each proton in a given molecule experiences a specific influence to the magnetic field depending of the electron shielding, generating a specific NMR spectrum. High-resolution instrumentation allows an accurate measurement of the shift of each proton in the spectrum, reducing the number of potential candidates for each measured proton shift. The information of common metabolites is available in in-house libraries or in public metabolomics databases, such as the Human Metabolome Database (Wishart et al. ).
Mass spectrometric strategies imply MS only and ion fragmentation strategies (MS/MS) to identify the accurate m / z of precursor and fragments ions related to the formula and the structure of the metabolite, respectively. The manual annotation of all signals is a tedious task, which is often supported by the use of databases, such as Lipidmaps, Metlin, and the Human Metabolome Database in combination with in-house built MS libraries (Wishart et al. ; Fahy et al. ; Smith et al. ). In silico fragmentation software assists the annotation of MS/MS-based strategies by predicting the potential fragments of the parent structures (Allen et al. ; Wolf et al. ; Kind et al. ; De Vijlder et al. ).
Any identification in metabolomics has to take into account a level of uncertainty. Wrong identifications yield erroneous biological interpretations, and this issue is particularly problematic, since chemical isomers can represent entirely different biological molecules. Based on consensus reached by the Metabolomics standards initiative (MSI), different proposals to provide the level of certainty of the annotation exist, but they are infrequently implemented (Schymanski et al. ; Dunn et al. ). In general, the identification can be categorised in different levels with increasing confidence, ranging from complete unknowns to tentative annotations (group-based annotation), matches against a database and confirmation with standards.
The scale of the MSI is oriented towards unambiguous identification, since it does include enantiomers’ characterisation (level 0), but it does not distinguish between a chemical formula and an accurate mass only (both level 4) (Dunn et al. ). The scale of Schymanski does provide an extra level for molecular formulas, which is a higher level of confidence in identification than m / z alone (Schymanski et al. ). The scales proposed by the MSI (Dunn et al. ) and Schymanski et al. ( ) are merged into Fig. 1 to reflect the entire range of identification confidence.
Fig. 1 Levels of annotation confidence as proposed by Dunn et al. ( ) with the inclusion of an additional level distinguishing between chemical formula an exact mass, as proposed by Schymanski et al. ( ). Adapted from Schymanski et al. ( ) and Dunn et al. ( )
The level of confidence should not necessarily be implemented as a restriction (e.g., removal of level 4 and 5-metabolites) in the subsequent analyses, the scale rather serves as an additional tool to consider the confidence during further downstream interpretation (e.g., levels 1 and 2 annotated metabolites are more reliable when different outcomes are possible during interpretation). Annotation of lipid species is a particular challenge, since the permutation of fatty acyls on the backbone of different lipid classes generates a large collection of lipid species, and often, no commercial standards and/or validated experimental MS/MS data are available for every lipid species (Fahy et al. ; Sud et al. ). As a result, lipid annotation confidence is theoretically lower than the average confidence in small molecule annotation.
The used identification strategies have been reported by 77% of the reviewed literature, of which 13% include a level of confidence (García-Cañaveras et al. ; Van den Eede et al. ; Cuykx et al. ). Providing the strategies and confidence of the identified structures is a challenge yet to be implied in the standard workflow, as it opens the opportunity to improve the integration of the outcomes between different research groups.
Pathway analysis
Integration of the identified markers of toxicity into biologically relevant modes of action is a challenging aspect of the metabolomics workflow. Strategies for interpretation can be divided in two main categories, namely pathway-based methodologies or unbiased (network)-based methodologies (Alonso ; Rosato et al. ).
Pathway-based interpretations rely on the existing databases, such as the Kyoto Encyclopedia for Genes and Genomes (KEGG) (Okuda et al. ). The discovered markers of toxicity are projected on metabolic pathways, such as the glycolysis, the Krebs cycle, and amino acid metabolism (Balcke et al. ; Sugiyama et al. ; Snouber et al. ; Yu et al. ; Wang et al. ; Brown et al. ; Cuykx et al. ). Similar to the transcriptomics (Khatri et al. ) and proteomics (Laukens et al. ) field, statistical enrichment analyses are used to test whether certain pathways are statistically enriched or depleted in the study. Popular pathway analysis software packages include Ingenuity Pathway analysis, MetaboAnalyst, and IMPaLA (Kim et al. ; Chatterjee et al. ; Van den Hof et al. ; Olsvik et al. ; Shi et al. ; Van den Eede et al. ; Kalkhof et al. ; García-Cañaveras et al. ; Kamburov et al. ; Xia et al. ; Chong et al. ).
Network-based approaches link covarying metabolites, creating a network of metabolite clusters (Alonso ). An extra advantage of data-driven pathway analyses is the omission of identification restriction of non-identified metabolites; which improves the potential coverage of the metabolome through inclusion of, for example, enzymatic transformation products, which are not reported in the standard libraries (Rosato et al. ). Network analyses are less frequently applied in the reviewed literature, but provide a valuable tool to infer new linkages between the existing pathways (Meissen et al. ; Alonso ; Rosato et al. ). The combination of data-driven network analysis supported by the biochemical knowledge represented in known pathways is a promising tool to construct new pathways and link existing pathways through new biochemical linkages, but it is yet to be applied in in vitro hepatic metabolomics research.
Metabolites as markers of hepatotoxicity
Most articles report the metabolic changes observed, describing the general function or pathway of the metabolite involved. Since the observed metabolic alterations do not always imply a toxic outcome (e.g., they are part of an adaptive response), the description of significant biomarkers can be fitted in potential pathways to argument potential toxicity. This is illustrated by the results of Søfteland, who noticed a higher number of metabolic perturbations with pesticides rather than with PAHs, although PAHs were toxic at lower concentrations (Søfteland et al. ). This description is a good example that the number of metabolomic alterations should not be considered from an NOAEL perspective, but that it should be interpreted as pathways involved in the potential toxicological mode of action (Ramirez et al. ). García-Cañaveras et al. ( ) and Ramirez et al. ( ) classified different toxicants according to their mode of action, observing specific responses for each different end-point of toxicity. Their approach proves the rationale of different metabolomic fingerprints in perspective to the modes of toxicity involved.
Differentiating between the different modes of action of hepatotoxicity reported by all articles is not straightforward: many articles focus on subsets of the metabolome. This results in under-representation of other common markers of toxicity, e.g., focussing on the lipidome for steatosis might not reveal the alterations in the glycolytic intermediates. Additional concerns related to the integration of the results are potential differences in response to a toxicant due to cell line bias. An ideal example is the difference between the HepG2 and the HepaRG cell line: the first does not possess biotransformation capacity, obscuring the importance of bio-activation in the potential pathways of toxicity. The latter does have an advanced CYP450-dependent metabolism, and can, therefore, include additional mechanisms in its pathways of toxicity (Rodrigues et al. ).
Combining the information of literature reveals common metabolic patterns in hepatotoxicity. Systematically altered metabolites can represent general toxicity, or they can be related to a specific mode of action of toxicity, discriminating between different adverse outcomes such as cholestasis and steatosis.
Of all metabolites reported as markers of toxicity, alanine was the most prevalent marker identified, being altered in over 40% of the studies. The regulation is not consistent, up-regulations being described as a catabolism response, while down-regulations are explained as a wasting and authophagic process (Ruiz-Aracama et al. ; Snouber et al. ; Li et al. ; García-Cañaveras et al. ; Chatterjee et al. ; Van den Hof et al. ; Toyoda et al. ; Olsvik et al. ; Liu et al. ).
Lactate is also frequently observed as a toxicological marker (40%) and is often correlated with the alanine levels (Ruiz-Aracama et al. ; Snouber et al. ; Li et al. ; Toyoda et al. ; Olsvik et al. ). A potential explanation can be the removal of pyruvate when the cell’s metabolism is altered towards the glycolysis; the excess of pyruvate is further metabolised in lactate, with a minor shift towards alanine through transamination. In addition, proline disruptions (30%) correlate with lactate levels (Ruiz-Aracama et al. ; Snouber et al. ; Toyoda et al. ; Olsvik et al. ; Liu et al. ).
3-Phospho-glycerate is mostly down-regulated (26%), being a general marker of toxicity. Its central position in the glycolysis might explain why it is considered to be less specific. Citrate, a key metabolite in the TCA cycle, is also often reported (21%), but the alterations are not consistent between the different studies.
Glutathione in the reduced and oxidised state is often described as an altered metabolite in 40% of the toxicological insults (Ruiz-Aracama et al. ; Li et al. ; García-Cañaveras et al. ; Olsvik et al. , ; Brown et al. ). A decreased GSH/GSSG ratio is hallmark for oxidative stress, but it is also observed in other processes, making it not a specific biomarker for hepatotoxicity. Oxidative stress can be a downstream effect of other toxicological molecular initiating events, for example, through uncoupling of the TCA cycle.
Other unspecific markers of hepatotoxicity are choline (21%) down-regulation and taurine disruptions (30%). For the latter, no specific up- or down-regulation could be observed.
The up-regulation of glutamate is an important notice in many toxicological insults, being associated with disruptions in the TCA cycle (Snouber et al. ; Yu et al. ; Olsvik et al. ; Liu et al. ). Potential suggestions are protein catabolism to feed the amino acids in degradation pathways for energy productions. This might be supported by the observed up-regulation of leucine, serine, and threonine (21%). In this setting, tryptophan and valine are also usually up-regulated (26%), especially observed during exposure to oxidative uncouplers (Snouber et al. ; García-Cañaveras et al. ; Olsvik et al. ; Kalkhof et al. ).
Phosphatidylcholines (PC) and phosphatidylethanolamines (PE) are often correlated in steatosis and phospholipidosis (García-Cañaveras et al. ; Olsvik et al. ; Van den Eede et al. ; Seeßle et al. ). Different behaviours of phospholipids by their chain length are observed by Kalkhof et al. ( ), Olsvik et al. ( ) and Cuykx et al. ( ), related to recruitment of unsaturated FAs for inflammatory intermediates (Olsvik et al. ; Cuykx et al. ; Kalkhof et al. ). PCs are inversely correlated with Lyso-PCs (LPC), dependent on the synthesis of PCs for the membrane function, or the catalysis of PCs to induce lipo-apoptosis. Yu et al. ( ) have observed a correlation of acyl carnitine and LPC with inverse correlation of PC supposing either a recruitment of LPC from PCs or an inhibition of PC formation. LPC is often up-regulated which is reported to induce lipo-apoptosis (García-Cañaveras et al. ; Yu et al. ; Meissen et al. ; Van den Eede et al. ; Seeßle et al. ). However, down-regulations can also relate to toxicity patterns when LPCs are combined with acyls to form PCs during oxidative stress (Shi et al. ).
Acylated carnitines are related to steatogenic processes, except by Shi and Meissen, which show a down-regulation (García-Cañaveras et al. ; Yu et al. ; Meissen et al. ; Olsvik et al. ; Brown et al. ). Sphingomyelines and ceramides are also up-regulated in steatotic processes, and their increases are often associated with oxidative stress (Brown et al. ; Cuykx et al. ; Gorden et al. ; Turner et al. ; Alonso et al. ; Amacher ).
Markers which were not often detected (< 20%), but have similar disruptions among different articles are prostaglandins (up), glycine (up), AMP (down), UDP-hexose (down), PEP (down), PI (up), SAM (up), lysine (down), and creatine (up).
Figure 2 summarizes all observations into a comprehensive pathway. Many of the reported markers of toxicity are related to each other, being part of the glucose metabolism or the lipid metabolism, linked by the mitochondrial respiration. This observation can be explained by (a) the direct impact of a toxicant on the energy homeostasis and (b) the reflection of end-stage toxicity, during which the final cell metabolism is the downstream effect of the initial Molecular Initiating Event (MIE). While the former is a diagnostic identification of the pathways involved, the latter obscures the initial MIE, but it can provide indirect information of the pathways of toxicity, depending on the severity of disruption for every biochemical pathway.
Fig. 2 Proposed connection between different markers of toxicity (black) and the most reported pathways of toxicity (red). 3-PG 3-phospho-glycerate, AA different amino acids, DG diacylglycerols, GSH/GSSG ratio of reduced and oxidised glutathione, LPC lyso-phosphatidylcholines, PC phosphatidylcholines, PE phosphatidylethanolamines, PEP phosphor-enol pyruvic acid, PG prostaglandins, ROS reactive oxygen species, SAMe s -adenosylmethionine, TCA tricarboxylic acid, TG triacylglycerols. (Color figure online)
Standardisation of analyses/future perspectives
To successfully increase the throughput of metabolomics applications for in vitro assessment of hepatotoxicity, standardisation is a critical point to ensure optimal comparison between studies (Ramirez et al. ; Sumner et al. ; Holmes et al. ). The standardisation of untargeted metabolomics protocols is a difficult task, since the chosen parameters are often based on the focus of a subset of the metabolome, which is dependent on the design of experiment and on the equipment used. The optimal cell type and culture conditions for the planned experiment can be incompatible with the conventional setup and might require alternative approaches. Furthermore, different metabolomics platforms have their own advantages in the detection of several subsets of the metabolome, improving the separation and/or detection of the selected metabolites. Rather than standardising protocols, reporting the full details of the experimental conditions will have a greater impact and value on the scientific field as it improves future comparisons in the literature.
Important is a standardisation of transparency and QC implementations, a condition often mentioned in metabolomics recommendations, but not yet widely reported (Sumner et al. ; Hayton et al. ). This is especially imperative for the experimental design and data-analytical processing steps, such as blank subtractions, missing value imputations, and scaling procedures, which have an important impact on the final outcome of the experiment (Sumner et al. ; Hayton et al. ; Hrydziuszko and Viant ). Full description of the applied workflow, mentioning every detail with its impact on the data set (even of processing steps not applied), can and should be included in supplementary information, assisting the reader to accurately define strengths and benefits of each in vitro metabolomics experiment.
As a final recommendation, the implementation of QC is a vital part of good metabolomics practices. Although QC is currently not fully implemented, which has a negative impact for further meta-analyses, this issue can be readily solved by easy implementations in further experiments. The addition of QC samples, such as the QC pool and blank samples, offers a way to assess the variance of the data set and reduce the complexity and the noise of the data to ultimately improve the final quality of the experiment. Such procedures should, therefore, be implemented and evaluated in every metabolomics application (Ramirez et al. ; Dunn et al. ; Holmes et al. ). As a guidance, Table 3 provides a checklist considering key events in the metabolomics workflow, warning for important pit falls and providing examples of good metabolomics practices.
Table 3 Critical steps in a metabolomics workflow, with inclusion of the most important risks and pitfalls, potential solutions, and important reporting conditions Workflow Risks Pitfalls Solutions and recommendations Examples Report Study design Cell selection Cell-type bias Low biotransformation capacity Interspecies differences Choose appropriately Choose appropriately HepG2 (low capacity) vs primary hepatocytes (high capacity) Fetal vs adult primary hepatocytes; rat vs human primary hepatocytes Cell culture: Cell-carrier, cell density, cell passage number, cell type, coating, replicates Cultivation parameters: cultivation time between seeding and exposure Incubator parameters, medium composition Exposure conditions: dosages, exposure method, time-frame, method for dosage estimation, randomisation of culture Culture conditions Culture bias Cell stability Biotransformation capacity Choose appropriate culture conditions 2D vs 3D culture, Validated time-frame, Cell DENSITY Medium composition (serum, DMSO, antibiotics, …) Replicates Not enough statistical power Low number of replicates High biological variance Change design Increase replicates Reduce variance Paired samples, time-point analysis Replication experiment Cell lines vs primary hepatocytes, reproducible seeding and exposure protocols Exposure Dosage bias No observed effects Biased cell viability assay General cell toxicity Decide dosages based on dose–response Choose an independent cell viability method Choose sub-cytotoxic concentrations Use cell viability assay to assess potential toxic dosages Neutral red uptake assay when mitochondrial toxicity is suspected Imply extra (lower) levels of concentrations, correct cell viability assay Temporal bias Time-frame too short Time-frame too long Longer time-frames Shorter sampling times 24 vs 72 h exposure Inclusion of extra time points Sample preparation Cell material Low concentrations of metabolites Low amount of cell material General cell death Cultivate on large surfaces Perform targeted analysis Apply lower dosages 6-well plate or flasks Imply extra (lower) levels of concentrations, correct cell viability assay Washing: mention if applied or not, washing solution composition, washing solution temperature Cell harvesting: harvesting method Quenching: quenching method, quenching temperature, quenching time Extraction: extraction method, extraction time, randomisation, solvent composition, temperatures QC: implementation, pooling method, source Storage conditions: temperature time Additional: biological normalisation techniques Quenching High variance Low concentrations Response bias Extraction extracellular matrix Metabolite leakage Trypsinisation Apply washing step Validate extraction Cell scraping Apply washing step Quench remaining metabolism as soon as possible Extraction Increased variability Matrix effects Protein precipitation Oxidation Extraction variability Choose method in function of application Validate protocol Internal standards (targeted methods) Bligh-and-Dyer extraction for lipidomics Addition of stabilisers Analytical platforms Platform decision Sensitivity bias Low sensitivity of lipids for NMR Choose in function of subset HILIC-MS for polar metabolites GC–MS for fatty-acid compositions Platform: equilibration, instrumentation, parameters, solvent compositions Reconstitution: reconstitution method, solvent composition, temperature, time between reconstitution and injection Platform use Time-bias Changing sensitivity due to contamination Randomise samples Implement instrument equilibration Internal standards (targeted methods) Block randomisation Repeated QC pool injections before sample analysis Data processing Feature selection Algorithm bias Isotopes Peak splitting Wrong alignments Isotope and adduct grouping Correct parameters of processing workflow Estimate system performance (mass accuracy, peak width) Software: version Algorithms: rationale, parameters Filter steps: filter thresholds, rationale Normalisation: applied algorithm, impact, parameters, rationale Feature filtering Selection bias Selectivity bias Sparce data set Retaining noisy features False positives No relevant markers False negatives Filter steps based on meta-data Missing value filtering Blanc subtraction Variance filtering Base thresholds on results of a repeated injection (e.g. QC) Normalisation Normalisation bias Scaling bias Loss of variance Normalisation is interfered by MOA Bias to most abundant signals Bias to low-abundant bias Estimate impact normalisation Data-dependent normalisation Scale Report variance reductions, select most significant variables PQN normalisation DNA normalisation Log transformation Pareto scale QC Variance estimation No implementation of QC No estimation of biases and variance Implement QC procedures QC samples Internal standards QC samples: equilibration, frequency, type Internal standards: implementation Corrections: outlier removal, rationale, report Between batches No implementation of QC RT-shifts Mass accuracy shifts Sensitivity shifts Implement a mix of standards External standards Internal standards Statistical interpretations Univariate No FDR correction Low power False positive results Implement correction Increase replicates Other tests FDR-correction Bonferroni/Benjamin–Hochberg Replication study Non-parametric tests or Multivariate test Univariate tests: false-discovery correction, output, rationale, thresholds, type Multivariate tests: algorithm, cross-validation, selection technique, output, rationale, thresholds Multivariate Wrong interpretation Classification only No cross-validation Loss of information Overfitting and false positive results Choose correct test Select important variables Cross-validate PCA: exploratory, PLS-DA: regression/classification, other models: classification Latent structures or variables of importance Leave-one-out, data permutations, replication study Identification Databases Identification bias Limited number of metabolites Cross-check multiple databases Metlin LipidMaps, Human metabolome database Databases: type, version Identification: algorithms, confidence of identification, identifiers, methods, parameters, techniques, thresholds Interpretation: pathway analysis software, structured pathway Untargeted Identification bias Wrong identification Isomers Single response of algorithms Multiple combinations for single m / z Confirmation of algorithm outcome Orthogonal platforms Retention time MS/MS Isotopes NMR Report certainty Interpretation Interpretation Interpretation bias Structure Relevance False positive outcomes Classification only Incomprehensible pathways of Toxicity Significant features are not necessarily markers of toxicity Validate biological relevance Validate metabolite relationships Validate biological relevance Literature Presence of biomarkers in common pathways Literature
Metabolomics is only beginning to be a platform to investigate in vitro hepatotoxicity. The growing number of publications shows the increasing importance of phenotype fingerprinting for mechanistic interpretations of hepatotoxicity. By keeping up with the recommendations of metabolomics consortia, combining good experimental design with transparency, the overall quality of in vitro assessments for hepatotoxicity improves towards a maturation of in vitro applications and will be a valuable additional tool providing important insights for the research field.