Introduction
The brain is a complex network of billions of interacting cells whose function is regulated by networks of proteins, which in turn are encoded by genes. It is believed that cognitive functions emerge from those complex networks of individual elements that process incoming information by changing their behavior. The field of connectomics [ , ] has the ultimate goal of providing a comprehensive description of the physical and functional coupling among all neural elements of the brain.
Great efforts are currently devoted to studying the brain network structure on multiple scales (Fig. 1 ): from the detailed connectivity of local neural circuits to the large-scale connection patterns among entire brain areas. The multiscale network structure is studied both at the anatomical and functional level. Both anatomical and functional connectivity patterns change across several time scales. These changes form the basis for development, learning, aging, and disease. Fig. 1 Brain networks span multiple scales: The macroscopic dynamics at the surface of the cortex results from the activity of the cortical circuits at the mesoscopic level, which is the outcome of microscopic changes in individual neurons and synapses
While we make progress in providing comprehensive maps of the physical coupling among neuronal elements in the brain, our understanding of the functional coupling is still incomplete. Knowledge of the physical connectome comprising all structural connections is comparable to our knowledge of the genome. This is a huge and important milestone. Nevertheless, knowing the genome structure is only one constraint on how it translates regulatory processes relevant for the expression of proteins. The functional aspect requires the integration of more information on the timing and localization of gene expression. By analogy, understanding the structure of the neuronal brain web is undoubtedly an important step toward understanding how the brain works. But knowing all physical connections in the brain is only one aspect that determines how certain brain connections are engaged for the emergence of cognition [ ].
To date, no single experimental method captures neural information processing across the multiple spatial and temporal scales at which neuronal processes evolve, especially for human research. The high number of interacting neuronal elements and feedback loops generate intricate functional dependencies. A verbal, graphic, or statistical description of empirical observations, as traditionally done in biology, would be insufficient for a truly quantitative understanding of brain function. Hence we need computational methods to provide mathematical descriptions of the processes evolving along the structural connectome. Such models provide a comprehensive theoretical framework for our observations, a construct that we label here the “computational connectome”. The bidirectional relationship of the brain’s structure and function, when described by mathematical laws and validated by empirical findings, offers more than only the sum of both modalities – it enables the investigation of causality, dependencies, and a new level of understanding of multiscale behavior.
Brain network models: finding the sweet spot between complexity and simplicity
While details about physiological functions at microscopic levels are important for understanding regional neuronal computation, it is likely at the large-scale level where the actual integration occurs and cognition appears. Non-invasive neuroimaging methods such as electroencephalography (EEG), magnetoencephalography, functional magnetic resonance imaging (fMRI), and positron emission tomography provide global macroscopic pictures of neuronal dynamics. Macroscopic signal features emerge from the interactions of neuronal populations at local and global scales.
A novel sophisticated whole-brain modeling approach is provided by The Virtual Brain (TVB, www.thevirtualbrain.org ; [ , , ], Fig. 2 ). This open-source platform integrates the large-scale structure of brain connectivity spanning salient brain regions, each of which is represented by a regional neuronal model that is able to capture relevant features of brain activity at the mesoscopic scale, i. e., cell populations comprising up to several hundreds of neurons. Thus regional dynamics can be evaluated in the context of long-range interactions, enabling the principles such as functional segregation and integration to be modeled explicitly [ ]. This whole-brain network modeling approach aims to understand the alterations of inter-areal communication and network dynamics as a consequence of changing network parameters in learning, aging, and brain disease [ , , ]. Fig. 2 Knowledge generation with The Virtual Brain (TVB): Imaging data are used to constrain biophysically grounded full brain models. Brain regions are represented by neural field models. TVB outputs signals of different imaging modalities. Selected metrics are computed from simulated and empirical functional imaging data. Prediction errors are computed for a large range of parameter sets and models. Information is stored in the TVB knowledge base for statistical evaluation and knowledge inference
The functional and structural measures provide a broad assessment of large-scale changes, particularly in terms of inter-area interactions. When merged into a single multiscale model, we can constrain the large-scale macroscopic measurements with local microscopic measurements. For example, the introduction of correlation patterns between neurons in different brain areas can be imposed on top of empirically constrained local correlations or firing rates, allowing for the integration of microscopic and macroscopic levels [ , ]. Models of the microcircuit can finally be set into a larger functional context, and the large body of computational neuroscience theory developed on microcircuits can be exploited for the investigation of brain function. Just recently, a fully automated open-source pipeline was release for standardized processing of human multimodal imaging data and aligning them all in the same space for subsequent use with TVB [ ] and a tool to integrate information from the macaque into human brain models as additional constraints [ ]. Novel algorithms that account for plasticity and learning in whole brain models have been developed [ ], thereby integrating different brain scales [ ].
Special focus for initial applications was put on a prominent rhythm in human EEG – the alpha rhythm – that exhibits the hallmarks of a nonlinear system [ ]. Candidate mechanisms explaining this rhythm’s role for learning and plasticity have been identified [ , , ]. Including realistic inhibitory connections in a thalamocortical network enables the identification of mechanisms for alpha-phase dependencies of firing rates and the inverse relation between alpha power and the cortical fMRI signal [ ]. This study underscores the relevance of considering the interactions of multiple brain regions in computational neuroscience [ ]. Owing to the many dependencies in a complex system such as the brain, the amount of data needed to map all possible scenarios of interactions would be enormous. Hence we need guidance by theory, for the selection of which sort of data are required to achieve new insights on brain function.
Bridging scales with individualized brain models: revolutionizing medicine
A key innovation in brain-network modeling is that an individual’s brain structure is used to set the initial constraints for model parameters, such as network connection strength and connection distance. The individual structural connectome is derived from noninvasive diffusion tensor imaging data (DTI). This approach has profound implications for the understanding and treatment of brain disease and disorders. Currently TVB is the only existing simulator platform with these capabilities. TVB has evolved out of a research program seeking to identify and reproduce realistic whole brain network dynamics, on the basis of empirical connectivity and neural field models [ , , ]. The long-term vision is using this approach to create a model of a patient’s brain to predict the trajectory of recovery or decline and to test different therapies to select the best one for that person. We need to understand the precise regulating principles and mechanisms to design drugs and therapies. Just as knowing the genome does not directly translate into new therapies and diagnoses, knowing the physical connectome will not be sufficient to revolutionize neurology. Individualized brain network models open up completely new possibilities for the understanding of brain disease and their possible treatments (Fig. 3 ). Fig. 3 Personalized biologically based computational models of the brain: Individualized brain network models can reveal the origin of inter-individual differences in brain function and are potential powerful tools for precision medicine
A fundamental basis of brain computations is synchronization of target nodes of the network. The brain consists of modules that are specialized to differential degrees. The activity of different modules distributed across the brain is integrated to yield cognitive function. Noninvasive brain imaging techniques such as EEG or fMRI measure those active networks. Since the imaging methods yield complementary information at different spatiotemporal scales, they provide the possibility to construct more detailed models that capture different aspects of neuronal activity. During aging and various brain diseases, inter-areal communication in the brain changes and may result in cognitive deficits, as well as characteristic changes of the ongoing brain dynamics [ ].
Most neural mass models of TVB offer an extensive selection of parameters that refer to real physiological entities – e. g., various types of ion channels, coupling between different neuron populations, or neural conduction speed [ , ]. Virtual brains allow for the manipulation of isolated neuronal features, which in many cases would be impossible empirically. Techniques like transcranial brain stimulation, which have well-documented effects on the functional connectome [ ] and are examined as a treatment, e. g., for depression [ ] and for neuro-enhancement in the elderly [ ], could in future be tailored to the individual brain. In TVB, the temporal evolution of neural activity can be analyzed without resolution limitations, in contrast to empirical analysis [ , ]. In this way TVB acts as a “mathematical microscope” (Fig. 4 ) that provides looks into the processes within a neural mass such as, for example, local excitation-inhibition balance or firing rates. Such findings can be interpreted as surrogates for microscale characteristics but also as biomarkers for aging, disease [ ], or post-disease recovery, e. g., in stroke patients [ , ]. Fig. 4 Brain network models such as The Virtual Brain can act as mathematical microscope. They infer mesoscopic population behaviors such as excitation-inhibition balance and firing rates as well as intrinsic coupling strength between inhibitory and excitatory mean fields, indicated here by parameters K 11, K 12, K 21
For example, recently an individualized brain model of a patient suffering from bitemporal epilepsy [ ] exhibited different types of epileptic behavior, such as simple and complex seizures. The model was constrained by structural, functional, and EEG data. In line with pathophysiological theories, an epileptogenic threshold was defined for each node in the interacting network system. This special case demonstrates the practicability of clinical use: The patient’s data were acquired preoperatively before a neurosurgical hamartoma resection. In future, individualized brain models could strengthen diagnostics and treatment in epilepsy by identifying an unknown epileptic zone, confirming an MRI lesion as epileptogenic, or simulating the surgical intervention result through the manipulation of the adjusted model. The huge amount of information brings great potential but also methodological challenges in systematizing and using algorithms and computational resources [ ].
Big data, big theories! Extracting information and insight from data
Ostensibly, once we are able to simulate the brain, we will understand how it works – and finally understand ourselves. Humans’ personalities, memories, dysfunctions, and diseases depend partly on the wiring of our brains. Progress toward this goal will hinge on a close interplay between experimentation, theory, and computational analysis. Connectomics datasets are huge, representing prototypical “big data” that are much too large and complex to be easily visualized and intuitively grasped by the human brain. Our goal is to uncover this structure, understand the principles governing the organization of the connectome, and understand how it forms through development and learning, how it gives rise to normal function, how it changes when we age or acquire a disease, and how such changes affect its function.
There are three challenges to our goal: 1. Computation power: Analysis of imaging data is costly in terms of computation time and currently poses the limiting step in cellular-resolution connectomics [ , ]. With reverse engineering approaches we can reveal the regulatory principles from experimental readouts. However, these algorithms simulate large number of possible scenarios testing large parameter spaces and hence quickly hitting the computational limits of even the most powerful supercomputers. 2. Data sharing: The multiscale models need various experimental approaches to collect data at the different temporal and spatial scales. Collection of empirical data draws heavy on resources, which necessitates collaboration of several research institutions. Enabling efficient data sharing requires novel informatics tools, curation of databases, standardization, and a research environment that facilitates those efforts. 3. Cross-disciplinary Integration: The utility of a model improves as data and knowledge from different subfields and disciplines are integrated. Scientists are specialized. A human mind can only grasp a certain amount of information and hence naturally scientists lack understanding of what is going on in neighboring fields. Development of powerful text-mining algorithms and central repositories/catalogues linking up the many pieces of information thus generating new knowledge is another important requirement.
A solution to these challenges may come from the multimodal and multidisciplinary data integration in the AETIONOMY project [ ]. Exemplary for the complex research field of neurodegenerative disease, initial approaches have been successfully established on the basis of a pathway terminology systems [ ]. The entirety of information about biochemical, cytological, and pharmacological findings, and additional information, e. g., through the algorithmic comparison of patients, has led to new possibilities for explorative analyses. The synopsis of what has been taken from big databases via computational methods has been implemented to the topological anatomy of TVB. As a result, a neuroanatomic mapping of underlying pathways was created for healthy brains and those affected by Alzheimer’s disease, which allowed us to compare mechanisms and even identify the possibly underestimated role of poorly understood factors such as malate aspartate metabolism or the ubiquitin proteolysis system in Alzheimer’s dementia [ ].
Neuroinformatics approaches like we propose will benefit from the increasing availability of large, well-organized databases (e. g., the UK biobank [ ], Human Connectome Project [ ], LONI [ ], etc.). Computational connectomics will benefit from the iterative integration of such types of structured information.
Many of the challenges we are facing in the field of computational connectomics do not differ from those in other disciplines with complex regulatory networks such as, for example, molecular biology. Interdisciplinary approaches and the restructuring of academia are needed to overcome the traditional boundaries of segregated disciplines and create a new way of epistemological thinking.