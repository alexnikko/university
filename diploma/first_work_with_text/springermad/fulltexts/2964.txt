Introduction
Concerns in the medical and scientific research communities regarding deficient research reporting have driven the establishment of criteria for detailing appropriate research methodology and reporting [ , , ]. One such effort was the Consolidated Standards of Reporting Trials (CONSORT) statement in 1996, which was subsequently updated in 2001 and 2010 and provides a checklist of information to include when reporting trials [ , ]. Although the CONSORT statement is directed at randomized control trials, the CONSORT recommendations can also be adapted to other common study designs [ , ]. Another such effort was the Standards for Reporting of Diagnostic Accuracy Studies (STARD) statement in 2003, which was subsequently updated in 2015 [ ], and further expanded in 2017 [ ] to include essential items for reporting diagnostic accuracy studies in journal or conference abstracts.
Two study designs particularly relevant to radiology are non-inferiority and equivalence, or similarity, study designs [ ]. In a non-inferiority design, the study aims to show that a new test is not worse than the reference standard. In both equivalence and non-inferiority study designs, an equivalence margin between performance of the new technique and performance of the reference standard must be determined before performing the comparison [ , ]. Then a sample-size calculation is performed to ensure that the number of patients enrolled into the study is adequate to assess if the new technique will perform within the pre-stated margin and thereby conclude similarity to the reference standard. Establishing a proper equivalence margin can be challenging [ ], and most diagnostic performance studies employ the more straightforward superiority study design, even when the stated objective is to demonstrate that a new technique is equivalent or non-inferior to the current reference standard, [ , ]. This approach is problematic because if a superiority design trial fails to detect a difference, it is not correct to conclude that the two tests are equivalent or similar [ , , , ].
Unfounded declarations of equivalence or similarity have been described in published manuscripts of other specialties to include oncology, surgery, internal medicine and infectious disease [ , , , ]. The frequency of these false assertions was also recently brought to the attention of the general radiology community. Park et al. [ ] reviewed all original manuscripts published in Radiology and European Radiology in 2012 utilizing the CONSORT statement for reference and showed that of the 38 studies claiming non-inferiority or similarity, only one study correctly used proper analytic methodology. The objective of our study was to evaluate the prevalence of false assertions of equivalence and non-inferiority in pediatric radiology by reviewing the scientific paper abstracts presented at the International Pediatric Radiology (IPR) Conjoint Meeting and Exhibition in 2016 as a representative sample of current pediatric radiology research. We aim to increase the awareness and ultimately reduce the prevalence of this fallacy in the pediatric radiology literature.
Materials and methods
Scientific paper abstracts presented at the 2016 IPR Conjoint Meeting and Exhibition in Chicago, IL, were selected for review due to the wide breadth of content and diversity of institutions represented. Two board-certified pediatric radiologists (R.P.G., with 20 years’ experience, and R.C.O., with 10 years’ experience) independently assessed the abstracts for study design (e.g., superiority vs. equivalence or non-inferiority), claims of equivalence or similar diagnostic performance, and statistical power analysis. A statistician (W.Z., with 10 years’ experience) reviewed abstracts to confirm claims of equivalence or similar diagnostic performance. Abstracts with statements similar to “no significant difference between techniques” were not considered as a statement of equivalence or similarity since such a statement would be the appropriate one to make in a superiority study design. Abstracts containing comparisons of multiple techniques were included if any of the comparisons demonstrated an unfounded inference of equivalence or non-inferiority. Abstracts were also categorized by topic and geographical origin. Discrepancies regarding appropriate characterization of an abstract by the two radiologists were resolved by consensus or additional review by a statistician.
Results
One hundred twelve of 194 (58%) scientific paper abstracts at the 2016 IPR meeting stated an objective to compare imaging techniques. Of these, 36 (32%) contained unsupported inferences of equivalence or non-inferiority in diagnostic performance and 76 (68%) studies did not make unsupported claims of equivalence or similarity. None of the 112 studies reported an equivalence or non-inferiority study design, and none specified the statistical power of the study. One study identified a small sample size as a limitation stating, “As expected for sample size, differences in area under ROC curves between sequences did not reach significance” [ ].
Of the 36 studies with unsupported inferences of equivalence or non-inferiority, 24 were retrospective, 11 were prospective and 1 study could not be classified based on limited information in the abstract and a PubMed search for an associated manuscript publication.
Abstracts with unfounded inferences were prevalent across nearly all sections presented at the 2016 IPR meeting with the exception of the informatics, education and policy section (Table 1 ). The gastrointestinal section contained the greatest number of abstracts with unfounded inferences (7) but also contained the greatest number of total abstracts (28). The greatest proportion of abstracts with unfounded inferences was found in the magnetic resonance imaging (MRI) contrast agents section, 33% (2/6), although there is substantial overlap in these proportions across sections when confidence intervals are considered. Abstracts with unfounded inferences were proportionately greatest from Canada, 6/23 (26.1%); Europe, 6/29 (20.7%) and the U.S., 23/122 (18.9%) (Table 2 ), with substantial overlap in these proportions across these geographical locales when confidence intervals are considered. Table 1 Abstracts with unfounded inferences by topic Abstracts with unfounded inferences of similarity or equivalence Total abstracts Percentage of unfounded abstracts per section 95% confidence intervals Gastrointestinal 7 28 25% 20–30% Cardiovascular 6 22 27% 22–32% Musculoskeletal 5 22 23% 18–28% Oncology 4 18 22% 17–27% Interventional 3 16 19% 14–24% Neuroradiology 3 18 17% 12–22% MR contrast agents 2 6 33% 28–38% Genitourinary 2 10 20% 15–25% Fetal/neonatal 2 21 10% 5–15% ALARA 1 10 10% 5–15% Thoracic 1 13 8% 3–13% Informatics/education/policy 0 10 0% 0–5% Total 36 194 19% 14–24% ALARA as low as reasonably achievable Table 2 Abstracts by geographical distribution Unfounded inferences Total abstracts Percentage 95% confidence intervals United States 23 122 19% 13–25% Europe 6 29 21% 15–27% Canada 6 23 26% 20–32% Asia 1 11 9% 3–15% Africa 0 6 0% 0–6% Australia 0 2 0% 0–6% South America 0 1 0% 0–6%
Discussion
Studies aimed at comparing imaging techniques were common at the 2016 IPR meeting, representing 58% of all scientific abstracts presented. Importantly, 32% of these abstracts concluded equivalence or non-inferiority but failed to employ the correct study design, using a superiority design instead.
Given the prevalence of this fallacy, a greater understanding of appropriate study design and pre-experiment power analysis is clearly needed. In many research studies, the goal is to demonstrate a group difference to show the effect of a certain test or treatment. The superiority test is designed to serve this purpose. In a superiority test, the null hypothesis is that there is no difference, and the alternative hypothesis is that there is some difference. If strong enough evidence is gathered to consider the observations as not attributable to chance and hence reject the null hypothesis, the new test or new treatment is considered to be superior. Not being able to reject the null hypothesis simply means that there is not strong enough evidence of superiority, but it does not prove that there is equivalence or non-inferiority [ , ]. Equivalence or non-inferiority can be incorrectly concluded, particularly when the appropriate study design is not used and/or sample sizes are small [ , ]. Importantly, it may be acceptable to use a superiority test in an equivalence or non-inferiority study if a pre-experimental power calculation is performed to determine an adequate sample size. In this case, the researcher’s goal is to show enough evidence to accept the null hypothesis.
Using an equivalence or non-inferiority test is most appropriate when a researcher’s goal is to show comparability. In practice, no two tests or treatments will yield exactly the same effects. Equivalence or non-inferiority, therefore, means that the effects differ by no more than a tolerable amount, which is known as the margin [ , ]. To set up the test, one simply flips the null and alternative hypotheses of the traditional superiority test. In other words, the null admits that there is a difference more than the margin, and the alternative is that the difference is within the margin. The null will be rejected if the statistics hold strong evidence against the null hypothesis. Non-inferiority tests frequently require a larger sample size than superiority tests, an important consideration for researchers when designing a study.
Table 3 provides two scenarios of fictitious abstracts with unfounded inferences of equivalence similar to those made in abstracts from the 2016 IPR meeting and corresponding fictitious abstracts with valid inferences of equivalence based on the data analysis and study methodology. In scenario 1, it is hypothesized that the newer and less expensive Radiopharmaceutical Y has a similar peak standardized uptake value (SUV) as the standard Radiopharmaceutical X in patients with lymphoma evaluated by positron emission tomography/computed tomography (PET/CT). There is no power analysis in the abstract with an unfounded inference of equivalence for scenario 1. It is possible that there is a significant difference between peak SUV of Radiopharmaceutical X and Radiopharmaceutical Y, but the small sample size precludes detecting that difference. In the abstract with a valid inference of equivalence for scenario 1, a power analysis demonstrates that 550 patients are needed to show that the peak SUV with Radiopharmaceutical Y is within the pre-stated margin of equivalence of +/− 10% peak SUV of Radiopharmaceutical X. The appropriately powered study design allows the researchers to declare equivalence in the conclusion based on the study results. Table 3 Examples of abstracts Unfounded Valid Scenario 1: Newly introduced Radiopharmaceutical Y is a less expensive alternative to the currently accepted standard, Radiopharmaceutical X. We hypothesize that Radiopharmaceutical Y has a similar peak SUV as Radiopharmaceutical X in patients with lymphoma evaluated by PET/CT. Methods Radiopharmaceutical X was administered to 25 patients with lymphoma undergoing PET/CT and Radiopharmaceutical Y was administered to 24 patients. Peak SUV was recorded. A pre-stated margin of equivalence of ± 10% the peak SUV value of Radiopharmaceutical X was established for the peak SUV value of Radiopharmaceutical Y. Given that the standard deviation of the peak SUV of the population is two, a power analysis demonstrated that 550 patients would be required to prove equivalence with a power of 0.8. Radiopharmaceutical X was administered to 275 patients and Radiopharmaceutical Y was administered to 275 patients. Results Average peak SUV for Radiopharmaceutical X was 5.2 and average peak SUV for Radiopharmaceutical Y was 5.1. Average peak SUV for Radiopharmaceutical X was 5.2. Average peak SUV for Radiopharmaceutical Y was 4.9, within the pre-stated margin of equivalence (4.7–5.7). Conclusion Radiopharmaceutical Y has similar peak SUV to Radiopharmaceutical X. Radiopharmaceutical Y has similar peak SUV to Radiopharmaceutical X. Scenario 2: New Contrast Agent Y is a less expensive alternative to the currently accepted standard, Contrast Agent X. We hypothesize that Contrast Agent Y has a similar safety profile as Contrast Agent X as determined by adverse event rate. Methods A retrospective review evaluated all exams performed with Contrast Agent X and Contrast Agent Y over 12 months to identify the rate of adverse events with each contrast agent. An acceptable margin of non-inferiority for safety of Contrast Agent Y was set as 5% + adverse event rate of Contrast Agent X. A power analysis demonstrated that each contrast agent would need to be administered to at least 250 patients to achieve statistical significance. Results Fifteen patients received Contrast Agent X and 60 patients received Contrast Agent Y. One adverse event occurred with Contrast Agent X (7%) and 5 occurred with Contrast Agent Y (8%). Fifteen adverse events occurred with Contrast Agent X (6%) and 29 adverse events occurred with Contrast Agent Y (12%). The rate of adverse events of Contrast Agent Y is not within the margin of non-inferiority (6%–11%). Conclusion Contrast Agent Y and Contrast Agent X have similar adverse event rates. The adverse event rate for Contrast Agent Y is not similar to Contrast Agent X. CT computed tomography, PET positron emission tomography, SUV standardized uptake value
In scenario 2, two contrast agents are tested to see if they have similar adverse event rates to validate the hypothesis that the newer and less expensive contrast agent, Contrast Agent Y, is as safe as the standard contrast agent, Contrast Agent X. The abstract with the unfounded inference of equivalence in scenario 2 uses a superiority study design but is so underpowered that it fails to detect a clinically significant difference in adverse event rates. In the abstract with a valid inference of equivalence for scenario 2, a power analysis for an equivalence study design showed that a minimum of 500 patients was needed to have appropriate power to conclude similarity within a pre-stated margin of equivalence of +/− 5% relative to the standard contrast agent, Contrast Agent X. A clinically and statically significant difference in safety performance is demonstrated for Contrast Agent Y relative to Contrast Agent X, and the conclusion that the two contrast agents do not exhibit similar adverse event rates is valid.
Previous studies show a pervasive pattern of unfounded claims of equivalence or similarity in other specialties. Costa et al. [ ] evaluated 235 published randomized cancer trial manuscripts with claims of equivalence and found that 54% of claims of equivalence were unfounded. Dimick et al. [ ] reviewed 90 randomized control trial manuscripts in the surgical literature and identified that only 39% met criteria for adequately demonstrating equivalency. Evaluation of 45 articles in a MEDLINE search of medical articles aimed at studying equivalence showed that 67% incorrectly claimed equivalence after a failed test for superiority and in 10% the claim of equivalence was not statistically evaluated [ ]. In a smaller study of the pediatric infectious disease literature, Krysan and Kemper [ ] showed that of 25 studies claiming therapeutic equivalence in antimicrobial treatments of bacterial meningitis, 23 (92%) studies used incorrect methodology of failed tests of superiority to conclude equivalence and 24 (96%) studies did not have a sufficient sample size to exclude a 20% difference in mortality between tested therapies.
Our results show that this fallacy of inference was common among studies of diagnostic performance in pediatric radiology presented at the 2016 IPR meeting. Additionally, our study is unique in that it addresses the prevalence of unfounded claims of equivalence or similarity in scientific abstracts presented at a national or international radiology meeting. Prior research into this area of study design and statistical inference has focused on published manuscripts in peer-reviewed journals. This is an important distinction, since many abstracts presented at national or international meetings will subsequently be submitted to peer-reviewed journals. Increased awareness of this error in study design and statistical inference during the early stages of a research project could help improve the scientific rigor of the research presented at scientific meetings and subsequently submitted as manuscripts to peer-reviewed journals. The importance of improving the informativeness of abstracts for conferences and journals has gained increased attention with the recent publication of the STARD for Abstracts, which lists 11 essential items to be reported in every abstract of a diagnostic accuracy study [ ].
Underpowered studies are of particular concern in pediatric radiology where it can be difficult to obtain an adequate sample size given the relatively small number of patients that can be accrued for some specialized techniques and uncommon pediatric diseases. This can lead to premature adoption of imaging techniques that have not been adequately validated. None of the abstracts claiming similarity or equivalence at the 2016 IPR meeting included a power analysis to support its conclusions, and it is possible that these studies were underpowered. For future scientific abstract presentations, this problem could be addressed by including a power analysis in the abstract. If a power analysis is not included, authors presenting results from studies employing a superiority design should refrain from making claims of equivalence or similarity. When the purpose of the study is to show equivalence or similarity, authors should use the appropriate equivalence or non-inferiority study design, state this explicitly in the abstract and detail the statistical analyses. Abstracts failing to use the appropriate study design and failing to show an adequate power analysis to support claims of equivalence or similarity should be rejected. This objective could be facilitated by recommending that the submitting authors follow research-reporting guidelines such as CONSORT and STARD. It may also be helpful to have reviewers use similar guidelines when reviewing abstracts.
Our study has several limitations. Abstracts must satisfy a strict word count limit, and it is possible that power analyses were performed for some studies but were not included in the abstracts. It is possible that some abstracts used an equivalence or non-inferiority study design but did not describe the methodology clearly in the Materials and Methods section of the abstract. However, we think this is likely infrequent given how results were reported and the lack of a pre-stated acceptance margin to justify claims of equivalence or non-inferiority between techniques. To allow authors the opportunity to address these items without infringing on the abstract word count limit, a separate section detailing power analyses and acceptance margins could be made viewable only to the abstract reviewers or could be included in the published abstract similar to “implications to clinical practice” listed in some scientific meetings. Finally, some abstracts contained comparisons of multiple techniques. If any single comparison invoked an unfounded inference of equivalence or similarity, then it was included for the purposes of our study even if the conclusions were warranted for other comparisons.
Conclusion
Inadequate reporting and unfounded inferences of equivalence or non-inferiority were common in the 2016 IPR scientific paper abstracts of diagnostic performance comparison studies. It is important that investigators and reviewers recognize that the failure to detect a difference, especially in a superiority design without adequate power analysis, does not equate to absence of a difference. Requiring submitting authors and reviewers to follow research-reporting guidelines may help improve the quality of future meeting abstracts and publications, and avoid adoption of imaging techniques that have not been validated.