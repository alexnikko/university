Introduction
In 2015, it was estimated that 196,900 new cancer cases would be diagnosed in Canada, with 78,000 deaths from cancer [ ]. Furthermore, a 79 % increase in cancer incidence is predicted by 2028 to 2032 compared to 2003 to 2007 [ ]. This rise in cancer incidence is primarily due to the increase in the aging population, making it especially important for future physicians to be well trained in prevention, treatment, and management of cancer [ ]. Additionally, as treatments improve, the likelihood of cancer patients surviving at least 5 years after diagnosis is 63 % compared to the survival of the general population [ ]. As the number of cancer survivors increase, patient care cannot be done in silos in oncology and medical trainees need good knowledge of management of common cancers. Medical trainees need to be educated in early diagnosis of common cancers and management of treatment side effects and psychological effects due to cancer.
Studies to date have indicated an inadequacy with oncology education across undergraduate and post-graduate training programs. A recent survey on Canadian post-graduate residents reported poor teaching of cancer prevention and screening, with 33.3 % of residents reporting that these subjects were never taught [ ]. Instead, the primary focus of oncology education includes diagnosis, management, treatment, palliative, and supportive care issues [ ]. Furthermore, oncology education in Canadian training programs for undergraduates and post-graduates make up less than 10 % of the curriculum [ ]. In 2014, Tam et al. found that only 7 of 13 Canadian internal medicine and no family medicine training programs had a mandatory oncology rotation [ ]. As a result, many students felt ill-prepared in this field, as no standard set of objectives were given for guidance [ ]. Many medical trainees have indicated a lack of knowledge and skills to assist patients with cancer prevention and detection even after training and graduation [ ]. Improved oncology education is not only important for prospective oncologists, but also essential for those in family and internal medicine. There is a growing need for those with an in-depth understanding of oncology because oncologists are neither the gatekeepers, nor the primary contact for patients when they present with symptoms [ ]. If primary care physicians are better informed of cancer-preventive strategies, it may help reduce incidence rates or improve survival rates with early detection and treatment [ ].
Many studies have revealed that oncology education is inadequate, primarily due to the inconsistencies across training programs and a lack of a dedicated oncology curriculum. There has yet to be a program or curriculum that standardizes oncology training. This study seeks to address this educational gap and fill the need for a standardized curriculum for medical trainees by developing a tool that covers common oncology topics. We created a comprehensive educational tool that included a comprehensive paper handbook, which provides basic information on common cancers and a companion website with case studies illustrating management of oncology cases. We believe that the integration of this tool into the oncology rotation will increase the knowledge base of medical trainees, and support better decision-making and knowledge application. This tool can also be used to guide medical trainees in self-directed learning, regardless of their specialization.
Materials and Methods
Our study was approved by the Research Ethics Board at St. Michael’s Hospital (SMH), Toronto, Ontario, Canada, in 2012. Post-graduate year 1, 2, and 3 trainees (PGY1, PGY2, and PGY3) entering their oncology rotation at SMH were approached and informed written consent obtained prior to any interventions. All undergraduate medical students were excluded. Data collection for phase I (usability evaluation) was completed in 2012. After the revision of the tool and implementation for phase II (quantitative evaluation), data collection was completed in 2015. A second edition of the handbook was developed in 2015 to update its content and it was subsequently used for phase III (qualitative assessment), which was completed in 2016.
Tool Development
The oncology handbook was designed to be used as a standardized educational tool for post-graduate trainees providing clinical care to cancer patients. The text was developed and reviewed by the study investigators, based on recognized oncology textbooks, teaching materials, and the American Joint Committee on Cancer (AJCC) staging classification. We selected eight cancer types: breast, colorectal, lung, lymphoma, melanoma, ovarian, prostate, and testicular. Breast, colorectal, lung, and prostate cancers are the most common types of cancers seen in the oncology clinics. While melanoma, lymphoma, testicular, and ovarian cancers are rare, they are curable if detected early. Therefore, trainees need to have a basic understanding of these cancers. In each section, we emphasized screening, prevention, diagnosis, disease staging, staging investigations, and disease management. This handbook was not exhaustive, but focused on practical management guidelines and the needs of cancer patients. After phase II, the handbook was updated with more detailed information.
A companion website was designed as an oncology education portal. This website contains identical information to the handbook, but also includes case studies that illustrated management of common cancers and references to pivotal oncology trials. The E-learning tool was an HTML site (Fig. 1 ) designed to provide an online version of the paper handbook, additional information on other cancer types, as well as links to pivotal clinical trials, for easy access to the needed oncology education (available at http://residents.utorontoeit.com ). The content was drawn from 2011 ASCO Educational Book, ASCO-SEP® Third Edition and evidence-based guidelines from Cancer Care Ontario. The website also includes a case-based, interactive clinical simulator designed using Articulate Rapid E-Learning Studio ‘09 (Fig. 2 ). Current cases were developed by a group consisting of medical oncologists, hematologists, and a research assistant. They include those relating to lung and breast cancer. Fig. 1 Screenshot of the “Introduction” section of the website ( http://residents.utorontoeit.com ) Fig. 2 Screenshot of a lung cancer case in the interactive clinical simulator
Evaluation
For evaluation of this educational tool, it underwent an initial phase to assess its usability, a second phase to assess its usefulness and effectiveness in improving trainee knowledge, and a third qualitative phase to assess the value of the tool. The initial trainees ( n = 4) were consented to participate in phase I of our study and subsequent trainees consented to phase II ( n = 39) and phase III ( n = 10) (Fig. 3 ). Fig. 3 Allocation of participants recruited from the oncology rotation at St. Michael’s Hospital
Phase I—Usability Testing of the Educational Tool
The teaching tools were evaluated through a combination of heuristic evaluation and user testing. Usability evaluation and refinement was conducted to identify and address issues prior to the evaluation of the tool, which may adversely affect the learning process.
Heuristic evaluation was performed by a usability engineer, using a set of usability heuristics as guidelines. As per the heuristic evaluation methodology, usability issues were first identified, then classified by severity into cosmetic, minor (issue will affect a small number of users), major (issue is expected to affect majority of users but workaround exists), or catastrophic (showstoppers, where issue will affect majority of users and users cannot proceed) [ , ]. The severity estimates were based on frequency, impact, and persistence of errors. Usability issues deemed to be major or catastrophic were addressed and rectified.
Following the heuristic evaluation, usability testing was conducted with the four phase I users. The usability testing sessions were led by a human factors engineer and a research assistant. After consent, enrollment, and receiving instructions on the talk-aloud protocol, the participants completed a set of pre-determined tasks using the website and handbook, while commenting out loud on how they were completing each task. Tasks involved answering questions on cancer treatment and staging using the website (Appendix A). The display on the computer screen was recorded using a screen capturing software. The sessions were audiotaped and transcribed for subsequent analysis. Field notes were taken during the usability sessions. The screenshots were reviewed by the human factors engineer for any further usability issues that may have been encountered by the test subjects. Following the completion of the tasks, the phase I participants were solicited for additional feedback on the usability of the website (Appendix B).
The website was refined based on the phase I participants’ feedback before proceeding to phase II. All major and catastrophic issues were addressed, and usability testing was repeated following the re-design. Addressing severe usability issues before the education assessment phase ensured that no known usability issues would interfere with the learning process.
Phase II—Quantitative Evaluation of the Oncology Training Intervention
For quantitative evaluation (phase II), a test (Appendix C) was developed to assess the impact of the educational tool on trainee’s knowledge. Questions were designed to cover the basic knowledge of common cancers, including topics related to cytotoxic therapy, its side effects, and the role of monoclonal antibodies in lymphoma, breast, and colon cancers. The paper-based test contains 10 multiple-choice questions and was graded out of a total of 10 marks. This test was administered once at the start of the oncology rotation, and again within 1 week following the completion of their oncology rotation. Baseline and post-intervention scores were compared using paired t -test. Three comparisons were computed. In the first, the participants who did not complete the post-intervention test were assumed to have no change in their score. In the second analysis, scores were compared only when both tests were completed. In the third analysis, participants were divided into subgroups based on their level of training (PGY1–3) and data sets were compared.
Phase III—Qualitative Assessment of the Oncology Tool and Rotation Experience
For qualitative assessment (phase III), a survey (Appendix D) containing 17 questions was created to evaluate trainees’ overall oncology rotation experience, their attitude on the content and usefulness of the tool, and their beliefs on the integration of the tool or similar educational material. One question asked the participant’s level of training (post-graduate year), 11 questions used a response scale from 1 to 10 to evaluate various aspects of the rotation (educational tool, ambulatory clinic, didactic lecture, etc.), one was a ranking question to determine the helpfulness of different cancer sections in the handbook, and the remainder were asking for comments and selection between the handbook and website.
Results
A total of 53 participants consented to participate in this study at the start of their oncology rotation at SMH.
Phase I—Findings of the Heuristic Evaluation
The initial four participants took part in evaluating the oncology educational tool. Overall, the participants found the website comprehensive. The content was found to be clear, concise, and relevant. Information was well organized. Six issues were rated as “moderate” and four were “major” issues, but no “catastrophic” issues were identified. The specific findings of the usability evaluation were rectified with appropriate actions to address the issue. Most of the issues were quickly addressed by simple reorganization of the text.
Phase II—Baseline and Post-intervention Knowledge Tests
A total of 39 trainees consented to phase II, with 32 completing both the baseline and post-intervention tests. Two participants did not complete the test at the start of their oncology rotation, thus baseline knowledge was not assessed and participants were excluded. Five participants did not complete the post-intervention test. All knowledge tests were graded out of a total of 10 marks.
Medical trainees who completed the baseline test only were assumed to have no change from baseline, thus baseline and post-intervention scores were assumed to be equal. The mean post-intervention score ( μ = 8.95, σ = 1.53) was found to be significantly higher ( p = 7.87 × 10 −8 ) than the mean baseline score ( μ = 7.24, σ = 1.46). A second analysis was performed and participants who did not complete the post-intervention test were excluded. For 32 participants who completed both tests, the mean baseline score was 7.47/10 ( σ = 1.37) and the post-intervention score was 9.44/10 ( σ = 0.80). This indicates an improvement of 19.7 %, with significantly higher ( p = 2.06 × 10 −8 ) average post-intervention score.
Also, we observed patterns of improved scores on questions that many trainees answered incorrectly in the baseline test. The results are shown in Table 1 and Fig. 4 . More than half the trainees answered question one (Q1) incorrectly at baseline and the same incorrect answer (option D) was chosen by 31.3 % (10/32) of trainees. This indicates that there may have been a misconception due to a lack of knowledge or exposure to oncology material. However, after receiving access to the study intervention and completion of the rotation, 93.8 % (30/32) of medical trainees answered Q1 correctly on the post-intervention test. Trainees showed similar struggles with question five (Q5), where only 34.4 % (11/32) of trainees answered it correctly at baseline. However, after receiving access to the intervention and completion of their rotation, 90.6 % (29/32) of participants answered Q5 correctly, corresponding to a 56.3 % increase. Table 1 Number of correct answers by participants who completed both the baseline and post-intervention tests Question number Evaluations Baseline test (/32) post Intervention test (/32) Q1 13 30 Q2 30 31 Q3 22 30 Q4 23 29 Q5 11 29 Q6 31 32 Q7 30 32 Q8 29 30 Q9 31 31 Q10 19 29 Fig. 4 Baseline and post-intervention results of 32 participants who completed both baseline and post-intervention tests
To account for the different level of training in the participants, a paired t -test was performed in subgroups arranged by their post-graduate year. The subgroups consisted of 6 PGY1, 8 PGY2, and 18 PGY3, with baseline mean scores of 7.83 ( σ = 1.33), 6.75 ( σ = 0.89), and 7.67 ( σ = 1.49), respectively. Baseline mean scores were not significantly different. Test scores were significantly higher in each subgroup, with post-intervention scores of 9.67 ( σ = 0.52, p = 0.028397); 9.25 ( σ = 1.16, p = 0.001565); and 9.44 ( σ = 0.70, p = 0.00015) in PGY1–3 subgroups, respectively.
Phase III—Qualitative Assessment
Ten participants were asked to complete the qualitative survey within a week following the completion of their oncology rotation; however, one participant was not able to complete the survey due to early withdrawal from the oncology rotation. Out of the nine participants who completed the survey, eight preferred the handbook over the website. In evaluating the handbook, it was rated an average of 9.22/10 in terms of the content being clear and an average of 9.33/10 in usefulness compared to other educational material. One participant said it was a “very useful quick reference guide, well organized and easy to use.” The handbook was also called “fantastic [for] having most common cancer staging tables in one booklet.” The participants felt that the handbook was comparable to didactic lectures (average rating of 9.33 and 9.44, respectively).
Participants strongly believed that the integration of this tool would help increase oncology knowledge in medical trainees (average rating of 9.44/10). Overall experience of the trainees with the handbook was very positive and some of the comments provided a clear picture of their appreciation for this tool during their oncology rotation. One participant summarized their experience in a comment: “It is easy for early trainees to get lost in many different treatment protocols for various cancers. Handbook provides a quick, easy to use guide that is imperative in the early part of the rotation for trainees to grasp most important concepts.” Another trainee stated that “the book was perfect” and “having a local book with cited evidence and local practice helped a lot to learn about cancer.” As one trainee was leaving his rotation, he also stated that the “handbook will stay with [him] as [he] transitions from medical student to junior doctor and into the future as an aspiring oncologist.” One suggestion for the handbook was for the information to be available on a mobile app for easier access.
Discussion
Standardization of training in oncology is necessary for setting benchmarks and measuring increases in oncology knowledge. To our knowledge, this study is the first in oncology that yields measurable effects of an educational tool, consisting of a structured comprehensive handbook and a website. Our assessment has indicated similar levels of baseline knowledge across medical trainees at the start of their rotation, but after implementation of the tool, knowledge significantly improved as a whole and within each post-graduate year. However, the improvement in the knowledge of trainees could not be completely attributed to the tool, as we did not have a control group to assess the effects of the overall rotation. Therefore, a qualitative assessment of the updated tool was conducted to determine its value to the trainees. The qualitative results revealed positive responses and strong support for the integration of the oncology educational tool or a similar intervention into the resident oncology rotation. The tool was not intended to replace existing lectures or teachings in the clinics, but was meant to be used as a practical guide for learning an overwhelming amount of oncology content. It is always difficult to review important and common topics in a 3–4-week rotation. Therefore, our educational tool stands as a possible solution to the educational gap if integrated into the oncology curriculum and it will encourage trainees to do self-directed learning.
Other studies in Europe have looked at the effects of “summer-school” oncology programs for medical students [ , , ]. The intervention of summer-school programs was shown to be effective even if they only provided small increases in oncology exposure (either in the form of additional courses, rotations, or electives); however, this would not be viable in Canada [ , , ]. Only 16 % of Canadian medical students stated they would attend and also, some medical schools in Canada (e.g., McMaster University and the University of Calgary) do not have summer breaks [ ]. Our educational tool is feasible, easily disseminated, and appropriate for providing learners a standard set of oncology topics. The use of web-based learning also has advantages of overcoming time and distance constraints, and provides individualized learning [ ].
Although evaluation of quality and impact of educational programs may not be straightforward, we hypothesize that improved oncology training in internal and family medicine would translate into better care for cancer patients. While this study did not assess the direct effect of increased oncology education on patient care, these results will be a useful reference for future studies. This is a landmark study in oncology education, as the intervention guides medical trainees to achieve standard oncology knowledge.
There are certain limitations to this study. This study was restricted to only medical trainees at SMH and consequently, the results of the study may not be generalizable. Also, as mentioned before, this study did not involve a control group, since it was considered unethical to deny access to the learning tool for medical trainees rotating through SMH. A control group would clarify whether improved scores resulted from the provision of the handbook and website, or from standard teaching, including exposure to the clinic and didactic lectures. Finally, the testing effect from the distribution of the same test at baseline and post-intervention could have influenced the post-intervention high scores.
To address these limitations, future evaluation of this oncology handbook and the web-based tool would include involvement of other institutions. This would allow comparing of the effectiveness of this educational tool to other educational methods.
Conclusion
In summary, implementation of our educational tool during medical trainees’ oncology rotation has demonstrated to improve knowledge test scores at the completion of the rotation compared to the baseline. While we could not isolate the effects of the educational tool on knowledge improvement, participants’ qualitative feedback points to student satisfaction with the additional resource of an oncology handbook, and a website that provided comprehensive and common oncology topics. A standardized educational tool such as this can be incorporated into the medical curriculum to provide students with more exposure to the topics in oncology and guide a better understanding of multidisciplinary oncology management. Improved knowledge is expected to ultimately increase the trainees’ self-perceived skills and confidence in assisting patients with cancer prevention, detection, and management. This tool can also be used for comparison with other teaching methods for future advanced educational programs.