Introduction
In population pharmacokinetics and pharmacodynamics (PK/PD) analysis using nonlinear mixed-effect models, one important aim is to identify relationships between individual model parameters and covariates [ , , ]. Covariates for demographics, genotypic, phenotypic disease and therapy explain parameter variability between subjects. This can play a key role in the prediction of optimal dosage schedules and amounts for an individual and identification of patient subpopulations at risk of receiving a suboptimal treatment [ ].
The stepwise method for covariate model building has often been used to select covariate relations in population PK/PD models; however, this procedure may result in poorly predictive covariate models in small or moderately-sized data sets [ ]. The least absolute shrinkage and selection operator (Lasso) in linear regression models was proposed by Tibshirani [ ] and is a penalized method that minimizes the residual sum of squares subject to restriction on the summation absolute of coefficients. The restriction causes Lasso to carry out covariate selection by assigning some covariate coefficients a value of zero [ ]. Furthermore, the Lasso estimates can be considered as a Bayesian mode of regression coefficients which have been given independent, double exponential (DE) prior distributions [ ].
Although Lasso is popular for simultaneous estimation and covariate selection, it suffers from two important shortcomings. First, Lasso does not enjoy oracle properties with which the right set of covariates with a probability converging to one can be correctly selected and it estimates the nonzero coefficients with the same asymptotic distribution as if the true model were known in advance [ ]. Second, when there is multicollinearity among covariates, Lasso cannot identify the true model and select one covariate from among correlated covariates [ ].
Zou [ ] introduced a new version of Lasso called adaptive Lasso (ALasso) to overcome the shortcomings of Lasso. He used ordinary least square (OLS) estimates as initial weights, which allowed a higher penalty for zero coefficients and lower penalty for non-zero coefficients. Although it was shown that ALasso enjoys oracle properties, OLS–ALasso did not outperform Lasso in the presence of high correlation between covariates. Lasso coefficients as initial weights in ALasso were proposed by Lian [ ] for variable selection in semiparametric models when the number of covariates exceeds the sample size. Griffin and Brown [ , ] generalized the Bayesian Lasso and proposed HyperLasso in which a normal exponential gamma (NEG) distribution was considered as a priori for regression coefficients. They showed that HyperLasso provided significant improvement compared to Lasso and other penalization methods when the number of variables was higher than the sample size.
A new version of ALasso called adjusted ALasso (AALasso), was proposed by Qian and Yang [ ] to overcome multicollinearity among covariates and took into account the standard error of the OLS estimates in initial weights. Algamal and Lee [ ] proposed the ratio of the standard error of the maximum likelihood (ML) estimator to the ML estimator as an initial weight in ALasso for Poisson regression models. They showed that the proposed method outperformed Lasso and ALasso with ML estimates as initial weights when covariates were correlated.
The Lasso method in non-linear mixed-effect models was proposed and implemented in NONMEM and PsN by Ribbing et al. [ ]. They compared Lasso to a stepwise method in different scenarios and showed that it predicted the covariate model better, particularly on a small data set. Moreover, because Lasso runs in parallel in NONMEM, it was two or three times faster than the stepwise procedure for small data sets (40 and 60 subjects), but was only marginally faster for larger data sets [ ].
HyperLasso using NEG priors was implemented by Hoggart et al. [ ] in a pharmacogenetic example having more variables than observations. HyperLasso has been compared with other penalized methods and the stepwise procedure by a number of authors. Bertrand and Balding [ ] used a simulation study to assess three penalized methods (HyperLasso, Lasso, ridge regression) and the stepwise procedure and concluded that the power of HyperLasso, Lasso and the stepwise procedure were similar; however, the penalized approaches were preferred over the stepwise procedure because they offered higher speed and were less computationally demanding.
Tessier et al. [ ] performed a simulation study with a limited number of subjects to compare four association tests, the stepwise procedure and three penalized methods (HyperLasso, Lasso, ridge regression) for PK phenotypes estimated by non-compartmental analysis and the nonlinear mixed-effect model. Although all methods showed low ability to identify influential genetic variables, ridge regression had the highest probability of identifying variables, but also the highest number of false positives.
To the best of our knowledge, no previous study has applied AALasso and ALasso to covariate selection in nonlinear mixed effects models. The present study used AALasso, which incorporates the ratio of the standard error of the ML estimator to the ML estimator as the initial weight in nonlinear mixed-effect models using NONMEM. ALasso was also implemented with Lasso as the initial weight in NONMEM and the performance of AALasso was compared with ALasso and Lasso.
Theoretical
Population PKs using nonlinear mixed-effects models are hierarchical models involving both fixed and random effects. In the first stage, concentrations are usually assumed to be modeled as follows: $$C_{obs,ij} = f\left({x_{i},\,P_{i}} \right) + \varepsilon_{ij} \quad i = 1, \ldots,N\quad j = 1, \ldots,n,$$ (1) where N is the number of subjects, n is the number of observations per subject, C obs , ij is the j th observed concentration for subject i , f (·) is a known nonlinear function of covariate vector x i and individual parameter vector P i describing the prediction of concentrations and ε ij random errors which is normally distributed with a mean of zero and standard deviation σ .
Different PK models can be used to predict concentrations. For simplicity, the current study used a one compartment bolus input model: $$\hat{C}_{ij} = \frac{Dose}{{V_{i}}}\cdot\exp \left({- \left({CL_{i}/V_{i}} \right)\cdot t_{j}} \right),$$ (2) where \(\hat{C}_{ij}\) is the predicted concentration for the i th subject at time point t j and CL i and V i are individuals parameters for the i th subject.
In the second stage, individual parameter values are obtained as: $$CL_{i} = \exp \left({LnTVCL_{i} + \eta_{CLi}} \right),$$ (3) $$V_{i} = \exp \left({LnTVV_{i} + \eta_{Vi}} \right),$$ (4) where random effects CL i and V i are normally distributed with a mean of zero and standard deviation ω .
The natural logarithm of typical value of clearance, Ln ( TVCL ), is a linear function of fixed effect covariates as: $$Ln\left({TVCL_{i}} \right) = Ln\left({\theta_{CL}} \right) + \mathop \sum \limits_{k = 1}^{{N_{COV}}} \beta_{k} x_{ik},$$ (5) where θ CL is the typical population value of clearance, N COV is the number of covariates and β k and x ik are the covariate coefficient and the value of covariate k in subject i , respectively. Ln ( TVV i ) can be a linear function of covariates; however, for simplicity it is considered here to be only a linear function of Ln ( θ V ).
Lasso
One important aim in population PKs is to explain variability between subjects using relationships between individual parameters and covariates. Here, linear parameterization is assumed for a natural logarithm of typical value of CL according to Eq. ( 5 ) in NONMEM. Letting \(\hat{\beta} = ({\hat{\beta}_{1}, \ldots,\hat{\beta}_{k}})^{T},\) the Lasso estimates are defined as: $$\hat{\beta} = \arg min\left\{{\mathop \sum \limits_{i = 1}^{N} \mathop \sum \limits_{j = 1}^{n} \left({C_{obs,ij} - \hat{C}_{ij}} \right)^{2}} \right\}\quad {\text{subject}}\;{\text{to}}\;\mathop \sum \limits_{k = 1}^{{N_{COV}}} \left| {\beta_{k}} \right| \le t,$$ (6) where t is a tuning parameter which selects the model size and is estimated by fivefold cross validation [ , ]. Tuning parameter t shrinks the coefficients toward zero and any estimate within the ± cut-off is considered to be zero. Ribbing et al. [ ] used a cut-off at a small value of 0.005 when implementing Lasso in NONMEM. The Lasso coefficients were implemented as: $$\beta_{k} = \theta_{k}\cdot F,$$ (7) where θ k is the fixed effect estimate and F is a factor that makes the coefficients satisfy the Lasso restriction as: $$F = exp\left({1 - \frac{{\mathop \sum \nolimits_{k = 1}^{{N_{COV}}} |{\theta_{K}} |}}{t}} \right),$$ (8) where t is usually small enough to shrink the coefficient estimates in comparison with ML estimates. For more details see Ribbing et al. [ ].
Adaptive Lasso (ALasso)
ALasso in linear regression takes into account weights \(\hat{w}_{k}\) in the Lasso restriction as: $$\hat{w}_{k} \left| {\hat{\beta}_{k}} \right| \le t,$$ (9) where \(\hat{w}_{k}\) is chosen by a data dependent process.
Suppose \(\hat{w}_{k} = 1/| {\hat{\beta}_{k}} |^{\gamma},\) where \(\hat{\beta}_{k}\) is an OLS estimate and γ is a positive value [ , , ]. Here, Lasso from the first stage is used as the initial estimator and the weights are defined as: $$\hat{w}_{k} = \frac{1}{{| {\hat{\beta}_{k,Lasso}} |}}.$$ (10) For simplicity, γ = 1 was chosen [ ]. The tuning parameter is selected by fivefold cross validation in two stages. In nonlinear mixed-effect models, ALasso estimates can be solved using Algorithm 1 [ ].
Algorithm 1: (a) Find \(\hat{w}_{k}.\) (b) Define \(x_{k}^{**} = x_{k}/\hat{w}_{k}.\) (c) Solve the Lasso for all t values using the Lasso tool in PsN/NONMEM, $$\hat{\beta}^{**} = \arg min\left\{{\mathop \sum \limits_{i = 1}^{N} \mathop \sum \limits_{j = 1}^{n} \left({C_{obs,ij} - \hat{C}_{ij}^{**}} \right)^{2}} \right\}\quad {\rm subject\,to}\,\mathop \sum \limits_{k = 1}^{{N_{COV}}} \left| {\beta_{k}} \right| \le t,$$ where \(\hat{C}_{ij}^{**}\) is predicted according to x **. (d) Output \(\hat{\beta}_{k(ALasso)}^{*} = \hat{\beta}_{k}^{**}/\hat{w}_{k}.\)
Adjusted adaptive Lasso (AALasso)
Lasso as the initial weight in ALasso can result in poor coefficient estimation in the presence of multicollinearity. Accordingly, the ratio of the standard error of the ML estimator to the ML estimator was proposed by Algamal and Lee [ ] as an initial weight in ALasso for Poisson regression models. Here, this ratio is proposed as an initial weight for ALasso in nonlinear mixed-effect models. The standard error of the ML estimator \(s_{{k(\hat{\beta}_{ML})}}\) adjusts ALasso when the ML estimator is used as an initial weight in ALasso. Thus, the weights are defined as: $$\hat{w}_{k} = \frac{{s_{{k(\hat{\beta}_{ML})}}}}{{| {\hat{\beta}_{k(ML)}}|}},\quad k = 1, \ldots,N_{COV}.$$ (11) AALasso estimates are computed similarly to ALasso using Algorithm 1.
Methods
Simulation study
PK data was simulated using Algorithm 2 implemented in NONMEM:
Algorithm 2 (1) Different numbers of covariates are created by sampling from a multivariate standard normal distribution with no, low (0.2), moderate (0.5) and high (0.7) correlations. (2) A one-compartment bolus model is used as in Eq. ( 2 ). Different numbers of subjects and observations per subject and different values of correlation between CL and V are applied. (3) The vector of true coefficients β with different magnitudes is influenced only by Ln ( TVCL i ) as in Eq. ( 5 ). (4) Individual parameter values are obtained as in Eqs. ( 3 ) and ( 4 ) in which ω , θ CL and LnTVV i are considered to be 15%, Ln(2) and 0.01, respectively. (5) The observed concentrations were generated using Eq. ( 1 ). Two values of σ are used.
Simulation studies were conducted using set-ups A–D. Each set-up comprised different scenarios composed of a combination of different numbers of subjects, covariates and values of correlation between covariates and the correlation between CL and V . Table 1 includes further details about the simulation set-up. Table 1 The simulation model used for different set-ups Set-up True coefficients Number of covariates Correlation between covariates Correlation between CL and V Number of subjects σ (%) Number of observation Observation times A β = (0.25, 0.15, 0.1, 0.05, 0,…,0) 10 0, 0.2, 0.5, 0.7 Zero 20, 40, 80, 120 10 3 0.1, 2, 3.5 B β = (0.25, 0.15, 0.1, 0.05, 0,…,0) 10 0, 0.2, 0.5, 0.7 Zero 40, 60, 80, 100, 120 30 2 0.1, 2 a and 2, 3.5 b C β = (0.25, 0.15, 0.1, 0.05, 0,…,0) 10 0, 0.2, 0.5, 0.7 0, 0.3, 0.6, 0.9 40 10 3 0.1, 2, 3.5 D β = (0.25, 0.15, 0, 0, 0) c 5, 10, 15 0, 0.2, 0.5, 0.7 Zero 40 10 3 0.1, 2, 3.5 β = (0.25, 0.15, 0.1, 0.05, 0,…,0) d β = (0.25, 0.25, 0.15, 0.15, 0.1, 0.05, 0,…,0) e a,b The time observation for the first half of the individuals (a) the time observation for the second half of the individuals (b) c–e True coefficients when there are 5 covariates (c) true coefficients when there are 10 covariates (d) true coefficients when there are 15 covariate (e)
All scenarios in the set-ups were replicated 100 times. Training data was generated for every simulation scenario and for each replication and then used for model fitting. A large data set of 5000 individuals was generated as validation data for every simulation scenario. The number of observations per subject in the validation data was similar to that of the training data. The validation data was used to evaluate the methods.
Evaluation of methods
AALasso and ALasso were implemented in NONMEM and their performance was investigated for all scenarios. AALasso was compared to ALasso, Lasso and the true model, where the model structure was correct and non-null parameters were re-estimated from the data in terms of the performance and error of the estimated coefficient of the first covariate. In order to evaluate the performance of Lasso, ALasso and AALasso, the mean absolute prediction error (MAE) was computed for the validation data sets using MAXEVALS = 0. MAE for a method in a given scenario was calculated as: $$MAE = \frac{1}{100}\mathop \sum \limits_{\upsilon= 1}^{100} \left({\frac{1}{{N_{\upsilon}}} \mathop \sum \limits_{n = 1}^{{N_{\upsilon}}} \left| {C_{obs,n\upsilon} - \hat{C}_{n\upsilon}} \right|} \right)\cdot 100\%,$$ (12) where C nυ and \(\hat{C}_{n\upsilon}\) are the observed and predicted n th concentration out of N υ observations in the validation data set. The error for the estimated coefficient of the first covariate is defined as: $$Error = \hat{\beta}_{1} - \beta_{1},$$ (13) where β 1 is the true value of the coefficient for the first covariate and \(\hat{\beta}_{1}\) the estimated coefficient.
Software
This study was conducted in NONMEM (version 7.3) aided by PsN (version 4.5.10). The SSE and Lasso tools in PsN were applied to simulate and estimates all models. Adaptive and adjusted options for running ALasso and AALasso were added to the Lasso tool in PsN. The statistical software RStudio 0.99.491 using R 3.3.0 and ggplot2 2.1.0 were implemented to generate statistical analysis and graphical presentations.
Results
Figure 1 a for set-up A shows the MAE of AALasso, ALasso, Lasso and the true model versus the number of subjects in combination with different levels of correlation between covariates. It is evident that AALasso had better predictive performance (lower MAE) than ALasso and Lasso when the number of subjects was low, but as the number of subjects increased, the difference between the methods decreased. Fig. 1 a MAE for AALasso, ALasso, Lasso and true model, investigated across different number of subjects ( x -axis) and different values of correlation among covariates ( panels ) for set-up A. b The error of the estimated coefficient of first covariate for AALasso, ALasso, Lasso, investigated in 16 scenarios for set-up A
Figure 1 b shows the box plots of errors for the estimated coefficient of the first covariate versus the number of subjects for different levels of correlation between covariates for set-up A for each method. The median and/or variability of the estimates favor AALasso over the other methods, particularly for small data sets and when the correlation between covariates was high.
Figure 2 a shows the performance of the methods for set-up B where individual data was less informative than in the previous set-up, because the number of observations per subject decreased from 3 to 2 and σ increased from 10 to 30%. The observations were similar to those for Fig. 1 a, except that ALasso performed much worse than AALasso and Lasso in all scenarios. Figure 2 b also illustrates the poorer performance of ALasso compared to the other methods. AALasso had the lowest error in all scenarios, even when dealing with high correlation between covariates or a high number of subjects. Fig. 2 a MAE for AALasso, ALasso, Lasso and true model, investigated across different number of subjects ( x -axis) and different values of correlation among covariates ( panels ) for set-up B. b The error of the estimated coefficient of first covariate for AALasso, ALasso, Lasso, investigated in 20 scenarios for set-up B
Figure 3 a shows for set-up C that the MAE of each method and the true model versus the levels of correlation between CL and V . AALasso performed slightly better than Lasso, especially when the correlation between covariates was low. ALasso performed poorly in all investigated scenarios. Increasing the correlation between CL and V decreased MAE for all methods. As in the two first set-ups, the error for the estimated coefficient using AALasso was lower than for the other methods (Fig. 3 b). The figure also shows a decrease in imprecision as the level of correlation between CL and V increased. Fig. 3 a MAE for AALasso, ALasso, Lasso and true model, investigated across different values of correlation between CL and V ( x -axis) and different values of correlation among covariates ( panels ) for set-up C. b The error of the estimated coefficient of first covariate for AALasso, ALasso, Lasso, investigated in 16 scenarios for set-up C
Figure 4 a shows, for set-up D, the MAE of all methods versus the number of covariates for different levels of correlation between covariates. When only five variables were present, no obvious distinction could be made between the three methods and their MAEs were close to that of the true model. When 10 covariates were applied, AALasso outperformed the other two methods when the level of correlation between covariates was low. When 15 covariates were used, AALasso performed the best whatever the level of correlation between covariates. It is apparent that the difference in MAE between AALasso and Lasso decreased as the level of correlation increased and that ALasso performed the worst. Fig. 4 a MAE for AALasso, ALasso, Lasso and true model, investigated across different number of covariates ( x -axis) and different values of correlation among covariates ( panels ) for set-up D. b The error of the estimated coefficient of first covariate for AALasso, ALasso, Lasso, investigated in 12 scenarios for set-up D
Discussion
The current study investigated a version of Lasso called AALasso for simultaneous estimation and covariate identification in nonlinear mixed-effect models. The proposed method and also ALasso with Lasso as the initial weight were implemented in NONMEM and PsN. Using a simulation-based framework in four set-ups, the performance of AALasso was assessed and compared to ALasso and Lasso in terms of prediction error (MAE) and error of the estimated coefficient of the first covariate.
Comparison of the MAE for the three methods indicated that AALasso was the best method followed by Lasso and ALasso for identifying influential covariates for small data sets (Figs. 1 a, 2 a, 3 a, 4 a). The basic idea behind AALasso is that a large ML standard error can lead to a poor ML estimate far from zero when the number of covariates is not low relative to the number of subjects and also when the covariates are correlated [ ]. Taking into account the standard error of the initial weight for ALasso can considerably improve the predictive model, particularly in small data sets. In large data sets, any notable difference can not be observed between methods (Figs. 1 a, 2 a). Ribbing et al. [ ] showed that Lasso could outperform the stepwise procedure for a small data set; however, the advantage was insignificant in large data sets. Lasso and its extended versions, ALasso and AALasso, are introduced to achieve efficient covariate selection when the number of subjects is not sufficiently large relative to the number of covariates. It is important to determine which model performs best for small data sets because, as the size of data set increases, the performance of Lasso, the extended versions and the stepwise procedure becomes similar.
It is shown that there are advantages for AALasso over the other two methods when the covariates are not highly correlated. Increasing the level of correlation between covariates increases the MAE of all methods and the true model. This finding is in accordance with the results of Bonate [ ] who found that when the collinearity between predictor variables in nonlinear mixed-effect models increased, the parameter estimates became increasingly biased and their standard error increased markedly. It was also shown that, in linear regression, increasing the degree of collinearity between covariates led to poor prediction for the OLS–ALasso and AALasso [ ]. Qian and Yang [ ] compared the performance of AALasso to OLS–ALasso with respect to correlation structures between covariates and found that AALasso outperformed OLS–ALasso in most scenarios. Another study investigated the performance of AALasso in high-dimensional Poisson regression and found smaller values for mean square error for AALasso, followed by OLS–ALasso and Lasso, for small, medium and high correlation between covariates [ ].
ALasso with Lasso as the initial estimator performed worse than Lasso in all set-ups, as shown in Figs. 1 a, 2 a, 3 a and 4 a, specifically for set-up 2, which was less informative and had fewer observations per subject than the other set-ups. These findings are not in line with the results of Bühlmann and Van De Geer [ ] in which Lasso was the initial estimator to overcome Lasso’s overestimation behavior. It is worth emphasizing that Lasso as the initial estimator was proposed in a high-dimensional context with a very large number of uncorrelated covariates compared to the number of subjects and when the number of covariates was larger than the number of subjects, as for example in gene selection in DNA microarray data [ ]. However, in the current study, the number of covariates was not very large but was reasonable for population PK models. Moreover, the value of the true coefficients was rational [ ], but small, compared to the true coefficients in linear regression models [ , , ] and correlation also existed among all pairs of covariates.
In the presence of multicollinearity, Lasso is not an adequate method for correctly selecting covariates [ ]. For small values for true coefficients, ALasso with Lasso as the initial weight tends to select the first covariate with the largest coefficient of 0.25 as the influential covariate and drops the other covariates from the model, which creates a large value for MAE and poor predictive performance. It also results in a smaller error for the coefficient of the first covariate for ALasso than for Lasso, as shown in all box plots.
It is worth mentioning that in set-up C, as the correlation between CL and V increased, the MAE of all methods and the true model dropped markedly (Fig. 3 a). The decrease in MAE stems from the fact that increasing the correlation between CL and V increases knowledge about CL because the sampling schedule is informative with respect to V. This helps to explain the variability in CL .
One interesting finding is that the MAE of the three methods and the true model increased as the number of covariates increased from 5 to 15. AALasso showed the least increase in MAE, including for the true model, particularly for 15 covariates (Fig. 4 a). The rationale for this increase in MAE is that increasing the number of covariates decreases the value of the tuning parameter which in turn decreases the estimated coefficients. This results in an increase in the MAE. These results are in line with the slightly increasing negative log-likelihood for Lasso in the generalized linear mixed effect model reported by Schelldorfer et al. [ ].
Major differences between the current study and Ribbing et al. [ ] with pharmacogenetics studies [ , , , ] should be noted. First, in the current study and Ribbing et al. [ ], the dependent variable in Eq. ( 6 ) for Lasso, ALasso and AALasso was the observed concentration whereas, in the pharmacogenetics studies, the empirical Bayes estimate of the individual PK parameters estimated by nonlinear mixed-effect models was the dependent variable. Consequently, penalized methods in the pharmacogenetics studies could not evaluate the effect of covariates on individual PK parameters such as CL and V simultaneously. For a nonlinear mixed-effect model, the correlation between individual parameters is not considered by the pharmacogenetics studies, as it was in the current study.
In the pharmacogenetics studies, the Lasso and HyperLasso estimates corresponded to a posterior mode estimate when DE and NEG priors were placed on the regression coefficients, respectively. In the current study and Ribbing et al. [ ], the Bayesian perspective was not applied in Lasso, ALasso and AALasso. Finally, the current study and Ribbing et al. [ ] simulated data sets in which the number of covariates was lower than the number of subjects. In the pharmacogenetics studies, the number of covariates was much greater than the number of subjects; thus, the Bayesian Lasso algorithms were able to select influential covariates in a high-dimensional context. In the pharmacogenetics studies, because the number of covariates was greater than the number of subjects, penalized methods were compared using counts of true and false positives or the empirical family-wise error rate [ ]. In the current study, Lasso, ALasso and AALasso were compared using MAE and the error of the estimated coefficient of the first covariate. There was no common criteria for comparison of the performance of the Lasso between the evaluated algorithms and the Bayesian Lasso algorithms.
Contradictory results have been reported for comparisons of Lasso and the stepwise procedure. Ribbing et al. [ ] found Lasso to be superior to the stepwise procedure, particularly when the number of subjects was less than 120. A similar probability for detection of SNPs for Lasso and stepwise was reported in the pharmacogenetics studies [ , ]. These conflicting results could be because different kinds of dependent variables were used (observed concentrations versus empirical Bayes estimates of individual PK parameters) and the designs of the simulations differed between studies. Whatever the dependent variable, Lasso was faster than the stepwise procedure and is recommended for variable selection [ , ].
In the present study, a one-compartment IV bolus model was used; however, this simple structural model cannot be considered as an important limitation to the results presented. The strategy for building covariate models and analyzing covariate relations in previous studies [ , , ] has not used complexity of the underlying structural model as a determinant for the applicability of a particular method. There is, thus, no reason for the results of the current study to be considered irrelevant for more complex structural models. It can be expected that for more complex models there will be practical difficulties in obtaining the full model parameters standard errors, which are needed for the AALasso. The covariance step of NONMEM may fail, but this in itself does not prevent the use of AALasso. Sampling importance resampling [ ] is a robust method for parameter uncertainty. It is available in PsN and can be used instead of the covariance step. Further, parameter estimation in the Lasso model will be more challenging as model complexity increases. Techniques such as restarting NONMEM with different initial parameter estimates or a non-gradient-based estimation method may be required.
The current study only explored the predictive performance of AALasso, ALasso and Lasso when the number of covariates was less than the number of subjects. It also implemented three methods when the number of covariates was greater than the number of subjects, but encountered problems obtaining successful minimization in NONMEM.
Conclusions
In summary, the successful performance of AALasso as a penalized method for selection of influential covariates in small data sets was demonstrated in a simulated one-compartment bolus input model. The method performed better than ALasso and Lasso, even in the presence of high multicollinearity. The method offered a lower MAE and error of the estimated coefficient of the first covariate, making it more promising than ALasso and Lasso, especially when dealing with low correlation between covariates and a low number of subjects.