Introduction
Atrial fibrillation (AF) is the most common cardiac arrhythmia and confers increased risk of cardioembolic stroke [ , , ]. Classically, it is thought that poor atrial contractility in AF leads to blood stagnation and thrombus formation within the left atrium (LA), specifically the left atrial appendage (LAA), with subsequent embolic events. However, the process of thrombogenesis in AF is likely much more complex [ , ]. A number of risk stratification scores have been derived to determine in which patients is anticoagulation (AC) warranted. But as long-term data from implantable cardiac devices becomes available, limitations on current scoring systems have become apparent. Undiagnosed asymptomatic paroxysmal AF (PAF), also known as subclinical AF (SCAF), affects a significant fraction of the population [ ]. For those who have undiagnosed SCAF, risk stratification scores have never been applied. Additionally, the clinical implications of data from implantable cardiac devices such as AF duration and timing remain to be determined and are not accounted for in current risk stratification schemas. We set out to review current risk stratification scores for stroke in AF, specifically their shortcomings, and to review the current evidence regarding long-term cardiac monitoring in AF risk stratification.
Current Risk Stratification Scores
The risk of stroke in non-valvular AF is increased fivefold compared to patients without AF, and anticoagulation is a cornerstone of AF management to ameliorate this risk [ ]. To identify those at increased risk of stroke and who would therefore benefit most from anticoagulation, many different risk stratification tools have been developed, notably the AFI (Atrial Fibrillation Investigators), SPAF (Stroke Prevention in Atrial Fibrillation) scoring system, Framingham score, CHADS 2 (Congestive heart failure, Hypertension, Age, Diabetes, Stroke/transient ischemic attack), CHA 2 DS 2 -VASc (CHADS 2 plus vascular disease and sex category), ATRIA (Anticoagulation and Risk Factors in Atrial Fibrillation), and ABC (Age, Biomarkers, Clinical history) scores (Table 1 ). Table 1 Risk scores in AF Scoring system Year Criteria AFI [ ] 1994 Hypertension Age > 65 History of stroke/transient ischemic attack (TIA) Diabetes SPAF [ ] 1999 Age > 75 and female sex SBP > 160 mmHg Heart failure (clinical or echocardiographic) History of stroke Framingham [ ] 2003 Age (55+) Blood pressure Diabetes Smoking Prior myocardial infarction/CHF Murmur heard on exam ECG criteria for left ventricular hypertrophy CHADS 2 [ ] 2001 Congestive heart failure Hypertension Age > 75 Diabetes History of stroke/TIA CHA 2 DS 2 -VASc [ ] 2010 Congestive heart failure Hypertension Age > 75 Diabetes History of stroke/TIA Vascular disease history Female ATRIA [ ] 2011 Age 65–85 Female Diabetes Previous stroke Chronic heart failure HTN Proteinuria eGFR < 45 or ESRD ABC [ ] 2016 Age N-terminal fragment B-type natriuretic peptide Cardiac troponin high-sensitivity Prior stroke/TIA
These tools use many of the same patient characteristics/risk factors. A comparison of 12 risk stratification scoring systems, including the aforementioned scores except CHA 2 DS 2 -VASc, ATRIA, and ABC, showed a five- to sevenfold difference in the fraction of patients found to be “low” vs. “high” risk [ ]. The differences in risk stratification using the different tools are due in part to the use of some invalidated risk factors, the differential weighing of the individual risk factors, and the inability to take into account dynamic risk factors, like hypertension [ ]. The first risk stratification system was the AFI, which used data from five randomized controlled trials to produce a scoring system that included age, history of stroke, diabetes history, and hypertension [ ]. Following the AFI came the SPAF scoring system from the SPAF I, II, III trials. These criteria included history of hypertension (systolic blood pressure > 160 mmHg), history of prior stroke, recent heart failure exacerbation using either clinical or echocardiographic features, and female sex and age greater than 75 years of age as a risk for thromboembolism [ ]. The Framingham risk calculator was derived from the eponymous cohort, included only patients not on warfarin, and included many of the risk factors that AFI and SPAF used (age, blood pressure, previous stroke, sex). Framingham was different from AFI and SPAF in that it was the first to use a gradation of age and blood pressure and ascribe subsequently higher risk scores for increasing age and blood pressure, as well as using left ventricular hypertrophy criteria on electrocardiogram, and a heart murmur heard on exam [ ]. Next came the CHADS 2 score, using congestive heart failure, hypertension, age, diabetes, and previous stroke or transient ischemic attack. This data was collated from the National Registry of Atrial Fibrillation (NRAF) data using chart review instead of ICD (International Classification of Diseases and Related Health Problems) coding, allowing for more accurate patient data collection [ ]. Within the NRAF cohort, CHADS 2 was shown to have a higher predictive value than either of the other risk stratification scores previously described, but was not shown to be superior to AFI, SPAF, or Framingham when it was validated using data from the ATRIA study group [ , ]. A major weakness of the CHADS 2 was that a higher proportion of patients were stratified into the “intermediate” risk group, without clear indication for or against anticoagulation. The CHA 2 DS 2 -VASc score, additionally incorporating vascular disease, sex, and graded age as a risk factor, was found to be a superior predictor of thromboembolic events as compared to CHADS 2 . It also decreased the fraction of patients stratified into the “intermediate risk” group, better identifying the true “low” risk group that was not in need of anticoagulation [ , ]. Because of its superior risk stratification, the CHA 2 DS 2 -VASc score has been incorporated into both the American and European guidelines to identify patients who benefit from anticoagulation in AF [ , ].
Despite being superior to AFI, SPAF, Framingham, and CHADS 2 , the CHA 2 DS 2 -VASc still has limitations, namely ignoring biochemical and morphological data; only clinical data is used. Renal dysfunction, a known risk factor for thromboembolism, is not incorporated into the CHA 2 DS 2 -VASc [ ]. Other biochemical data previously described such as markers of hypercoagulability, D-dimer levels, von Willebrand factor (vWF), and markers of endothelial damage, are not taken into account in the CHA 2 DS 2 -VASc score either [ , ]. Lastly, echocardiographic risk factors, such as left atrial size and spontaneous echo contrast, despite being associated with increased thromboembolic risk, are not incorporated into the CHA 2 DS 2 -VASc score [ , ]. All risk factors are equally weighted and treated uniformly; control of risk factors is not accounted for, and age only has two cutoffs without progressively increasing risk with increasing age. Biochemical data ignored by the CHA 2 DS 2 -VASc is not limited by this static risk assessment; like stroke risk, these data are measured on a continuum and not limited to binary risk assessment [ ]. Additionally, the most recent ACC/AHA guidelines redefined hypertension, and there is now a lower blood pressure cut-off for the definition of “hypertension.” It is unknown how clinicians should incorporate this new data into risk stratification models and how the risk prediction will change with the lower definition of hypertension [ ].
The ATRIA score addresses some of these shortcomings. It incorporates many of the same risk factors as the CHA 2 DS 2 -VASc score, but also takes into account additional risk factors that CHA 2 DS 2 -VASc does not include, such as estimated glomerular filtration rate (eGFR) and proteinuria. Additionally, the ATRIA score uses a graded scoring system to assess risk in the elderly population and drops the “vascular” category when risk stratifying [ ]. The ATRIA score was developed initially within the ATRIA cohort and externally validated using the Anticoagulation and Risk Factors in Atrial Fibrillation-Cardiovascular Research Network (ATRIA-CVRN) cohort, and the Swedish AF (SAF) study group, which was the cohort used to validate the CHA 2 DS 2 -VASc for use in the ESC guidelines [ , ]. The ATRIA score has consistently outperformed both CHADS 2 and CHA 2 DS 2 -VASc in predicting ischemic stroke risk in the SAF study group (C-indices of 0.708, 0.690, and 0.694 respectively) [ , ]. Not only has ATRIA outperformed CHA 2 DS 2 -VASc at predicting stroke, but it more accurately identifies the low risk group who may not benefit from anticoagulation [ ].
The ABC score, which was derived from a cohort of 14,701 patients in the Apixaban for Reduction in Stroke and Other Thromboembolic Events in Atrial Fibrillation (ARISTOTLE) trial, like ATRIA, incorporates biochemical data (eGFR, urine protein), also outperformed CHA 2 DS 2 -VASc (C-indices 0.66 vs. 0.58, p < 0.001) in stroke risk prediction when externally validated in a different 1400 patient cohort [ ].
Although they do improve risk prediction for thromboembolism, ATRIA and ABC are still susceptible to many of the shortcomings that plague all other risk stratification systems. For instance, not all strokes occurring in AF patients are cardioembolic in origin [ ]. An analysis of the SPAF study data by neurologists revealed that of the 217 ischemic strokes occurring in patients taking dose-adjusted warfarin, 56% were deemed “probably” non-cardioembolic in origin [ ]. AF patients share many of the same risk characteristics, such as hypertension, diabetes, and vascular disease, among others, that can predispose to other stroke etiologies. These data support the notion that risk factors for atherosclerotic disease, cerebrovascular disease, and AF all overlap, something that anticoagulation cannot fully address, nor can any risk stratification system completely take into account. Also, none of the scoring systems take into account underlying procoagulable states that confer elevated thromboembolic risk, such as cancer. In 24,125 patients with newly diagnosed cancer, the CHADS 2 did not accurately predict thromboembolism risk in those individuals who developed new-onset AF in setting of active malignancy [ ]. Lastly, the risk stratification scores cannot take into account subclinical AF which increases risk of thromboembolic events [ , ]. Risk stratification cannot be applied to a patient population without first identifying the at-risk patient population.
Limitations to AF Risk Stratifying Scores
Information on Stroke Risk from LA Structure, Function, and Milieu
Traditionally, it is thought that uncoordinated atrial contractility in AF leads to blood stagnation and thrombus formation within the LA appendage (LAA), resulting in cardioembolic stroke. Three separate factors all contribute to the hemostasis within the LA: the loss of atrial systole, anatomy and dysfunction of the LAA, and LA dilatation [ ]. Hemostasis from the loss of atrial systole that occurs in AF can be quantified by observing spontaneous echo contrast (SEC) or “smoke” during transesophageal echocardiography (TEE) [ ]. Many studies have quantified SEC in AF. “Dense” SEC is associated with other risk factors for stroke, notably increasing LA size, and it has been associated with increased future risk of systemic thromboembolism, strokes, and mortality in AF [ , , , ]. Additionally, there is evidence that reduced LAA emptying velocity is associated with clot risk. In one study, patients with LAA flow < 25 cm/s on TEE were found to have a 17% chance of thrombi compared 5% in higher velocities [ , ]. Slow LAA velocity (odds ratio 3.59, 95% confidence interval 1.42 to 9.08, p = 0.007) was found to be significant risk factors of stroke [ ]. However, the process of thrombogenesis in AF is likely much more multifaceted and includes, in addition to slow blood flow within the LAA, structural defects of the LA and LAA, and a systemic prothrombotic state that occurs in AF [ , ].
Structural Changes Within LA/LAA
Structural and functional changes within the LA/LAA have also been implicated as risk factors for stroke in patients with AF. LA size index is an independent risk factor for recurrent cardioembolic and cryptogenic stroke (hazard ratio 2.83 compared to normal LA size) in patients with and without AF, as shown in the Northern Manhattan Stroke Study [ , ]. Lee et al. also showed that large LAA orifice area (odds ratio 6.16, 95% confidence interval 2.67 to 14.18, p < 0.001) is independently associated with increased risk of stroke in patients with persistent AF [ ]. Data from the Effective Anticoagulation with Factor Xa Next Generation in Atrial Fibrillation-Thrombolysis in Myocardial Infarction 48 (ENGAGE-AF TIMI 48) show that increased burden of AF, as differentiated by paroxysmal, persistent, or permanent AF, was associated with increase in LA size and increased LA contractile dysfunction [ ]. The LAA has unique anatomy with multiple trabecular and a narrow opening. Dilation of the LAA, along with reduced contractility, is associated with thrombus formation in AF [ ]. These characteristics predispose the LAA to hemostasis, and it is the most common origin of thrombi within the LA regardless of AF or sinus rhythm. Up to 90% of thrombi originate within the LAA [ , ]. Thus, poor contraction of the LAA and subsequent dilatation are both associated with increased stroke risk [ ].
Post-mortem examination of atrial endothelium in AF patients who died of cerebral embolism has shown endothelial edema, fibrosis, and thickening [ , ]. Underlying these changes may be abnormal activity within the extracellular matrix (ECM). The interaction of collagen, matrix metalloproteinases (MMPs), and tissue metalloproteinases (TMPs) controls the ECM and its constant remodeling. This regulated process is disrupted in AF, leading to fibrosis and distortion of the microstructure, resulting in larger structural and functional changes within the LA [ ]. The changes go beyond the dilatation of the LA and LAA, with both endothelial and ECM dysregulation contributing to the enhanced thrombogenesis seen in AF. Endocardial remodeling in AF involves a complex interplay between oxidative stress, LA dilatation, mishandling of calcium, as well as other poorly understood pathological mechanisms. This remodeling induced by AF itself perpetuates AF [ ].
Atrial Cardiomyopathy
It is becoming increasingly recognized that both macroscopic and histopathological changes occur within the atria during disease states, particularly AF. These changes lead to an “atrial cardiomyopathy” that itself can result in clinically relevant manifestations. This disease state of the atria can lead to fibrosis, structural remodeling, and remodeling of the conduction systems, creating a pro-arrhythmic milieu within the atria. These changes can become irreversible in the long-term which leads to a permanent atrial cardiomyopathy [ ].
Prothrombotic State
The risk of stroke remains elevated in patients who have converted from AF into sinus rhythm, and rhythm control strategies have not shown a decrease in stroke or mortality risk in long-term follow-up studies [ , ]. In addition to “atrial stunning,” the temporary depression in LA function after cardioversion, a persistent prothrombotic state in AF patients is believed to contribute to the elevated risk of stroke despite being in sinus rhythm [ ].
The prothrombic state is due to inflammation, growth factors, endocardial changes, and platelet and coagulation factors. AF is an inflammatory condition, and inflammation drives both AF and predisposes to thrombogenesis. Inflammation, measured through indices like interleukin-6 (IL-6) and C reactive protein (CRP) levels, has been associated with increased stroke and mortality risk in AF [ , , , , ]. Other drivers of the prothrombotic state in AF include growth factors, endothelial changes, and changes in coagulation. Vascular endothelial growth factor (VEGF) a pro-angiogenesis growth factor that plays a role in the hypercoagulability in cancer, is increased in AF [ ]. VEGF increases tissue factor expression in endothelial tissue, allowing activation of the extrinsic pathway of the coagulation cascade and thrombogenesis [ ]. Increased MMP activity in AF may also contribute to the procoagulant state through decreasing the activity of plasmin, thereby decreasing fibrinolysis [ , ]. Finally, within the endocardium, nitric oxide (NO) acts as an antithrombotic through vasodilatory and antiplatelet effects, and is increased in high shear-stress environments. With the loss of coordinated atrial contraction, there is a loss of shear-stress and decreased NO levels (and its antithrombotic activity) [ , ].
Primary hemostasis involves both platelets and cellular factors like von Willebrand factor (vWF), a marker of endothelial damage and dysfunction [ ]. vWF concentration independently predicts presence of LAA thrombus, and post-mortem studies of patients with cardioembolic stroke have shown excessive vWF expression within the LA [ , ]. vWF concentration has been associated with both increasing rates of AF as well as stroke risk in patients with AF who were taking aspirin [ , ]. The levels of another marker of platelet activation, β-thromboglobulin (BTG), are also directly correlated with AF [ , ]. BTG, a protein within platelets granules, plays an integral role in platelet activation, aggregation, and hence thrombus formation. Levels are inversely associated to LAA flow velocities, pointing to a link between stasis and platelet activation through BTG activity [ ]. Despite these data, clinical studies have not shown a decrease in stroke rates or mortality in patients treated with antiplatelets compared with oral anticoagulants, therefore the clinical implication of the increased platelet activation is still being investigated [ , ]. Two markers of the coagulation cascade are related to the presence of AF and thromboembolic risk: D-dimer and prothrombin fragment levels. D-dimer levels are associated with the presence of thrombus within the LAA, and low D-dimer levels (< 1.5 μg/mL) have a 97% negative predictive value for LAA thrombus [ ]. Increased prothrombin fragments were higher in patients with stroke and AF compared to those in sinus rhythm [ ]. Thusly, such markers of coagulation may serve a role in future AF risk stratification models. As illustrated, the prothrombotic state in AF is a complex process involving abnormal endothelial and ECM interactions as well as dysfunctional primary and secondary hemostasis.
Diagnosis of Subclinical AF
The aforementioned AF risk stratification scoring systems are validated for those with known, documented AF. AF is traditionally diagnosed on standard electrocardiogram (ECG) which captures seconds of rhythm data or on 24-h ambulatory Holter monitoring in individuals with suspicion for AF [ , ]. However, in cases of paroxysmal AF (PAF), diagnosis remains a challenge as diagnostic testing must occur during episodes of AF. Symptom-based timing of ECG is suboptimal, as 65–80% of episodes of AF are asymptomatic [ , , , ]. Moreover, AF symptoms are also non-specific with a positive predictive value of less than 60% [ ]. Given poor relation to symptoms, it becomes little more than a matter of chance if a PAF event is captured during standard ECG. Twenty-four hour-Holter monitoring extends the observation period from seconds to 24–48 h; however, detection rates are still poor with sensitivity ranging from 44 to 60% when compared to long-term continuous monitoring [ , , , ]. Using techniques to extend monitoring even further, a number of studies have shed light on the potential burden of SCAF and its true clinical significance.
Extended Non-invasive Testing
Increasing the window of cardiac monitoring increases the yield of detection. The FIND-AF trial randomized 398 patients without known AF history after stroke to either standard of care monitoring (minimum 24 h) or 10-day Holters at baseline, 3 months, and 6 months and found a 5% rate of AF (lasting 30 s on rhythm strip or long enough to capture a 12-lead ECG) in the control arm versus a 14% rate of AF in the study arm ( p = 0.002) [ ]. The 30-Day Cardiac Event Monitor Belt for Recording Atrial Fibrillation After a Cerebral Ischemic Event (EMBRACE) trial examined the use of an externally worn, event-triggered loop recorder to monitor for SCAF in patients with recent cryptogenic stroke. The device was a belt with a dry electrode that was programmed to record when irregular R-R intervals were detected over 30 beats; if episodes lasted longer than 30 s, diagnosis of SCAF was conferred. The trial included 572 post-stroke patients with no known AF, post-stroke ECG and 24-h Holter without evidence of AF, 55 years of age or older, and with cryptogenic stroke or TIA (i.e., no etiology identified after standard work up) in the prior 6 months. Participants were randomized to 30-day external loop recorder versus repeat 24-h Holter. AF was detected in 16% of the external loop recorder group and only 3.2% of the Holter group, suggesting a significant interval improvement in detection when the observation window was extended. Moreover, it suggests that a sizeable proportion of previously diagnosed cryptogenic strokes is potentially AF-related, with clinical implications for anticoagulation. However, compliance with an externally worn device remained an issue; only 62% without detected AF had completed the 30-day protocol [ ].
There is also interest in using smart phone technology as a means of detecting AF. The SEARCH-AF study used an iPhone-based ECG screening program in the UK pharmacies. In 1000 persons screened, new AF was found in 1.5%, and the automated algorithm had a sensitivity of 98.5% and specificity of 91.4% when the ECG data obtained by the smartphone was transmitted to and verified by a cardiologist [ ]. The SPOT-AF trial equipped nurses with a smart phone-based ECG device in 294 patients admitted after stroke so that spot ECGs could be collected with routine vital signs. It detected AF in 8.5% of patients compared to 2.7% in the standard Holter arm. Versions of similar technology have been approved for smart watches as well [ ]. The Assessment of Remote Heart Rhythm Sampling Using AliveCor Heart Monitor to Screen for Atrial Fibrillation (REHEARSE-AF) Study randomized 1001 ambulatory patients 65 years old or older with a CHA 2 DS 2 -VASc score greater than or equal to 2 to twice weekly AF screening using a smartphone-based ECG device versus standard of care. The study arm was more likely to be diagnosed with AF (HR 3.9 (1.4–10.4), p = 0.007); however, absolute rates of diagnosed AF were low, 3.8% in the study arm and 1% in the control arm. The cost per diagnosis was $10,780 [ ]. As technology improves and becomes cheaper, home rhythm monitoring may become more cost beneficial.
Invasive Monitoring
Extending monitoring even further, implantable cardiac devices such as implantable cardioverter-defibrillators (ICDs), permanent pacemakers, and implantable loop recorders (ILRs) have been used to monitor for AF continuously, for potentially indefinite lengths of time, without the limitations of patient compliance [ , ].
The Asymptomatic Atrial Fibrillation and Stroke Evaluation in Pacemaker Patients and the Atrial Fibrillation Reduction Atrial Pacing Trial (ASSERT) was a prospective study of 2580 patients greater than or equal to 65 years of age, with known hypertension, recent pacemaker and/or defibrillator insertion, no known history of AF, and not on anticoagulation. It followed participants for a mean of 2.5 years, monitoring for a primary endpoint of ischemic stroke of systemic embolism. It used the detection of an atrial rate of greater than 190 beats per minute for greater than 6 min as a surrogate for SCAF [ ]. Similar cut offs of atrial rate as a surrogate for AF have been examined in other studies and found to be more than 95% sensitive and specific [ ]. A total of 24.5% of patients enrolled were found to have SCAF. Of those found to have SCAF by 3-month follow-up, only 15.7% had been captured on standard ECG by that point, demonstrating the difficulty of diagnosing PAF. This delay in diagnosis was clinically significant; 4.2% of those found to have SCAF by 3-month follow-up had a ischemic stroke or systemic embolism, compared to 1.7% of those without SCAF (HR 2.49, 95% CI 1.28 to 4.85) [ ].
However, because ASSERT studied patients who had clinical indication for ICD or pacemaker, it may not be generalizable to patients without said indications. ASSERT-II used implantable loop recorders (ILRs) in 256 patients ≥ 65 years old with CHA2DS2-VASc score greater than 2, left atrial enlargement or elevated serum NT-preBNP, and either BMI > 30 or obstructive sleep apnea (OSA) followed for an average of 16 months [ ]. ILRs are devices that are implanted subcutaneously overlying the heart and recorded electrocardiographic activity continuously [ ]. It used a cut off of 5 min for classification of SCAF, and found a 34.4% person-year incidence under these parameters. The average weekly burden was only 3 min. Only 20% of those found to have SCAF were diagnosed with clinical AF by traditional means over the interval, again demonstrating the difficulty of capturing short-lived events, but demonstrating a significant disease burden in this cohort [ ].
The Cryptogenic Stroke and Underlying Atrial Fibrillation (CRYSTAL-AF) study included 441 patients 40 years old or older with history of cryptogenic stroke or transient ischemic attack (TIA) in the preceding 90 days with negative ECG and negative 24-h Holter or equivalent telemetry, no known AF history, no pre-existing indication or contraindication for oral anticoagulation, and no indication for ICD or pacemaker. They were randomized to undergo monitoring with an implantable cardiac monitor (ICM) that can detect AF independently of rate or a control group which received clinical follow-up. At 6 months, 8.9% of patients in the ICM group were found to have AF as opposed to 1.4% in the control (HR 6.4, CI 1.9–21.7). By 12 months, the gap expanded to 12.4% compared to 2% (HR 7.3, CI 2.6 to 20.8). In secondary analysis, higher rates of anticoagulant use were found in the ICM arm (14.1% compared to 6.0% at 12 months, p = 0.007); this is likely in response to new diagnosis of AF which was not blinded. There was a statistically insignificant trend to lower rates of new stroke or TIA in the ICM group versus the control (7.1% compared to 9.1%); however, the study was not adequately powered to evaluate this end-point [ ]. By 36-month follow-up, the AF rate was noted to be 30% compared to a 3% detection rate using conventional means. Of note, 2.4% of implanted devices had to be removed due to insertion site infection or pocket erosion [ ].
The findings of CRYSTAL-AF have since been replicated in real-world patient populations. A real-world cohort of 1247 patients after cryptogenic stroke was monitored with ECG-based implantable cardiac monitors, following for a mean of 6 months. Using a 2-min episode cutoff for device-specific technical reasons, AF rates of 4.6% and 12.2% were found at 1 month and 6 months respectively, an even higher incidence than found in CRYSTAL-AF. Episodes greater than 1 h were found in 70.7% of those with AF. Older patients had higher incidences; in those over 60 years old, the rate of AF at 6 months was 15% [ ]. On 2-year follow-up of the same cohort, AF was detected in 21.5% of patients [ ]. The Stroke Prior to Diagnosis of Atrial Fibrillation Using Long-term Observation with Implantable Cardiac Monitoring Apparatus Reveal (SURPRISE) group found a 20.7% SCAF rate when monitored for an average of 19.2 months [ ]. Israel et.al. found a 23.6% rate of SCAF in a series of 123 post-cryptogenic stroke patients who were followed for a mean of 12.7 months [ ]. While the clinical impact in terms of mortality or repeat stroke of extended monitoring using ICMs remains to be determined, the evidence to date demonstrates a sizable fraction of PAF goes undetected using traditional non-invasive monitoring techniques.
Moreover, even in non-cryptogenic, non-AF-related strokes, there appears to be a sizeable population of SCAF. Katz et al. monitored 51 patients after non-cryptogenic, non-AF strokes with ICMs for a median period of just over a year and found PAF in 11.8%. As they were not previously known to have AF, this had treatment implications for the initiation of anticoagulation [ ].
Subclinical tachyarrhythmias are likely clinically significant. An ancillary of the Mode Selection Trial (MOST) enrolled 310 patients with or without known AF and pacemaker for sinus node dysfunction and followed for an average of 33 months. It compared individuals with device-detected heart rates of > 220 beats per minute for greater than 10 min with those who did not, and found high rate atrial tachycardia to be an independent risk factor for death and nonfatal stroke (HR 2.79, CI 1.51–5.15) [ ].
Relationship Between Timing and Duration of PAF and Thromboembolic Risk
Common scoring systems such as ATRIA and CHA 2 DS 2 -VASc estimate risk of embolic event in those with known AF based on comorbid risk factors; however, neither system weighs the extent of the AF itself. These scoring systems would assign the same thromboembolism risk to an individual with rare PAF episodes as it would to one with heavy AF burden. Additionally, the temporal relation between PAF and thromboembolic events may help to clarify mechanisms of thrombosis. Data generated by long-term cardiac monitoring devices described above has shed some light on the effect of the duration and timing of AF in relation to thromboembolic event.
More Is More: AF Duration and Stroke Risk
It seems logical that higher burden of AF should be associated with a higher risk for stroke and embolic events. However, current guidelines do not take burden into consideration for treatment decisions. The 2014 American Heart Association/American College of Cardiology/Heart Rhythm Society Practice Guidelines on AF state that “selection of antithrombotic therapies should be based on risk of thromboembolism irrespective of whether the AF patter in paroxysmal, persistent, or permanent (Level of Evidence B)” [ ]. The 2016 European Society of Cardiology AF Guidelines state that AF burden “should not be a major factor in deciding on the usefulness of an intervention”; however, does go on to state that further evidence is needed [ ].
The Relationship Between Daily Atrial Tachyarrhythmia Burden from Implantable Device Diagnostics and Stroke Risk (TRENDS) study examined the relationship between AF burden and thromboembolic events. It enrolled 2486 patients with indication for ICD or pacemaker device and at least 1 stroke risk factor and observed them prospectively for a mean follow-up of 1.4 years. It enrolled regardless of prior AF; however, only included for analysis are those with prior AF or atrial tachycardia found during the study and those who remained off of warfarin. It used atrial tachycardia > 175 beats per minute lasting a minimum of 20 s as a threshold for what would qualify as a surrogate for AF. It then stratified patients into “no AF,” “low burden,” and “high-burden” groups. The maximum duration of AF on any given day during a 30-day rolling window was used to determine the cutoff between “low burden” and “high burden” groups. The “low burden” group had a maximum daily duration of AF of < 5.5 h; the “high burden” group had ≥ 5.5 h. It compared the rates of stroke and systemic embolism in the groups and found that the high-burden group had an event rate of 2.4% compared to 1.1% for the low and no burden group. The hazard ratio of the high-burden group when compared to the no burden group approached statistical significance in adjusted cox proportion hazard (HR 2.2 [0.96 to 5.05]) [ ]. The Stroke Prevention Strategies based on Atrial Fibrillation Information from Implanted Devices (SOS AF) project was a pooled analysis of over 10,000 patients with implanted cardiac devices with a mean follow-up of 2 years. Initial analysis did not show a relationship between duration of AF and stroke; however, once adjusted for higher anticoagulation rates in cohorts with longer duration of AF, the study demonstrated an increase in the risk of stroke if AF duration was greater than or equal to an hour than if less than an hour (HR 2.05, CI 1.24–3.39), p = 0.0051.
Although present risk scoring systems and guidelines do not weigh duration of AF in decision to start anticoagulation, it is likely that clinicians consider it when duration data is available. In the unblinded CRYSTAL AF study, 100% of patients with AF episodes greater than 1 h were started on anticoagulation, compared to 70% of those with episodes less than 1 h [ ]. More research is needed to evaluate how to weigh duration in treatment decision making. Two ongoing studies aim to address this issue: the Non-vitamin K antagonist Oral anticoagulants in patients with Atrial High rate episodes (NOAH) will randomize patients with > 1 stroke risk factor who experience SCAF, but without clinical AF diagnosis to either edoxaban therapy or no anticoagulation therapy and compare rates of stroke and cardiovascular death [ ]. The Apixaban for the Reduction of Thrombo-Embolism in Patients With Device-Detected Sub-Clinical Atrial Fibrillation (ARTESiA) study will randomize similar patients with SCAF to either apixaban or low-dose aspirin therapy and study stroke, TIA, or systemic embolism as the primary outcome [ ].
A confounding issue is that the duration of PAF may significantly vary over time, so risk estimation could significantly change over time. However, we will be able to address it only retrospectively. Additionally, AF burden may affect different patient populations differently. Botto et al. analyzed data from 569 patients with brady-tachy syndrome, AF and implanted pacemakers and compared thromboembolic event rates in groups stratified by CHADS2 score and AF burden. They found that patients with maximal daily duration of AF of < 5 min and CHADS2 score of ≤ 2, patients with > 5 min but < 24 h of and CHADS2 ≤ 1, or 24 h of AF and CHADS2 = 0 had a lower risk of thromboembolic event than patients with combinations of higher AF burden or higher CHADS2 scores (0.8% vs. 5%, p = 0.035). This suggests that the effect of AF burden is synergistic with underlying risk thromboembolic risk factors, and treatment decisions may need to be tailored accordingly [ ].
Timing Is Everything?: Temporal Relation Between AF Events and Stroke Risk
Numerous analyses of the aforementioned data sets have examined the temporal relationship between AF events and thromboembolic events, with some perplexing findings. Looking at the CRYSTAL-AF data set, the median time to detection of AF was greater than 8 months after stroke, casting doubt on temporal causality [ ]. It should be noted however that regardless of temporal association, the AF data gleaned is still clinically valuable in determination of need for anticoagulation for secondary stroke prevention. A subgroup analysis of the TRENDS data set examined the relationship in time between AF events and thromboembolic events. In the cohort, 40 stroke or systemic embolic events occurred, and atrial tachycardia (AT) or AF was diagnosed at any time in 80% of those cases. Of those that had AT/AF prior to stroke or systemic embolic event, only 30% were in AF at the time of event diagnosis, reiterating the challenges in identifying AF in stroke patients. In the 30-days preceding events, AF/AT was found only 50% of the time. A total of 30% had AF/AT in the 30 days following stroke or systemic embolic events [ ]. Brambatti et.al. analyzed the previously mentioned ASSERT data set to examine for the temporal relationship between SCAF and strokes and found an even lower rate of SCAF in the 30-day period preceding stroke or systemic embolic events with only 8% of the total. This represented only 15% of the events of those who had SCAF at any time during monitoring. For context, 51% of those with stroke or systemic embolic events had SCAF during monitoring [ ].
In a trial attempting to match anticoagulation with periods of AF, Martin et al. enrolled 2718 patients with ICDs with remote monitoring capability from a single arrhythmia center and randomized them into a study arm where anticoagulation was started and stopped based on CHADS 2 risk and remote AT/AF data versus a control arm in which anticoagulation was determined by standard of care. It did not exclude patients with known PAF. In the study arm, anticoagulation was initiated on detection of AT/AF and was stopped if patient was free of AT/AF for greater than 30 days for patients with CHADS 2 scores of 1 or 2, and greater than 90 days for patients with CHADS 2 score of 3 or 4. CHADS 2 scores of 5 and 6 and any AF were placed on anticoagulation regardless of duration or AF-free interval. Patients in the treatment arm were started on anticoagulation sooner than the control (3 days vs. 54 days p < 0.001); however, they failed to find a difference in either stroke, systemic embolism, or major bleeding rates between the arms. Additionally, it did not find a temporal relationship between periods of AT/AF and stroke in either arm. It cannot be determined if clinical benefit of early initiation of anticoagulation was offset by early discontinuation in reducing stroke risk; however, these findings suggest stroke and systemic embolism risk in PAF patients extend beyond only periods of AF [ ].
However, it should be noted that the enrollment criteria of some of the aforementioned studies may limit generalizability. Both CRYSTAL-AF and ASSERT excluded patients with known AF at baseline as they were assessing the role of SCAF specifically in stroke [ , ]; it may be possible that SCAF confers a different risk profile than “traditional” AF; thus, a robust temporal relationship was not demonstrated that may have been seen in clinical AF. In TRENDS, AF was not an exclusion criteria; however, only 20% of the study population had AF at baseline, potentially dampening the role of “traditional” AF [ ].
Conversely, Turakhia et al. retrospectively studied a cohort of 9850 patients with remotely monitored ICDs, identified a sub-group of 187 patients with ischemic stroke and 120 days of preceding continuous rhythm monitoring. Thirty-nine percent of the 187 patients had a known history of AF. Eighty-three percent of the 187 patients did not have any episodes of AF (defined as ≥ 5.5 h of AF on any day) in the 120 days preceding stroke, excluding them from final analysis. Of the remaining 17% of patients, they then compared the burden of AF in the 30 days prior to stroke to the 30-day control period 91–120 days prior to stroke. A total of 16 patients had discordance in the presence or absence of AF in the two 30-day periods. After controlling for warfarin, an odds ratio of stroke in the 30 days after AF was calculated among this group [4.2 (95% C.I. 1.5–13.4)], with the highest odds ratio in the 5 days after AF [17.4 (95% C.I. 5.39–73.1)], suggesting in fact that there is a temporal relationship between episodes of AF and stroke. However, three of the patients had AF only in the more remote 30-day control period. Moreover, 15 patients had AF in both the more remote, control period and the 30-day period preceding stroke and were not included in the calculation of odds ratios [ ].
More study is needed to clarify the temporal role between episodes of AF and stroke, but the current body of evidence suggests that the temporal relationship between the two should not be assumed.
Clinical Implications and Future Directions
Overall, these findings suggest that the mechanism underlying the relationship between AF and thromboembolism goes far beyond atrial stasis, giving credence to the role of structural changes and the prothrombotic state associated with AF. More research is needed to better describe the significance of AF which does not immediately precede thrombotic events; however, the evidence to date suggests a complex interaction of factors leading to increased risk of stroke and thromboembolism. Given that LA structural abnormalities predict stroke risk even independent of AF and that there is not a clear temporal relationship between arrhythmia onset and CVA event, perhaps AF burden serves as a marker of the severity of LA pathology that ultimately drives increased risk of stroke. With this in mind, perhaps there is a role of incorporating commonly or previously available imaging findings (i.e., echocardiography, computerized tomography, and cardiac magnetic resonance imaging), biomarkers, and AF burden, which may more directly reflect the degree of LA pathology and may provide additional value in risk prediction of stroke when added to traditional risk factors. This may serve to improve risk stratification in challenging intermediate risk patients. However, benefit cost analyses should be performed.
There are direct clinical implications of this potential paradigm shift. If thromboembolism is a product of the cardiac milieu manifested in AF, not episodes of AF itself, then correction of AF through ablation or anti-arrhythmic agents may not return an individual to a non-AF risk profile. With regard to ablation, preliminary data suggests that ablation reduces ischemic stroke risk and mortality [ , ]. However, these data are complicated by the risk of selection bias and narrow inclusion criteria for ablation [ ]. Further studies are being conducted in this field to address these questions. Currently, the ESC guidelines recommend the same anticoagulation strategy for cardioversion be applied to catheter ablation, but a focused update from the Heart Rhythm Society recommended anticoagulation for 2 months after ablation, with further anticoagulation based on individualized stroke risk [ , ].
Similarly, while current evidence mandates that anti-arrhythmic agents do not preclude anticoagulation on the observation that in most cases PAF is not completely eliminated [ ], the rare individuals with perfect AF suppression may be identified with long-term monitoring. However, the same question faced in the successful ablation population would remain, as to whether elimination of AF alone is sufficient to return a similar risk profile as a non-AF patient. The absent temporal relationship between AF and thromboembolic events in some studies suggests that it may not be sufficient.
Finally, with the advent of reliable long-term monitoring, the role of screening for AF for primary prevention remains an unanswered question. Data from the Framingham Heart Study suggests that the lifetime risk for AF is 1 in 4 [ ]. AF incidence is age dependent; at age 65, the incidence is estimated to be 5.5%, increasing to 15.7% by 75 [ ]. It is possible that with long-term monitoring, the incidence will be found to be considerably higher. Given the increased risk of stroke associated with AF and significant associated morbidity and mortality, extended screening for AF for primary prevention, much like widely accepted cancer screening, may be worth consideration. The screening technology, screening duration, and screening frequency all stand to affect the potential yield and cost efficacy of screening. This should prove to be an exciting area of future study. Moving outside of the clinic, as sensors on smart phones and wearable devices become more accurate and prevalent, self-screening and detection may be the future.
Conclusion
Current, guidelines-based medical practice relies heavily on risk stratification scoring systems to determine who should be placed on anticoagulation. Data obtained over long periods of time by implantable cardiac devices suggest that the amount of time spent in AF correlates with the risk of stroke and that there is a significant amount of undiagnosed SCAF. Current risk stratification scores fail to consider AF burden. For those whom AF goes undiagnosed, risk stratification scores are never applied. The observed temporal dissociation between AF and stroke in some studies suggests a complex interplay of factors that is not completely understood. More study is needed to determine the role of expanded use of implanted cardiac devices for screening and how to best weigh AF burden data when available in risk stratification systems. Diagnosing PAF remains a challenge, but with the advent of long-term monitoring, a large number of cases of SCAF are being detected, and AC decisions will have to be made for these individuals. The role of using long-term monitors for screening for primary prevention remains an exciting, but unanswered question.