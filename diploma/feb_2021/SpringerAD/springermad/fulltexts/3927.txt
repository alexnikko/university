Introduction
Markov models are stochastic models used to model changing systems that moves sequentially from one state to another and where each transition to a next state is regarded as random and depends only on its most recent values. In particular, a Markov chain with memory 1 assumes that only the current state is necessary to predict the future state [ ].
Markov models are therefore well-suited for diseases for which multiple states may exist. In particular, Markov models may describe the transitions between different states in disorders such as depression, migraine or epilepsy.
The state is directly visible to the observer in a Markov model. In a hidden Markov model (HMM), the state is not directly visible, but some outcome, dependent on the state, is visible [ , ]. HMMs can be applied to many domains and have turned out to be particularly useful in several biological contexts. For example, this model structure has been applied for modelling the individual epileptic activity [ , , , ]. Indeed, individual profiles of epileptic seizures usually show distinct periods of low and high epileptic activity. Then, observed seizure frequencies would represent the observed output, while the unobserved state would represent the disease activity.
HMMs have shown to be helpful when characterizing different stages of migraines from a headache score (observed variable) [ , ]. Transitions between specific sleep states have also been successfully described using a hidden Markov model [ ].
In the HMMs considered above, the state space of the hidden variables is discrete with a small number of states, while the observations themselves can either be discrete (typically generated from a categorical or a count distribution) or continuous (typically from a Gaussian distribution).
Hidden Markov models can also be generalized to allow continuous state spaces. In other words, the sequence of latent variables is a discrete-time Markovian process that can take any value in some continuous set. In particular, a Gaussian autoregressive process of order 1 is a Markovian process with memory 1 that can take any real value. The autoregressive model of order 1, AR(1), specifies that the prediction of the next value depends linearly on the current value [ ].
Markov chains and autoregressive models are discrete-time processes. A value of the variable is defined each hour, day, week,…. On the other hand, biological phenomena are continuous-time phenomena. Then, the value of any biological parameter should be well defined at any time. Discrete-time processes implicitly assume that this parameter value is piecewise constant (during one hour, one day, one week,…) and suddenly jumps at the end of each time interval.
Such an assumption is acceptable if the latent variable is a hidden state, such as low and high epileptic activity, for instance, but probably not if this latent variable is a PK parameter such as the absorption rate constant or the clearance. If these PK characteristics are now considered as continuous time varying functions, then, continuous processes should be used for properly modelling the fluctuations of these parameters over time.
This naturally leads us to introduce a stochastic differential equations (SDEs) based model as an extension of an ordinary differential equations (ODEs) based model [ ]. In particular, we show that the continuous-time Ornsein–Uhlenbeck diffusion process is the limit of a discrete-time AR(1) process when the time interval tends to 0. Then, a diffusion model is a continuous-time and continuous-state space Markovian process well-suited for properly describing biological phenomena. The use of such stochastic process has therefore been widely used for PKPD model development [ , , , , , , ].
In the population approach, mixed effects hidden Markov models for discrete time processes and mixed effects diffusion models for continuous time processes aim to describe each individual series of observations using either a HMM or a system of SDEs while also taking into account variability between individuals [ , , , , , , ].
Parameter estimation in mixed effects models with (hidden) Markovian dynamics then requires specific methodologies. New inference procedures for mixed effects HMMs were proposed in [ ]. These methods combine the SAEM algorithm with the Forward/Backward algorithm for maximum likelihood estimation of the population parameters and the Viterbi algorithm for recovering the individual sequences of hidden states [ ]. Note that these methods are now available in NONMEM [ ].
Donnet and Samson propose a survey of existing estimation methods for PKPD models based on SDEs [ ]. A first comment is that most parametric estimation methods proposed for SDEs require high frequency data and are often poorly suited for sparse PKPD data.
In simple SDE models, such as a linear dynamical system, exact inference is tractable using the Kalman recursive filter for computing the exact likelihood [ , ]; however, in general, exact inference in diffusion models is infeasible, and approximate methods must be used, such as the extended Kalman filter. Specific combinations of the extended Kalman filter with the FOCE method have been suggested in [ , , , ], and with the SAEM algorithm in [ ].
Approximations of the likelihood using the particle filter were proposed in [ , ]. Other approximations of the likelihood in mixed-effects diffusion models can be found in [ , ].
The paper is organized as follows. Section “ Hidden Markov models ” is devoted to hidden Markov models. We use a Poisson model to illustrate the differences between a mixture model, where the sequence of states over time are independent random variables, and a Poisson HMM where the states form a Markov chain. An application to epileptic seizure counts illustrates the extension to mixed effects HMM. Continuous state-space Markovian models, including autoregressive models and diffusion models are introduced in Section “ Continuous state-space Markovian models ”. A very simple PK model with time varying clearance illustrates these different models. An application to the theophyllin PK data shows that random fluctuations of this PK parameter may explain a significant part of the within subject variability of the data.
Hidden Markov models
The individual approach
The model
HMMs were developed to describe how a given system moves from one state to another over time in situations where the successive visited states are unknown and a set of observations is the only available information to describe the system’s dynamics [ ]. HMMs can be seen as a variant of mixture models that allow for possible memory in the sequence of hidden states. Fig. 1 Dynamics of a hidden Markov model
An HMM is thus defined as a pair of processes \(((z_j,y_j),\, j=1,2,\ldots )\) , where the latent sequence \((z_j)\) is a Markov chain and the distribution of observation \(y_j\) at time \(t_j\) depends on state \(z_j\) (see Fig. 1 ).
Here, we consider a parametric framework with homogeneous Markov chains on a discrete and finite state space \(\{1,2,\ldots ,L\}\) . Then, there exists transition probabilities \((p_{k\ell }, 1 \le k,\ell \le L)\) such that, for all \(j=1,2,\ldots , n\) , $$\begin{aligned} \mathbb {P}\!\left( z_{j}=\ell | z_{j-1}=k\right) = p_{k\ell } ; \quad 1 \le k,\ell \le L \end{aligned}$$ and where \(\sum _{\ell =1}^L p_{k\ell } = 1\) for all \(k \in \{1,2,\ldots ,L\}\) .
The joint distribution of \((z_1, \ldots z_n)\) is then defined by the transition matrix \(P=(p_{k\ell }, 1 \le k,\ell \le L)\) and the probability distribution of the initial state \(z_0\) .
There are several possible options for defining the distribution of \(z_0\) : 1. A first option consists in assuming that \(z_0\) is uniformly distributed on \(\{1,2,\ldots ,L\}\) : $$\begin{aligned} \mathbb {P}\!\left( z_0 = \ell \right) = 1/L ; \quad 1 \le \ell \le L \end{aligned}$$ 2. Assuming that the sequence \((z_j, j\ge 0)\) is stationary means that there exists a vector of probabilities \(\pi =(\pi _1,\pi _2,\ldots ,\pi _L)\) such that \(\mathbb {P}(z_j=\ell )=\pi _\ell\) for any \(\ell \in \{1,2,\ldots ,L\}\) and any \(j\ge 0\) . \(\pi\) is therefore the distribution of \(z_0\) and is solution of $$\begin{aligned} \pi = \pi \, P . \end{aligned}$$ 3. The simplest option consists in fixing \(z_0\) to some arbitrary value in \(\{1,2,\ldots ,L\}\) . For instance, we can select the most likely value, i.e., the value \(\ell\) which maximizes \(\pi _\ell = \mathbb {P}(z_j=\ell )\) . The choice of the initial distribution has usually a very limited impact on the results in practice. A uniform distribution will be used in the numerical examples presented below but any other choice would also be valid and lead to (almost) the same results.
Conditionally on \(z_j=\ell\) the distribution of the observation \(y_j\) is assumed to be a parametric distribution whose parameter depends on the state \(\ell\) . Poisson distributions will be used in the example below for modelling daily counts of epileptic seizures. In this model, the parameters are therefore the Poisson rates \((\lambda _\ell , 1 \le \ell \le L)\) where $$\begin{aligned} y_j | z_j=\ell \ \sim \ \mathrm{Poisson}(\lambda _\ell ) \end{aligned}$$ Parameters of a Poisson hidden Markov model are \(\psi =(P,\lambda _1,\ldots ,\lambda _L)\) where P is the transition matrix of the Markov chain.
Remark
A mixture model can be seen as an extension of a HMM, without any memory. Indeed, a mixture model assumes that the states are independent, i.e., \(\mathbb {P}(z_{j}=\ell | z_{j-1}=k) = \mathbb {P}(z_{j}=\ell ) = \pi _\ell\) . Then, the observations are also independent and $$\begin{aligned} y_j \mathop {\sim }_\mathrm{i.i.d.}\sum _{\ell =1}^L \pi _\ell \mathrm{Poisson}(\lambda _\ell ) \end{aligned}$$ Parameters of a Poisson mixture model are \(\psi =(\pi _1,\ldots ,\pi _L,\lambda _1,\ldots ,\lambda _L)\)
Inference in HMM
Inferring hidden Markov models is challenging, mostly due to the complex expression of the likelihood and to the non observable visited states. As a consequence, hidden Markov models are associated with three basic problems: 1. Compute the likelihood function, i.e., the probability distribution function of the observations \(\mathcal{L}(\psi ; y) = {\texttt p}(y; \psi )\) . The Forward-Backward algorithm (FB) is a recursive procedure for solving this problem with a limited complexity. 2. Estimate the vector of model parameters \(\psi\) . The maximum likelihood estimate \(\hat{\psi }\) maximizes the likelihood \(\mathcal{L}(\psi ; y)\) . The Expectation-Maximization (EM) algorithm for HMM is known as the Baum-Welch algorithm and can be used for computing \(\hat{\psi }\) . 3. Estimate the unknown sequence of states \((z_j)\) . The most probable sequence of states is the one that maximizes the conditional distribution \(\mathbb {P}(z_1,z_2,\ldots ,z_n|y_1,\ldots ,y_n)\) . This sequence can be computed using the Viterbi algorithm . These algorithms are referenced and discussed in [ ]. They are implemented in most of the packages for inference in HMM, including the mhsmm R package [ ] that we will use in the next section.
Example
We will use the data previously analyzed by Delattre et al. [ ]. The data displayed in Fig. 2 consists of daily counts of epileptic seizures measured over 237 days for a representative patient of the study Fig. 2 Observed daily seizure counts for a single patient .
A first model proposed Trokoñiz et al. in [ ] assumes that the daily number of seizures are independent Poisson random variables with intensity \(\lambda\) . Fig. 3 Empirical cumulative distribution function of the sequence of the 237 daily seizure counts ( black ) and cdf of the fitted model ( red ): a Poisson distribution, b mixture of two Poisson distributions, c mixture of three Poisson distributions (Color figure online)
Here, the maximum likelihood estimate of \(\lambda\) is \(\bar{y}=6.12\) . We clearly see from Fig. 3 a that the estimated Poisson distribution doesn’t fit well the empirical distribution of the data. Indeed the overdispersion in the data is not taken into account in this model since \(\lambda\) is both the mean and the variance of the number of daily seizures in a basic Poisson model [ ].
In order to better take into account this overdispersion, the authors proposed a mixture of two Poisson distributions with parameters \(\lambda _1\) and \(\lambda _2\) . The estimated parameters of the mixture obtained with the EM algorithm were \(\hat{\pi }_1=0.84\) , \(\hat{\pi }_2=0.16\) , \(\hat{\lambda }_1=3.59\) and \(\hat{\lambda }_2=19.56\) . Figure 3 b shows that the fit is improved with this mixture model.
Finally, we see from Fig. 3 c that the empirical distribution of the data is very well described by a mixture of three Poisson distributions with the following parameters obtained with the EM algorithm: \(\hat{\pi }_1=0.749\) , \(\hat{\pi }_2=0.200\) , \(\hat{\pi }_3=0.051\) , \(\hat{\lambda }_1=3.07\) , \(\hat{\lambda }_2=10.99\) and \(\hat{\lambda }_3=32.11\) . Fig. 4 Observed daily seizure counts and the estimated states assuming a mixture model with different states of epileptic activity ( top two states, bottom three states). The mixture model assumes that the states are independent
Once the vector of model parameters \(\psi\) has been estimated, each state \(z_j\) , \(1 \le j \le n\) , is estimated by maximizing the conditional distribution \(\mathbb {P}(z_j=\ell | y_j , \hat{\psi })\) . The estimated states are displayed in Fig. 4 .
However, according to Delattre et al. [ ], it is not realistic to assume that the states are independent if these states correspond to different levels of epileptic activity. Indeed, epileptic patients are reasonably expected to stay in the same state several days rather than switching randomly every day between states. Delattre et al. then propose to describe the dynamics of the states by using a discrete Markov model Fig. 5 Observed daily seizure counts and the estimated states assuming different states of epileptic activity ( top two states, bottom three states). A HMM describes the dynamics of the states
The Baum-Welch algorithm implemented in the mhsmm R package was used to find the unknown parameters of the model, assuming two and three states: $$\begin{aligned} \mathcal{M}_\mathrm{2 states}: \quad&\hat{\lambda } = \left( \begin{array}{r} 3.5 \\ 18.9 \end{array} \right) \quad \hat{P} =\left( \begin{array}{cc} 0.890 &{} 0.110 \\ 0.533 &{} 0.467 \end{array} \right) \\ \mathcal{M}_\mathrm{3 states}: \quad&\hat{\lambda } = \left( \begin{array}{r} 3.1 \\ 11.2 \\ 32.2 \end{array} \right) \quad \hat{P} = \left( \begin{array}{ccc} 0.875 &{}0.096&{} 0.029 \\ 0.355&{} 0.542 &{}0.103 \\ 0.527&{} 0.305 &{}0.168 \end{array} \right) \end{aligned}$$ The states \((z_j)\) estimated using the mhsmm package are displayed in Fig. 5 . It is worth to highlight some interesting differences between the solutions obtained from a mixture model and a HMM. If we consider the model with 3 states, a mixture model will predict \(\hat{z}_j=1\) if \(y_j\le 6\) , \(\hat{z}_j=2\) if \(7\le y_j\le 20\) and \(\hat{z}_j=3\) if \(y_j\ge 21\) . On the other hand, we can see around \(t=50\) days or \(t=150\) days that the classification rule of a HMM takes into account not only the value of the observed count, but also the estimated states the day before and the day after. In particular, even if few seizures have been observed during days 49 and 151, these two days are classified in state 2 rather than in state 1 because the neighboring days are also classified in state 2.
The population approach
Mixed effects hidden Markov model
A mixed effects hidden Markov model (MEHMM) is a straightforward extension of an HMM. In a population context, an HMM is first specified for each individual. The N individual HMMs are supposed independent with the same structure, but their parameters may vary from one individual to another.
Let \(\psi _i\) denote the set of parameters of the \(i^{th}\) HMM, \(i=1,\ldots ,N\) . We have seen in Section “ The model ” that the vector of parameters \(\psi _i\) of a Poisson HMM is basically composed of the transition probabilities \((p_{k\ell ,i} = \mathbb {P}(z_{i,j}=\ell | z_{i,j-1}=k))\) and the Poisson intensities \((\lambda _{\ell , i})\) .
The population distribution of the parameters is the probability distribution of the vector of individual parameters \(\psi _i\) : $$\begin{aligned} \psi _i \sim {\texttt p}(\, \cdot \, ; \, \theta ) \end{aligned}$$ where \(\theta\) is the vector of population parameters .
Inference in mixed effects HMM
Usual tasks, methods and algorithms for mixed effects models include 1. Estimation of the population parameters \(\theta\) . The maximum likelihood estimate \(\hat{\theta }\) maximizes the observed likelihood \(\mathcal{L}(\theta ; y)\) . EM-like algorithms (SAEM, MCEM, MCPEM [ ]) as well as approximation based algorithms (FO, FOCE, Laplace [ ]) require to be able to compute, for each individual i , the complete likelihood $$\begin{aligned} \mathcal{L}(\theta ; y_i,\psi _i)&= {\texttt p}(y_i , \psi _i; \theta ) \\&= {\texttt p}(y_i | \psi _i){\texttt p}(\psi _i; \theta ) \end{aligned}$$ The population pdf \({\texttt p}(\psi _i; \theta )\) of the individual parameters usually has a closed-form expression (when the random effects are normally distributed for instance). On the other hand, we have seen in Section “ Inference in HMM ” that the Forward/Backward algorithm can be used for computing the conditional pdf \({\texttt p}(y_i | \psi _i)\) .
All these estimation methods can therefore be combined with the FB algorithm for estimating \(\theta\) . 2. Computation of the likelihood function $$\begin{aligned} \mathcal{L}(\hat{\theta }; y)&= {\texttt p}(y; \hat{\theta }) \\&= \sum _{i=1}^N \int {\texttt p}(y_i | \psi _i ){\texttt p}(\psi _i; \hat{\theta })d\psi _i \end{aligned}$$ Monte Carlo integration methods (e.g., Importance Sampling) as well as approximation based methods (e.g., linearization, Laplace) can then be combined with the FB algorithm for evaluating the likelihood function \(\mathcal{L}(\hat{\theta }; y)\) . 3. Estimation of the individual parameters \((\psi _i , 1 \le i \le N)\) . For \(i=1,2,\ldots ,N\) , the empirical Bayes estimate (EBE) \(\hat{\psi }_i\) maximizes the conditional distribution $${\texttt{p}}(\psi_i|y_i; {\hat{\theta}}) = \frac{{\texttt{p}}(y_i|\psi_i){\texttt{p}}(\psi_i; {\hat{\theta}})}{{\texttt{p}}(y_i ; {\hat{\theta}})}$$ Here again, the product \({\texttt p}(y_i|\psi _i){\texttt p}(\psi _i; \hat{\theta })\) can be maximized as soon as the conditional pdf \({\texttt p}(y_i|\psi _i)\) can be computed using the FB algorithm. 4. For each individual, estimate the unknown sequence of states \((z_{i,j}, 1 \le j \le n_i)\) . Once the vector of individual parameter \(\psi _i\) has been estimated, the Viterbi algorithm introduced in Section “ Inference in HMM ” allows one to obtain the most probable sequence of states \((\hat{z}_{i,1},\ldots ,\hat{z}_{i,{n_i}})\) that maximizes the conditional distribution \({\texttt p}(z_{i,1},\ldots ,z_{i,{n_i}}|y_{i,1},\ldots ,y_{i,{n_i}},\hat{\psi }_i)\) .
Application to epileptic seizure count data
The data base analyzed by Delattre et al. in [ ] consisted of six clinical studies. Recruited patients were on standard anti-epileptic therapy during several weeks before being randomized to parallel treatment groups. Delattre et al. proposed to describe exposure-response relationship of an antiepileptic agent, using a mixed effects HMM. This analysis shown demonstrated that MEHMM is able to mimic dynamics of seizure frequencies very well.
A complete model was developed for both the screening and the active treatment phases in [ ] but we will restrict our analysis to the screening phase. The data therefore consists of 71447 daily counts of epileptic seizures measured between 3 and 30 weeks for \(N=788\) patients.
An homogeneous Markov chain with two states is used for these data: $$\begin{aligned} \mathbb {P}\!\left( z_{i,j}=1 | z_{i,j-1}=1\right)&= 1 - \mathbb {P}\!\left( z_{i,j}=2 | z_{i,j-1}=1\right) = p_{11,i} \\ \mathbb {P}\!\left( z_{i,j}=2 | z_{i,j-1}=2\right)&= 1 - \mathbb {P}\!\left( z_{i,j}=1 | z_{i,j-1}=2\right) = p_{22,i} \end{aligned}$$ Then, the distribution of the seizure counts is a mixture of two Poisson distributions: $$\begin{aligned} y_{i,j} | z_{i,j}=1 \sim \mathrm{Poisson}(\lambda _{1,i})\\ y_{i,j} | z_{i,j}=2 \sim \mathrm{Poisson}(\lambda _{2,i})\\ \end{aligned}$$ According to this model, the vector of individual parameters for patient i is \(\psi _i = (p_{11,i}, p_{22,i}, \lambda _{1,i}, \lambda _{2,i})\) . We assume logit-normal distributions for \(p_{11,i}\) and \(p_{22,i}\) and log-normal distributions for \(\lambda _{1,i}\) and \(\lambda _{2,i}\) : $$\begin{aligned} \mathrm{logit}(p_{11,i})&\sim \mathcal{N}(\mathrm{logit}(p_{11,\mathrm{pop}}) , \omega _{11}^2) \\ \mathrm{logit}(p_{22,i})&\sim \mathcal{N}(\mathrm{logit}(p_{22,\mathrm{pop}}) , \omega _{22}^2) \\ \log (\lambda _{1,i})&\sim \mathcal{N}(\log (\lambda _{1,\mathrm{pop}}) , \omega _{1}^2) \\ \log (\lambda _{2,i})&\sim \mathcal{N}(\log (\lambda _{2,\mathrm{pop}}) , \omega _{2}^2) \end{aligned}$$ where \(\mathrm{logit}(x)=\log (x/(1-x))\) for \(0< x < 1\) .
Here, the vector of population parameters is \(\theta = (p_{11,\mathrm{pop}},p_{22,\mathrm{pop}},\lambda _{1,\mathrm{pop}},\lambda _{2,\mathrm{pop}},\omega _{11}^2,\omega _{22}^2,\omega _{1}^2,\omega _{2}^2).\)
Using the proposed model, we can estimate what is, for each individual, the individual probability of transitioning to the high/low activity state as well as the probability of remaining in the same state.
Estimation of the population parameter \(\theta\) , the individual parameters \((\psi _i)\) and the individual sequences of states \((z_{ij})\) were performed with the Monolix software (version 4.3.3) combined with a Matlab implementation of the Forward-Backward and Viterbi algorithms. The Monolix project and the Matlab codes are available as supplementary material. Table 1 Estimation of the population parameters for the epileptic seizure count data obtained with a mixture model and a hidden Markov model Mixture model HMM model Parameter Estimate SE Estimate SE \(p_\mathrm{1, pop}\) 0.907 0.011 − − \(p_\mathrm{11, pop}\) − − 0.87 0.007 \(p_\mathrm{22, pop}\) − − 0.48 0.03 \(\lambda _\mathrm{1, pop}\) 0.209 0.011 0.126 0.009 \(\lambda _\mathrm{2, pop}\) 1.86 0.10 1.25 0.05 \(\omega _{p_1}\) 1.94 0.10 − − \(\omega _{p_{11}}\) − − 0.88 0.05 \(\omega _{p_{22}}\) − − 2.17 0.12 \(\omega _{\lambda _{1}}\) 1.23 0.04 1.44 0.05 \(\omega _{\lambda _{21}}\) 0.93 0.03 1.03 0.03
Results obtained with a mixture model and a HMM are presented in Table 1 . Fig. 6 Top observed seizure counts for three typical subjects; bottom estimated sequences of hidden states for the three selected subjects obtained with the Viterbi algorithm; black states of high epileptic activity, pink states of low epileptic activity (Color figure online)
The observed data and the estimated states for three representative patients are displayed in Fig. 6 . Huge inter individual variability of the distribution of the seizure counts can be clearly seen in this Figure. Nevertheless, a Markovian dynamics between two states of epilectic activity seems to properly describe the data for these three patients (similar results are obtained with the other patients of the study).
Continuous state-space Markovian models
The individual approach
Discrete time model
We were assuming in the previous section that there exists some latent variable that takes a finite number of values. Such an assumption may be a good approximation when this latent variable aims to describe the state of a patient over time, for instance, but it may be not appropriated when the phenomena to describe cannot be reduced to a finite number of states.
Consider, for instance, the pharmacokinetics of a drug which characteristics may change over time. Then, it is not realistic to assume a finite number of PK characteristics for a given patient (slow/fast absorption, slow/fast elimination,…).
A widely used approach consists of splitting the study into M time periods, or “occasions” and assuming that some PK parameters can vary from occasion to occasion but remain constant within occasion. In other word, the sequence of PK parameters \((\psi _{m}, 1 \le m \le M)\) randomly fluctuates around some typical vector of PK parameters \(\psi ^\star\) . In pharmacometrics, an occasion usually represents an entire dosing cycle, i.e., the time interval between two successive doses, but this definition can easily be extended to any arbitrary sequence of time intervals.
As an illustration, consider a one compartment PK model for a single bolus administration: $$\begin{aligned} A(t) = D e^{- \frac{Cl}{V}\, t} \end{aligned}$$ where D is the amount of drug administered at time \(t=0\) . Here, A ( t ) is the amount of drug in the central compartment at time t , V the volume of the central compartment and \(Cl\) is the drug clearance.
A simple model for inter occasion variability (IOV) assumes that \(Cl_m\) , the clearance during occasion m , is log-normally distributed around some typical value \(Cl^\star\) $$\begin{aligned} \log (Cl_{m}) = \log (Cl^\star ) + \zeta _{m} ; \quad m=1,2,\ldots , M \end{aligned}$$ (1) where \(\zeta _{m} \mathop {\sim }_\mathrm{i.i.d.}\mathcal{N}(0, \tau ^2)\) . Fig. 7 Left two sequences of log-clearances \((\log (Cl_m), 1\le m \le 3)\) simulated around \(\log (Cl^\star )=3\) with an inter-occasion variability model and different variances \(\tau ^2\) ; right predicted amounts for these two sequences (the amount for a constant clearance \(Cl^\star =3\) is displayed in black dotted line )
Let \(\delta\) be the length of an occasion and let \(t_m=m\delta\) for \(m=0,1,\ldots , M\) . Then, \(Cl(t) = Cl_m\) for \(t_{m-1} \delta \le t < t_m \delta\) and, conditionally to the sequence \((Cl_m)\) , the predicted amount defined by the model is $$\begin{aligned} A(0)&= D \\ A(t)&= A(t_{m-1}) e^{- \frac{Cl_m}{V} (t-t_{m-1})} ; \quad t_{m-1} \le t < t_{m} \end{aligned}$$ An example is displayed in Fig. 7 . There are only three occasions of length \(\delta = 10\) h in this example. Two sequences \((Cl_m, 1\le m \le 3)\) were simulated with \(\tau =0.20\) and \(\tau =0.50\) , respectively, \(Cl^\star =3\,\mathrm{L.h}^{-1}\) , \(V=1\mathrm{L}\) and \(D=1\) g.
In this model, the sequence \((Cl_m)\) of clearances is independent. Then, the model allows any jump between slow and fast eliminations. If we prefer to assume that the clearance cannot drastically change from one day to the next, a Markovian dynamics can be introduced in order to better control the fluctuations of this parameter around \(Cl^\star\) .
Let $$\begin{aligned} X_{m} = \log (Cl_{m}) - \log (Cl^\star ) \end{aligned}$$ be the deviation of the log-clearance from its typical value at period m . A discrete-time Markov process with memory 1 may be represented by an autoregressive process of order 1 (AR(1) process). $$\begin{aligned} X_{m+1} = \rho X_{m} + \zeta _{m+1} \end{aligned}$$ (2) where \(0 \le \rho < 1\) and where \(\zeta _{m} \mathop {\sim }_\mathrm{i.i.d.}\mathcal{N}(0, \tau ^2)\) . Furthermore, \(\zeta _{m+1}\) is independent of \((X_\ell , \ell \le m)\) in this model. Because of this property, the sequence \((\zeta _{m})\) is known as the innovation of the process \(X\) .
We can easily check that \(\rho\) is the correlation between \(X_{m}\) and \(X_{m+1}\) . More generally, \(\rho ^\ell\) is the correlation between \(X_{m}\) and \(X_{m+\ell }\) .
When \(\rho =0\) , the \(X_{m}\) ’s are independent and identically distributed around 0. The model is indeed the one represented in equation ( 1 ): the \(Cl_m\) are independent and log-normally distributed around the typical value \(Cl^\star\) . Fig. 8 Left two sequences of log-clearances \((\log (Cl_j), 1\le j \le 30)\) simulated around \(\log (Cl^\star )=3\) with an autoregressive model of order 1, different correlations \(\rho\) and different variances of innovation \(\tau ^2\) ; right predicted amounts for these two sequences (the amount for a constant clearance \(Cl^\star =3\) is displayed in black dotted line )
On the other hand, when \(\rho\) is close to 1, \(X_{m+1}\) tends to be close to the previous value \(X_{m}\) , which also means that \(Cl_{m+1}\) tends to be close to \(Cl_{m}\) . More precisely, \((Cl_m, m \ge 1)\) is a Markov chain with a log-Gaussian transition kernel $$\begin{aligned} \log (Cl_{m+1}) |Cl_{m} \sim \mathcal{N}\left( \rho \log (Cl_{m})+(1-\rho )\log (Cl^\star ) \ , \ \tau ^2 \right) \end{aligned}$$ \(\delta =1\) h was used in the example displayed in Fig. 8 . We can see that when both the variance \(\tau ^2\) and the correlation \(\rho\) are small \((\tau ^2=0.25^2, \rho =0.1)\) the predicted amount profile remains very close to the typical amount profile obtained with \(Cl^\star =3\,\mathrm{L.h}^{-1}\) . On the other hand, successive stages of slow and fast elimination clearly appear with higher values for these parameters \((\tau ^2=0.50^2, \rho =0.8)\) .
Stochastic differential equation based model
The AR model represented in ( 2 ) assumes that some PK parameter is piecewise constant and suddenly jumps at the end of each period. Such an assumption may not be biologically realistic and a continuous process should probably be considered for modelling the fluctuations of this parameter over time.
One approach consists in letting the period length \(\delta\) tend to 0. Since the limiting process (when \(\delta =0\) ) is a continuous process, the correlation \(\rho\) of the process should tend to 1 and the variance \(\tau ^2\) of the innovation should tend to 0 when \(\delta\) goes to 0. These constraints are satisfied setting, for instance, \(\rho =(1 - \alpha \delta )\) and \(\tau = \gamma \sqrt{\delta }\) where \(0\le \alpha <1\) and \(\gamma >0\) . With this new parameterization, the AR model now writes $$\begin{aligned} X_{m+1} = (1 - \alpha \delta ) X_m + \gamma \sqrt{\delta } e_{m+1} \end{aligned}$$ or, equivalently, $$\begin{aligned} X_{m+1} - X_m = - \alpha X_m \delta + \gamma \sqrt{\delta } e_{m+1} \end{aligned}$$ where \(e_{m+1} = \zeta _{m+1}/\tau \sim \mathcal{N}(0,1)\) . Letting \(\delta\) go to 0 leads to the Ornstein–Uhlenbeck diffusion process $$\begin{aligned} \mathrm{d}X(t) = -\alpha X(t) \mathrm{d}t + \gamma \mathrm{d}W(t) \end{aligned}$$ where \((W(t), t \ge 0)\) is a standard Wiener process in \(\mathbb {R}\) : $$\begin{aligned}&W(t) \sim \mathcal{N}(0, t) \\&\mathbb {E}\left( W(t)(W(t+s)-W(t))\right) = 0 \end{aligned}$$ We can extend the discrete-time model for the clearance \(Cl\) presented in Section “ Discrete time model ” to this continuous-time model: $$\begin{aligned} \mathrm{d}\log (Cl(t)) = -\alpha \left( \log (Cl(t)) - \log (Cl^\star ) \right) \mathrm{d}t + \gamma \mathrm{d}W(t) \end{aligned}$$ \((Cl(t), t\ge 0)\) is now a continuous-time Markovian process which takes its values in \(\mathbb {R}^+\) . The PK model combines this diffusion model for \(Cl(t)\) with an ODE model for the amount A ( t ): $$\begin{aligned} \frac{\mathrm{d}\log (Cl(t))}{\mathrm{d}t}&= -\alpha \left( \log (Cl(t)) - \log (Cl^\star ) \right) + \gamma \frac{\mathrm{d}W(t)}{\mathrm{d}t} \nonumber \\ \frac{\mathrm{d}A(t)}{\mathrm{d}t}&= - \frac{Cl(t)}{V} A(t) \end{aligned}$$ (3) where \(A(0)=D\) . Fig. 9 Left two sequences of log-clearances \((\log (Cl(t)), 0 \le t \le 30)\) simulated around \(\log (Cl^\star =3)\) with a Ornstein–Uhlenbeck diffusion model, different drift parameters \(\alpha\) and different noise variances \(\gamma ^2\) ; right predicted amounts for these two sequences (the amount for a constant clearance \(Cl^\star =3\) is displayed in black dotted line )
The numerical example displayed in Fig. 9 shows that when the coefficient of the Wiener process term is small ( \(\gamma =1.5\) ) and the coefficient of the drift is large ( \(\alpha =10\) ), then \(Cl(t)\) remains close to the typical value \(Cl^\star\) and the predicted amount profile remains very close to the typical amount profile obtained with \(Cl^\star =3\,\mathrm{L.h}^{-1}\) . Fluctuations show more randomness with \(\gamma =3.5\) and \(\alpha =5\) .
PK model represented in ( 3 ) can easily be extended to more complex models. The absorption rate constant of a model for oral administration, for instance, could also be described by a diffusion model if we suspect that the rate of absorption may fluctuate: $$\begin{aligned} \frac{\mathrm{d}\log (k{ a}(t))}{\mathrm{d}t}&= -\alpha _1 \left( \log (k{ a}(t)) - \log (k{ a}^\star ) \right) + \gamma _1 \frac{\mathrm{d}W_1(t)}{\mathrm{d}t}\nonumber \\ \frac{\mathrm{d}\log (Cl(t))}{\mathrm{d}t}&= -\alpha _2 \left( \log (Cl(t)) - \log (Cl^\star ) \right) + \gamma _2 \frac{\mathrm{d}W_2(t)}{\mathrm{d}t} \nonumber \\ \frac{\mathrm{d}A{ d}(t)}{\mathrm{d}t}&= - k{ a}(t) A{ d}(t) \nonumber \\ \frac{\mathrm{d}A{ c}(t)}{\mathrm{d}t}&= k{ a}(t) A{ d}(t) - \frac{Cl(t)}{V} A{ c}(t) \end{aligned}$$ (4) where \(A{ d}(0)=D\) and \(A{ c}(0)=0\) . Here, \(A{ d}\) and \(A{ c}\) are the amounts of drug in the depot and central compartments.
Both \((k{ a}(t),t\ge 0)\) and \((Cl(t),t\ge 0)\) are Markov processes in this model.
It is also possible to consider that some parameters of the model randomly vary over time while other parameters remain constant. If, for example, the rate of absorption is constant over time, then the first equation of system ( 4 ) should be replaced by \(k{ a}(t)=k{ a}\) .
An Ornstein–Uhlenbeck diffusion process is very convenient for easily introducing continuous random fluctuations of some parameters around some typical values. There are also more general stochastic differential equations that could be considered. For instance, the solution of the typical stochastic differential equation (SDE) $$\begin{aligned} \mathrm{d}X(t) = \mu (X(t),t)\mathrm{d}t + \nu (X(t),t)\mathrm{d}W(t) \end{aligned}$$ is called a diffusion process and is a Markov process under general conditions on functions \(\mu\) and \(\nu\) [ ]. Functions \(\mu\) and \(\nu\) are known, respectively, as the drift and the diffusion coefficient of X . In pharmacology, X can be a time varying PK parameter for instance.
There are also more general SDE’s where the functions \(\mu\) and \(\nu\) depend not only on the present value of the process X ( t ), but also on previous values of the process. In that case the defining equation is called a stochastic delay differential equation. If \(\mu\) and/or \(\nu\) depend on present or previous values of other processes, the solution process X ( t ) is not a Markov process, and it is called an Itô process and not a diffusion process.
Inference in a diffusion model
We consider parametric diffusion models of the form $$\begin{aligned} \mathrm{d}X(t) = \mu (X(t),\psi )\mathrm{d}t + \nu (X(t),\psi )\mathrm{d}W(t) \end{aligned}$$ (5) where \(\psi\) is a vector of parameters.
In practice, X is an hidden process since it cannot be observed directly. Observations \((y_1, y_2,\ldots y_n)\) are some noisy functions of \((X(t_1),\ldots , X(t_n))\) , $$\begin{aligned} y_j = g(X(t_j),\psi ) + \varepsilon _j \end{aligned}$$ where \(\varepsilon _j \mathop {\sim }_\mathrm{i.i.d.}\mathcal{N}(0, \sigma ^2)\) .
PK model ( 3 ) for bolus administration can be written in this form with $$\begin{aligned} X(t) = \left( \begin{array}{cc} \log (Cl(t)) \\ A(t) \end{array} \right) ; \quad \mu (X(t)) = \left( \begin{array}{cc} -\alpha (\log (Cl(t))-\log (Cl^\star )) \\ -\frac{e^{\log (Cl(t))}}{V} A(t) \end{array} \right) ; \quad \nu (X(t)) = \left( \begin{array}{cc} \gamma \\ 0 \end{array} \right) \end{aligned}$$ In this model, observations are observed concentrations and \(g(X(t)) = A(t)/V\) .
Parameters of this model are therefore \(\psi = (Cl^\star , \alpha , \gamma , V, \sigma )\) . Maximum likelihood estimation of \(\psi\) requires to compute the likelihood $$\begin{aligned} \mathcal{L}(\psi ; y)&= {\texttt p}(y ; \psi ) \\&= {\texttt p}(y_1; \psi ){\texttt p}(y_2|y_1; \psi )\cdots {\texttt p}(y_n|y_{n-1},\ldots ,y_1 ; \psi ) \end{aligned}$$ Except in some very specific classes of diffusion models, the transition density \({\texttt p}(y_{j}|y_{j-1},\ldots ,y_{1} ; \psi )\) does not have a closed-form expression since it involves the transition densities of the underlying diffusion process X . When the underlying system is a Gaussian linear dynamical one, this density is Gaussian with mean and variance that can be computed using the Kalman filter. When the system is not linear, a solution comes from approximating this density by a Gaussian one and using the extended Kalman filter (EKF) for quickly computing its mean and variance [ ].
Once \(\psi\) has been estimated, it is possible to recover the latent process \((X(t),t\ge 0)\) at the measurement times \((t_{1},\ldots ,t_{n})\) . The fixed-interval Kalman smoother [ ] estimates \((X(t_{1}), \ldots , X(t_n))\) as the most likely sequence of hidden variables: $$\begin{aligned} (\hat{X}(t_{1}), \ldots ,\hat{X}(t_{n})) = \underset{X(t_{1}), \ldots , X(t_n)}{\text {argmax}} \; {\texttt p}(X(t_{1}), \ldots , X(t_n))|y_{1},\ldots ,y_{n},\hat{\psi }). \end{aligned}$$ Implementation of the EKF requires to solve a system of ordinary differential equations that generally do not have any closed form solution. A simplified version of the numerical resolution method of these ODEs and based on higher-order Taylor approximations has been proposed by Mazzoni [ ].
We should remark that this method only requires the knowledge of the Jacobian function of the drift function \(\mu\) of the dynamical process. It is therefore easily implementable for any diffusion model such that derivatives of \(\mu\) can be computed. Note also that this approach can be easily extended to models involving multidimensional observations.
In comparison to the EKF, particle filters do not require making approximations of the transition density, but are quite demanding in terms of simulation requirements and computation time (see [ ] for an application in quantitative biology).
The population approach
Mixed effects diffusion model
Diffusion model introduced above can be extended in a straightforward manner to mixed effects diffusion model (MEDM) by defining system’s parameters as a vector of individual parameters.
In other words, N individual diffusion models are specified with the same structure as the one represented in ( 5 ), but their parameters \(\psi _i\) may vary from one individual to another.
For an Ornstein–Uhlenbeck diffusion process, \(\psi _i\) is basically composed of the coefficients \(\alpha _i\) and \(\gamma _i\) and the typical parameter value \(\psi _i^\star\) .
As usual, \(\psi _i\) is treated as a random parameter whose distribution depends on a vector of population parameters \(\theta\) .
Inference in mixed effects diffusion models
Here again, the first tasks to perform consist in estimating the vector of population parameters \(\theta\) , computing the likelihood function and estimating the individual parameters \((\psi _i)\) .
It was shown in Section “ Inference in mixed effects HMM ” that suitable methods and algorithms for performing these tasks require to be able to compute the conditional pdf of the observations \({\texttt p}(y_i |\psi _i)\) for each individual i . The Forward-Backward algorithm was shown to be very efficient for computing this conditional pdf for HMM based model. In the case of a diffusion based model, we have seen in Section “ Inference in a diffusion model ” that the extended Kalman filter can be used for computing \({\texttt p}(y_i |\psi _i)\) .
The EKF combined with “standard” inference algorithms for mixed effects model (FOCE, SAEM, Importance Sampling,…) can then be used for mixed effects diffusion models. For a combination of the FOCE approximation of the population likelihood and the EKF, see for example [ , , , ]. The combination of the SAEM algorithm and EKF proposed in [ ] is shown to have very good statistical properties. See [ ] for a review of parameter estimation methods in SDE population models.
Once the population and individual parameters have been estimated, each individual hidden sequence \((X_i(t_j), 1 \le j \le n_i)\) can be recovered using the fixed-interval Kalman smoother [ ] or the particle filter [ ].
Application to the theophyllin data
The proposed approach was previously used in [ ] to analyze the theophyllin PK data. These data come from twelve subjects who received a single oral dose of D =320 mg of theophyllin. For each subject, the concentration of theophyllin in plasma is measured through ten consecutive blood samples after the dose administration. These data are classically described by a one compartment model with first order absorption and linear elimination. For \(i = 1,2,\ldots N\) , $$\begin{aligned} \frac{\mathrm{d}A{ d}_i(t)}{\mathrm{d}t}&= - k{ a}_i A{ d}_i(t)\nonumber \\ \frac{\mathrm{d}A{ c}_i(t)}{\mathrm{d}t}&= k{ a}_i A{ d}_i(t) - \frac{Cl_i}{V_i} A{ c}_i(t)\nonumber \\ C{ c}_i(t)&= \frac{A{ c}_i(t)}{V_i} \end{aligned}$$ (6) where \(A{ d}_i(0)=D\) and \(A{ c}_i(0)=0\) . Here, \(C{ c}\) is the concentration in the central compartment.
Following Delattre and Lavielle, we can assume that the clearance \(Cl_i\) is a diffusion process: $$\begin{aligned} \frac{\mathrm{d}\log (Cl_i(t))}{\mathrm{d}t}&= -\alpha \left( \log (Cl_i(t)) - \log (Cl^\star _i) \right) + \gamma \frac{\mathrm{d}W_i(t)}{\mathrm{d}t}\nonumber \\ \frac{\mathrm{d}A{ d}_i(t)}{\mathrm{d}t}&= - k{ a}_i A{ d}_i(t) \nonumber \\ \frac{\mathrm{d}A{ c}_i(t)}{\mathrm{d}t}&= k{ a}_i A{ d}_i(t) - \frac{Cl_i(t)}{V_i} A{ c}_i(t) \end{aligned}$$ (7) Log-normal distributions describe the inter individual variability of the PK parameters: $$\begin{aligned} \log (k{ a}_i)&\sim \mathcal{N}(\log (k{ a}_\mathrm{pop}), \ \omega _{k{ a}}^2)\nonumber \\ \log (V_i)&\sim \mathcal{N}(\log (V_\mathrm{pop}), \ \omega _{k{ a}}^2) \nonumber \\ \log (Cl^\star _i)&\sim \mathcal{N}(\log (Cl^\star _\mathrm{pop}), \ \omega _{Cl}^2). \end{aligned}$$ (8) On the other hand, coefficients \(\alpha\) and \(\gamma\) of the diffusion are the same for the twelve subjects.
For each individual i , observed concentrations \((y_{ij}, 1 \le j \le n_i)\) measured at times \((t_{ij}, 1 \le j \le n_i)\) are assumed to be log-normally distributed around the concentrations predicted by the PK model: $$\begin{aligned} y_{ij} = \frac{A{ c}_i(t_{ij})}{V_i} e^{\varepsilon _{ij}} \quad ; \quad 1 \le j \le n_i \end{aligned}$$ where \(\varepsilon _{ij} \mathop {\sim }_\mathrm{i.i.d.}\mathcal{N}(0, \sigma ^2)\)
Monolix 4.3.3 was combined with a Matlab implementation of the EKF for estimating the parameters of the model.
When the solution of a system of SDEs is only observed at few time points with noise, it becomes difficult to distinguish stochastic variability from residual variability [ , , ]. Without any prior information on the parameter values, maximum likelihood estimation of the model parameters leads to the solution obtained with the ODE based model ( \(\hat{\sigma }=0.173\) with the theophyllin data). It is then necessary to introduce some informative prior in order to enforce the solution to include a significant stochastic component and not only residual variability. Table 2 Estimation of the population parameters for the theophyllin data with the ODE based model eq:pkode and the SDE based model eq:pksde3 ODE model SDE model Parameter Estimate SE Estimate SE \(k{ a}_\mathrm{pop}\) 1.31 0.25 1.02 0.31 \(V_\mathrm{pop}\) 32.2 1.6 25.2 3.5 \(Cl^\star _\mathrm{pop}\) 2.79 0.20 2.92 0.24 \(\alpha\) − − 0.519 (MAP) 0.22 \(\gamma\) − − 0.433 (MAP) 0.11 \(\omega _{k{ a}}\) 0.627 0.160 0.968 0.240 \(\omega _{V}\) 0.132 0.053 0.345 0.130 \(\omega _{Cl^\star }\) 0.239 0.056 0.164 0.094 \(\sigma\) 0.174 0.014 0.056 (MAP) 0.002
Maximum likelihood estimation of the population PK parameters was therefore combined with a Bayesian estimation of \(\alpha\) , \(\gamma\) and \(\sigma\) using log-normal priors: $$\begin{aligned} \log (\alpha ) \sim \mathcal{N}(\log (0.5) , 0.05^2) ; \quad \log (\gamma ) \sim \mathcal{N}(\log (0.4) , 0.05^2) ; \quad \log (\sigma ) \sim \mathcal{N}(\log (0.04) , 0.05^2). \end{aligned}$$ The Maximum a posteriori (MAP) estimate of these parameters was computed by maximizing the posterior distribution of \(\alpha\) , \(\gamma\) and \(\sigma\) . Estimated parameters are given in Table 2 . The Monolix project files and the Matlab implementation of the extended Kalman filter used for this example are available as supplementary material. Fig. 10 Top observed plasmatic concentrations of theophyllin ( blue dots ) for three subjects and their predicted concentration profiles given by the ODE based model ( green ) and the SDE based model ( red ); bottom predicted evolution of the clearance over time given by the SDE based model (Color figure online)
The predicted PK profiles for three subjects obtained with the two models are displayed in Fig. 10 . The predicted curves obtained under both models are close to the observed concentrations for the three subjects, but we see that the SDE based model leads to a much better fit since the model enables to capture smooth random variations of the concentration kinetics. We can also remark that these three patients share the same profile: a large value of the clearance during the first 6-7 hours indicate a first stage of fast elimination of the drug, followed by a slower elimination. Then, assuming that the elimination rate of theophyllin is not constant over time is a plausible hypothesis. Other models able to take into account this behavior should therefore be tested, including nonlinear elimination, peripheral compartment(s), or inter occasion variability.
Conclusions
Mixed effects hidden Markov models and mixed effects diffusion models have recently been defined as the extensions of HMM and SDE based models to population studies. They have shown to be relevant for modelling biological phenomena that involve some non observed Markovian dynamics.
Sound methodologies for these models have been developed and published during the last decade. Furthermore, several simulation based studies have confirmed their very good statistical behavior: estimators are consistent and show small biases.
These methods consist in treating the hidden states as “nuisance parameters” Indeed, both the Forward-Backward algorithm for HMM and the (extended) Kalman filter for diffusion models allow one to “ignore” these hidden states by integrating the joint distribution of the observations and the hidden process over the hidden process. Standard methods for inference in mixed effects models can then be efficiently combined with these algorithms. In particular, combining these algorithms with the SAEM algorithm allows for the computational burden to be minimal.
Applications on real data examples have demonstrated that these methodologies are also useful in practice, provided that rich data are available. On one hand, the use of mixed effects HMM offers a new insight in the analysis of epilepsy data where a better description of the individual epileptic activity can be taken into account as well as the variability between individuals. On the other hand, mixed effects diffusion models can be used as a realistic alternative to inter occasion variability for properly modelling within and between subject variability of PK parameters.
Unfortunately, implementation and use of MEHMM and MEDM remain quite limited and mainly—not to say only—in the realm of academic research. Indeed, these models cannot be easily implemented and used with the standard software tools available today for mixed effects modelling for pharmacometrics. Then, comparing, for instance, a classical IOV model with a SDE based model represents a tremendous challenge for a pharmacometrician in term of coding. Indeed, in the absence of reliable and easy-to-use tools, a strong statistical expertise is required for implementing the EKF. Having the possibility to easily implement such models in a software tool would probably help to popularize their use.
Further improvements of the present applications of MEHMM and MEDM deserve to be considered for future works. A question of interest concerns the general nature of the latent process. The main assumption for the analysis of seizure counts is that the dynamics of the epileptic activity is described with a two-state first-order Markov chain. We could also consider models with more than two states or higher-order Markov dependence. Other distributions than Poisson distribution could also be considered for the count data. An Ornstein–Uhlenbeck diffusion model was used for modelling the fluctuations of the clearance. Other SDE based models could also be used for the same purpose. Then, specific tools should be developed for model assessment and model selection.